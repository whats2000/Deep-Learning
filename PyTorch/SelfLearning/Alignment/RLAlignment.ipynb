{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Reinforcement Learning for LLM Alignment\n",
    "\n",
    "為何要對齊?\n",
    "1. 透過對齊，可以使模型更好的學習到對齊的特徵，符合人類的直覺喜好\n",
    "2. Safety 與 Fairness: 透過對齊，可以確保模型不會偏向某些特定的族群，或是某些特定的特徵。像是不應該教學製作炸彈的技巧，或是不應該教學種族歧視的言論。但是這有時會違背\"Helpful\"的原則，故需要使用對齊的方法來解決這個問題。\n",
    "3. Factuality: 由於Hallucination的問題，模型可能會產生一些不符合事實的結果，透過對齊，可以確保模型的結果是符合事實的。\n",
    "4. Complex Instruction Following: LLMs可能會無法完全理解複雜的指令，即便透過instruction tuning，也可能無法完全解決這個問題。透過對齊，可以確保模型能夠完全理解指令。\n",
    "5. Thinking Ability: 雖然Chain of Thought有被提出，但是這個方法可能無法完全解決這個問題。尤其該方法僅對數學與邏輯問題有效，對於其他問題可能無法完全解決。故更佳的方法是透過思維對齊，使模型更好的理解人類的思考方式。比較最近提出是先強行Think，再輸出結果，這樣的方法能進一步提升模型的Thinking Ability。\n",
    "\n",
    "### Reward Model\n",
    "1. 學習一個模擬人類的喜好，並給回應打分數的模型。並將這些分數反饋給模型，讓模型學習到人類的喜好。\n",
    "2. 紅隊測試，反之，透過紅隊測試，可以利用誘導模型產初始LLM容易犯錯的地方，使LLM學會避免這些錯誤。\n",
    "3. 然而，由於人類偏好較長的文本，故可能會導致模型生成更傾向於長文本，這使follow字數的指標可能會有偏差。\n",
    "\n",
    "### Challenges\n",
    "- Alignment Tax: 透過對齊，可能會導致模型的其他能力下降，這是因為對齊的目標與模型的目標不同，故可能會導致模型的其他能力下降。解決方法有提出很多，其中一個是平均化目標函數，使模型能夠平衡對齊與原本的目標。\n",
    "- Limited resources: 透過微調，Alignment可能會丟失，例如: 英文Alignment的模型可能會在Finetune的到中文的模型上丟失Alignment。解決方法有提出很多，其中一個是使用Model Merge的方法，將兩個模型合併，使模型能夠保持Alignment。"
   ],
   "id": "d57eb00d722d5243"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
