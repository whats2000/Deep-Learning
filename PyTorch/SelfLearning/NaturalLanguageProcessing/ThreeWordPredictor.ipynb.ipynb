{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "{'A': 'at',\n 'B': 'ate',\n 'C': 'bed',\n 'D': 'boy',\n 'E': 'cat',\n 'F': 'chair',\n 'G': 'chased',\n 'H': 'dog',\n 'I': 'floor',\n 'J': 'in',\n 'K': 'jar',\n 'L': 'lay',\n 'M': 'mouse',\n 'N': 'on',\n 'O': 'pickle',\n 'P': 'refrigerator',\n 'Q': 'sat',\n 'R': 'table',\n 'S': 'the',\n 'T': 'under',\n 'U': '.'}"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the code to read the new token file format\n",
    "token_dictionary = {}\n",
    "with open('Resource/tokens.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        token, word = line.strip().split(': ')\n",
    "        token = token.strip('\" ')\n",
    "        word = word.strip('\" ')\n",
    "        token_dictionary[token] = word\n",
    "\n",
    "# Test the updated token dictionary\n",
    "token_dictionary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:07:13.795659500Z",
     "start_time": "2023-10-30T14:07:13.790261400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['The pickle under the table lay in the pickle jar.',\n 'Under the table sat the pickle in the pickle jar.',\n 'The pickle on the floor lay in the pickle jar under the bed.',\n 'The pickle on the floor lay in the pickle jar under the chair.',\n 'The pickle on the floor lay in the pickle jar under the table.']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the file into a 'pandas' Series\n",
    "file_path = 'Resource/sentences.txt'\n",
    "\n",
    "# Reading the file line by line to treat each line as a tokenized sentence\n",
    "with open(file_path, 'r') as f:\n",
    "    sentences = f.readlines()\n",
    "\n",
    "# Remove newline characters\n",
    "sentences = [sentence.strip() for sentence in sentences]\n",
    "\n",
    "# Display the first few sentences for verification\n",
    "sentences[-5:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:07:51.409029300Z",
     "start_time": "2023-10-30T14:07:51.399862400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def convert_sentences_to_tokens(sentences_list: list, token_dict: dict) -> list:\n",
    "    convert_token_arrays = []\n",
    "    reversed_token_dict = {v: k for k, v in token_dict.items()}  # Reverse the token dictionary for look-up\n",
    "\n",
    "    for sentence in sentences_list:\n",
    "        token_array = []\n",
    "        words = sentence.split()\n",
    "\n",
    "        for w in words:\n",
    "            clean_word = w.lower().rstrip('.')\n",
    "            t = reversed_token_dict.get(clean_word)  # Remove a trailing period and convert to lowercase\n",
    "            if t:\n",
    "                token_array.append(t)\n",
    "            else:\n",
    "                token_array.append(\"[Unknown]\")\n",
    "\n",
    "        # Add the 'U' token for the period at the end of the sentence, if applicable\n",
    "        if sentence.endswith('.'):\n",
    "            token_array.append('U')\n",
    "\n",
    "        convert_token_arrays.append(token_array)\n",
    "\n",
    "    return convert_token_arrays"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:07:52.794290600Z",
     "start_time": "2023-10-30T14:07:52.788243100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[['S', 'O', 'T', 'S', 'R', 'L', 'J', 'S', 'O', 'K', 'U'],\n ['T', 'S', 'R', 'Q', 'S', 'O', 'J', 'S', 'O', 'K', 'U'],\n ['S', 'O', 'N', 'S', 'I', 'L', 'J', 'S', 'O', 'K', 'T', 'S', 'C', 'U'],\n ['S', 'O', 'N', 'S', 'I', 'L', 'J', 'S', 'O', 'K', 'T', 'S', 'F', 'U'],\n ['S', 'O', 'N', 'S', 'I', 'L', 'J', 'S', 'O', 'K', 'T', 'S', 'R', 'U']]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "token_arrays = convert_sentences_to_tokens(sentences, token_dictionary)\n",
    "token_arrays[-5:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:07:53.448247700Z",
     "start_time": "2023-10-30T14:07:53.424941400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "outputs": [
    {
     "data": {
      "text/plain": "'Resource/tokenized_sentences_output.txt'"
     },
     "execution_count": 775,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the token arrays to strings\n",
    "token_strings = [''.join(token_array) for token_array in token_arrays]\n",
    "\n",
    "# Write the strings to a txt file\n",
    "output_file_path = 'Resource/tokenized_sentences_output.txt'\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for token_string in token_strings:\n",
    "        f.write(f\"{token_string}\\n\")\n",
    "\n",
    "output_file_path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:42:08.214903800Z",
     "start_time": "2023-10-30T13:42:08.196937200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "['SOTSRLJSOKU\\n',\n 'TSRQSOJSOKU\\n',\n 'SONSILJSOKTSCU\\n',\n 'SONSILJSOKTSFU\\n',\n 'SONSILJSOKTSRU\\n']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the file into a pandas.Series\n",
    "file_path = 'Resource/tokenized_sentences_output.txt'\n",
    "\n",
    "# Reading the file line by line to treat each line as a tokenized sentence\n",
    "with open(file_path, 'r') as f:\n",
    "    tokenized_sentences = f.readlines()\n",
    "\n",
    "tokenized_sentences[-5:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T14:08:06.024458700Z",
     "start_time": "2023-10-30T14:08:06.008651100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "outputs": [
    {
     "data": {
      "text/plain": "0                SDBU\n1                SDLU\n2                SDQU\n3                SEBU\n4                SELU\n            ...      \n708       SOTSRLJSOKU\n709       TSRQSOJSOKU\n710    SONSILJSOKTSCU\n711    SONSILJSOKTSFU\n712    SONSILJSOKTSRU\nLength: 713, dtype: object"
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Converting the list of tokenized sentences to a pandas.Series\n",
    "tokenized_sentences_series = pd.Series(tokenized_sentences).map(lambda x: x.strip('\\n'))\n",
    "\n",
    "# Displaying the first few entries to get an idea of the content\n",
    "tokenized_sentences_series"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:12:35.618498900Z",
     "start_time": "2023-10-30T13:12:35.572907600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "outputs": [
    {
     "data": {
      "text/plain": "     X_sequences y_tokens\n0            SDB        U\n1            SDL        U\n2            SDQ        U\n3            SEB        U\n4            SEL        U\n...          ...      ...\n4331         JSO        K\n4332         SOK        T\n4333         OKT        S\n4334         KTS        R\n4335         TSR        U\n\n[4336 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X_sequences</th>\n      <th>y_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SDB</td>\n      <td>U</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SDL</td>\n      <td>U</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SDQ</td>\n      <td>U</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SEB</td>\n      <td>U</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SEL</td>\n      <td>U</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4331</th>\n      <td>JSO</td>\n      <td>K</td>\n    </tr>\n    <tr>\n      <th>4332</th>\n      <td>SOK</td>\n      <td>T</td>\n    </tr>\n    <tr>\n      <th>4333</th>\n      <td>OKT</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4334</th>\n      <td>KTS</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>4335</th>\n      <td>TSR</td>\n      <td>U</td>\n    </tr>\n  </tbody>\n</table>\n<p>4336 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating sequences of four characters where the first three characters are the input (X), and the 4th character is what we want to predict (y)\n",
    "X_sequences = []\n",
    "y_tokens = []\n",
    "\n",
    "# Loop through each string in the Series\n",
    "for string in tokenized_sentences_series:\n",
    "    # Create sequences of four characters\n",
    "    for i in range(len(string) - 3):\n",
    "        X_sequences.append(string[i:i + 3])\n",
    "        y_tokens.append(string[i + 3])\n",
    "\n",
    "# Converting to pandas.Series for easier manipulation later\n",
    "X_series = pd.Series(X_sequences)\n",
    "y_series = pd.Series(y_tokens)\n",
    "\n",
    "# Displaying some sample X sequences and corresponding y tokens for verification\n",
    "sample_X_sequences = X_series\n",
    "sample_y_tokens = y_series\n",
    "\n",
    "# Converting the X_series and y_series into a pandas.DataFrame\n",
    "conversation_df = pd.DataFrame({\n",
    "    'X_sequences': X_series,\n",
    "    'y_tokens': y_series\n",
    "})\n",
    "\n",
    "# Displaying the first few rows of the DataFrame for verification\n",
    "conversation_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:12:35.618498900Z",
     "start_time": "2023-10-30T13:12:35.582664500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "outputs": [
    {
     "data": {
      "text/plain": "S    555\nN     92\nJ     49\nT     11\nA      6\nName: count, dtype: int64"
     },
     "execution_count": 715,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the occurrences of the first character in each sequence in the Series\n",
    "first_char_counts = tokenized_sentences_series.apply(lambda x: x[0]).value_counts()\n",
    "\n",
    "# Displaying the frequency table for the first character in each sequence\n",
    "first_char_counts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:12:35.721317200Z",
     "start_time": "2023-10-30T13:12:35.601662400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "def get_token_and_chance(chance_dict: dict, decimals: int = 2, token_dict: dict=token_dictionary) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get each token's corresponding word and its chance to be the first character in a sequence.\n",
    "\n",
    "    Parameters:\n",
    "    - token_dict (dict): Dictionary that maps tokens to words.\n",
    "    - decimals (int): Number of decimal places for rounding the chances. Default is 2.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of lists where each inner list contains the word and its rounded chance.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame([[token_dict.get(key[-1], f\"[Unknown: {key}]\"), np.around(chance_dict.get(key, 0), decimals=decimals)] for key in chance_dict], columns=['Word', 'Chance'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:12:35.721317200Z",
     "start_time": "2023-10-30T13:12:35.617498900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_first_n_char_chance(n: int, start_with: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Get the chance for each sequence of N characters to be the first N characters in a sequence,\n",
    "    optionally filtering sequences that start with a given string.\n",
    "\n",
    "    Parameters:\n",
    "    - n (int): The length of the character sequence.\n",
    "    - start_with (str, optional): A string that the sequence should start with.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary showing the chance for each sequence of N characters to be the first ones in a sequence.\n",
    "    \"\"\"\n",
    "    if start_with:\n",
    "        # Count the occurrences of each sequence of first N characters in the sequences, filtering by the start_with string\n",
    "        counter = Counter(tokenized_sentences_series.apply(lambda x: x[:n] if (len(x) >= n and x.startswith(start_with)) else None).dropna())\n",
    "    else:\n",
    "        # Count the occurrences of each sequence of first N characters in the sequences without filtering\n",
    "        counter = Counter(tokenized_sentences_series.apply(lambda x: x[:n] if len(x) >= n else None).dropna())\n",
    "\n",
    "    # Calculate the chance for each sequence\n",
    "    total_count = sum(counter.values())\n",
    "    chance_dict = {char_seq: count / total_count for char_seq, count in counter.items()}\n",
    "\n",
    "    return chance_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:12:35.721317200Z",
     "start_time": "2023-10-30T13:12:35.621498800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# Modify the function to handle token arrays of length less than 3 by adding 'U' at the front until the length is 3\n",
    "def predict_next_step(triplet: list, df: pd.DataFrame, token_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    Predict the next character based on different conditions:\n",
    "    1. If the triplet exists in the DataFrame, select y based on occurrences in that subset.\n",
    "    2. If the triplet doesn't exist, try getting the chance to start with the last two characters of the triplet.\n",
    "    3. If that fails, try getting the chance to start with the last character of the triplet.\n",
    "    4. If all else fails, start a random conversation.\n",
    "\n",
    "    Parameters:\n",
    "    - triplet (list): List of three characters that form the sequence to predict the next character for.\n",
    "    - df (pd.DataFrame): DataFrame containing the sequences and corresponding tokens to predict.\n",
    "    - token_dict (dict): Dictionary that maps tokens to words.\n",
    "\n",
    "    Returns:\n",
    "    - str: The next character based on one of the conditions above.\n",
    "    \"\"\"\n",
    "    # Add 'U' at the front until the length is 3\n",
    "    while len(triplet) < 3:\n",
    "        triplet.insert(0, 'U')\n",
    "\n",
    "    triplet_str = ''.join(triplet)\n",
    "\n",
    "    # Condition 1: Triplet exists in DataFrame\n",
    "    if triplet_str in df['X_sequences'].values:\n",
    "        print(f'Continue With {[token_dict.get(triplet_str[n], f\"[Unknown: {triplet_str[n]}]\") for n in range(0, 3)]}')\n",
    "        # Subset DataFrame and calculate chance\n",
    "        subset_y = df[df['X_sequences'] == triplet_str]['y_tokens']\n",
    "        counter = Counter(subset_y)\n",
    "        total_count = sum(counter.values())\n",
    "        chance_dict = {char: count / total_count for char, count in counter.items()}\n",
    "        print(f'{get_token_and_chance(chance_dict)}\\n')\n",
    "        characters = list(chance_dict.keys())\n",
    "        probabilities = list(chance_dict.values())\n",
    "        return random.choices(characters, probabilities)[0]\n",
    "\n",
    "    # Condition 2: Try getting chance starting with the last two characters of triplet\n",
    "    chance_dict = get_first_n_char_chance(3, start_with=triplet_str[1:])\n",
    "    if chance_dict:\n",
    "        print(f'Start Conversation With {[token_dict.get(triplet_str[n], f\"[Unknown: {triplet_str[n]}]\") for n in range(1, 3)]}')\n",
    "        print(f'{get_token_and_chance(chance_dict)}\\n')\n",
    "        characters = list(chance_dict.keys())\n",
    "        probabilities = list(chance_dict.values())\n",
    "        return random.choices(characters, probabilities)[0][-1]  # Take the last character of the chosen triplet\n",
    "\n",
    "    # Condition 3: Try getting chance starting with last character of triplet\n",
    "    chance_dict = get_first_n_char_chance(2, start_with=triplet_str[-1])\n",
    "    if chance_dict:\n",
    "        print(f'Start Conversation With {[token_dict.get(triplet_str[-1], f\"[Unknown: {triplet_str[-1]}]\")]}')\n",
    "        print(f'{get_token_and_chance(chance_dict)}\\n')\n",
    "        characters = list(chance_dict.keys())\n",
    "        probabilities = list(chance_dict.values())\n",
    "        return random.choices(characters, probabilities)[0][-1]  # Take the last character of the chosen triplet\n",
    "\n",
    "    print(f'Start A Random Conversation!')\n",
    "    chance_dict = get_first_n_char_chance(1)\n",
    "    print(f'{get_token_and_chance(chance_dict)}\\n')\n",
    "    characters = list(chance_dict.keys())\n",
    "    probabilities = list(chance_dict.values())\n",
    "    return random.choices(characters, probabilities)[0][-1]  # Take the last character of the chosen triplet"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:27:11.510742400Z",
     "start_time": "2023-10-30T13:27:11.467940800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start A Random Conversation!\n",
      "    Word  Chance\n",
      "0    the    0.78\n",
      "1     at    0.01\n",
      "2     in    0.07\n",
      "3     on    0.13\n",
      "4  under    0.02\n",
      "\n",
      "Select word: the\n"
     ]
    }
   ],
   "source": [
    "# Test the function with various conditions\n",
    "predict_token_0 = predict_next_step(['U', 'U', 'U'], conversation_df, token_dictionary)\n",
    "print(f'Select word: {token_dictionary.get(predict_token_0, f\"[Unknown: {predict_token_0}]\")}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:22:37.709784100Z",
     "start_time": "2023-10-30T13:22:37.686729600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Conversation With ['the']\n",
      "            Word  Chance\n",
      "0            boy    0.23\n",
      "1            cat    0.21\n",
      "2            dog    0.12\n",
      "3            jar    0.06\n",
      "4          mouse    0.07\n",
      "5         pickle    0.27\n",
      "6            bed    0.00\n",
      "7          chair    0.01\n",
      "8   refrigerator    0.00\n",
      "9          table    0.00\n",
      "10         floor    0.01\n",
      "\n",
      "Select word: cat\n"
     ]
    }
   ],
   "source": [
    "# Test the function with various conditions\n",
    "predict_token_1 = predict_next_step(['U', 'U', predict_token_0], conversation_df, token_dictionary)\n",
    "print(f'Select word: {token_dictionary.get(predict_token_1, f\"[Unknown: {predict_token_1}]\")}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:22:38.380679Z",
     "start_time": "2023-10-30T13:22:38.351415300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Conversation With ['the', 'cat']\n",
      "     Word  Chance\n",
      "0     ate    0.09\n",
      "1     lay    0.13\n",
      "2     sat    0.13\n",
      "3  chased    0.07\n",
      "4      at    0.03\n",
      "5      in    0.01\n",
      "6      on    0.39\n",
      "7   under    0.15\n",
      "\n",
      "Select word: ate\n"
     ]
    }
   ],
   "source": [
    "# Test the function with various conditions\n",
    "predict_token_2 = predict_next_step(['U', predict_token_0, predict_token_1], conversation_df, token_dictionary)\n",
    "print(f'Select word: {token_dictionary.get(predict_token_2, f\"[Unknown: {predict_token_2}]\")}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:22:38.722207900Z",
     "start_time": "2023-10-30T13:22:38.698239800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue With ['the', 'cat', 'ate']\n",
      "    Word  Chance\n",
      "0      .    0.09\n",
      "1    the    0.45\n",
      "2     on    0.18\n",
      "3  under    0.27\n",
      "\n",
      "Select word: the\n"
     ]
    }
   ],
   "source": [
    "# Test the function with various conditions\n",
    "predict_token = predict_next_step([predict_token_0, predict_token_1, predict_token_2], conversation_df, token_dictionary)\n",
    "print(f'Select word: {token_dictionary.get(predict_token, f\"[Unknown: {predict_token}]\")}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:22:39.197211800Z",
     "start_time": "2023-10-30T13:22:39.183859400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "outputs": [],
   "source": [
    "def continue_conversation_until_end(conversation: list, df: pd.DataFrame, token_dict: dict) -> list:\n",
    "    \"\"\"\n",
    "    Continue an existing conversation by generating next characters until 'U' is generated.\n",
    "\n",
    "    Parameters:\n",
    "    - conversation (list): List of characters that form the existing conversation.\n",
    "    - df (pd.DataFrame): DataFrame containing the sequences and corresponding tokens to predict.\n",
    "\n",
    "    Returns:\n",
    "    - list: The extended conversation including new characters.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # Take the last three characters from the conversation to predict the next character\n",
    "        last_triplet = conversation[-3:]\n",
    "\n",
    "        # Predict the next character or start a new conversation if the triplet doesn't exist\n",
    "        next_char = predict_next_step(last_triplet, df, token_dict)\n",
    "\n",
    "        # Print the selected character\n",
    "        print(f'Select token {next_char}: {token_dict.get(next_char, f\"[Unknown: {next_char}]\")}\\n')\n",
    "\n",
    "        # Append the predicted character to the conversation\n",
    "        conversation.append(next_char)\n",
    "\n",
    "        # Break the loop if the predicted character is 'U'\n",
    "        if next_char == 'U':\n",
    "            break\n",
    "\n",
    "    return conversation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:22:39.916587400Z",
     "start_time": "2023-10-30T13:22:39.910892200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start A Random Conversation!\n",
      "    Word  Chance\n",
      "0    the    0.78\n",
      "1     at    0.01\n",
      "2     in    0.07\n",
      "3     on    0.13\n",
      "4  under    0.02\n",
      "\n",
      "Select token T: under\n",
      "\n",
      "Start Conversation With ['under']\n",
      "  Word  Chance\n",
      "0  the     1.0\n",
      "\n",
      "Select token S: the\n",
      "\n",
      "Start Conversation With ['under', 'the']\n",
      "    Word  Chance\n",
      "0  table     1.0\n",
      "\n",
      "Select token R: table\n",
      "\n",
      "Continue With ['under', 'the', 'table']\n",
      "     Word  Chance\n",
      "0     ate    0.10\n",
      "1     lay    0.16\n",
      "2     sat    0.19\n",
      "3       .    0.53\n",
      "4  chased    0.01\n",
      "5      on    0.02\n",
      "\n",
      "Select token U: .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the function with the modified predict_next_step function\n",
    "initial_conversation = ['U', 'U', 'U']\n",
    "extended_conversation = continue_conversation_until_end(initial_conversation.copy(), conversation_df, token_dictionary)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:22:53.457692Z",
     "start_time": "2023-10-30T13:22:53.444982600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "outputs": [
    {
     "data": {
      "text/plain": "['U', 'U', 'U', 'T', 'S', 'R', 'U']"
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the extended conversation\n",
    "extended_conversation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:22:54.472715900Z",
     "start_time": "2023-10-30T13:22:54.467497300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "outputs": [],
   "source": [
    "# Redefine the function to decode a token array back to words and make the first word's first letter uppercase\n",
    "def decode_tokens_to_words(token_array: list, token_dict: dict) -> str:\n",
    "    \"\"\"\n",
    "    Decode an array of tokens back to a string of words based on a given token dictionary.\n",
    "    The first word in the sentence (that is not a period) will have its first letter capitalized.\n",
    "\n",
    "    Parameters:\n",
    "    - token_array (list): Array of tokens to decode.\n",
    "    - token_dict (dict): Dictionary that maps tokens to words.\n",
    "\n",
    "    Returns:\n",
    "    - str: A string of words obtained by decoding the token array.\n",
    "    \"\"\"\n",
    "    words = [token_dict.get(t, f\"[Unknown: {t}]\") for t in token_array]\n",
    "\n",
    "    # Capitalize the first letter of the first word that is not a period\n",
    "    for n in range(len(words)):\n",
    "        if words[n] != '.':\n",
    "            words[n] = words[n].capitalize()\n",
    "            break\n",
    "\n",
    "    return ' '.join(words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:22:55.413472300Z",
     "start_time": "2023-10-30T13:22:55.405957900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "outputs": [
    {
     "data": {
      "text/plain": "'. . . Under the table .'"
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function again with the updated token dictionary\n",
    "decoded_sentence = decode_tokens_to_words(extended_conversation, token_dictionary)\n",
    "decoded_sentence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:22:56.246044700Z",
     "start_time": "2023-10-30T13:22:56.239984200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start A Random Conversation!\n",
      "    Word  Chance\n",
      "0    the    0.78\n",
      "1     at    0.01\n",
      "2     in    0.07\n",
      "3     on    0.13\n",
      "4  under    0.02\n",
      "\n",
      "Select token S: the\n",
      "\n",
      "Start Conversation With ['the']\n",
      "            Word  Chance\n",
      "0            boy    0.23\n",
      "1            cat    0.21\n",
      "2            dog    0.12\n",
      "3            jar    0.06\n",
      "4          mouse    0.07\n",
      "5         pickle    0.27\n",
      "6            bed    0.00\n",
      "7          chair    0.01\n",
      "8   refrigerator    0.00\n",
      "9          table    0.00\n",
      "10         floor    0.01\n",
      "\n",
      "Select token E: cat\n",
      "\n",
      "Start Conversation With ['the', 'cat']\n",
      "     Word  Chance\n",
      "0     ate    0.09\n",
      "1     lay    0.13\n",
      "2     sat    0.13\n",
      "3  chased    0.07\n",
      "4      at    0.03\n",
      "5      in    0.01\n",
      "6      on    0.39\n",
      "7   under    0.15\n",
      "\n",
      "Select token L: lay\n",
      "\n",
      "Continue With ['the', 'cat', 'lay']\n",
      "    Word  Chance\n",
      "0      .    0.06\n",
      "1     in    0.12\n",
      "2     on    0.62\n",
      "3  under    0.19\n",
      "\n",
      "Select token N: on\n",
      "\n",
      "Continue With ['cat', 'lay', 'on']\n",
      "  Word  Chance\n",
      "0  the     1.0\n",
      "\n",
      "Select token S: the\n",
      "\n",
      "Continue With ['lay', 'on', 'the']\n",
      "           Word  Chance\n",
      "0           bed    0.11\n",
      "1         floor    0.36\n",
      "2           boy    0.32\n",
      "3         chair    0.08\n",
      "4         table    0.08\n",
      "5  refrigerator    0.06\n",
      "\n",
      "Select token I: floor\n",
      "\n",
      "Continue With ['on', 'the', 'floor']\n",
      "     Word  Chance\n",
      "0       .    0.34\n",
      "1     ate    0.14\n",
      "2     lay    0.21\n",
      "3     sat    0.21\n",
      "4  chased    0.10\n",
      "\n",
      "Select token U: .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = decode_tokens_to_words(continue_conversation_until_end(['U', 'U', 'U'], conversation_df, token_dictionary), token_dictionary)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:22:57.936785100Z",
     "start_time": "2023-10-30T13:22:57.902652100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "outputs": [
    {
     "data": {
      "text/plain": "'. . . The cat lay on the floor .'"
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:22:58.805145800Z",
     "start_time": "2023-10-30T13:22:58.801040500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "outputs": [],
   "source": [
    "# Define the function to encode a sentence, extend the conversation, and then decode it back to words\n",
    "def encode_extend_decode(sentences_list: list, token_dict: dict, df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Encode a sentence into tokens, extend the conversation based on those tokens, and then decode it back into words.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence (str): The sentence to start the conversation with.\n",
    "    - token_dict (dict): Dictionary that maps tokens to words.\n",
    "    - df (pd.DataFrame): DataFrame containing the sequences and corresponding tokens to predict.\n",
    "\n",
    "    Returns:\n",
    "    - str: The extended conversation decoded back into words.\n",
    "    \"\"\"\n",
    "    # Step 1: Encode the sentence into tokens\n",
    "    encode_sentences = convert_sentences_to_tokens(sentences_list, token_dict)\n",
    "\n",
    "    # Step 2: Extend the conversation\n",
    "    extended_conversation_encode = [continue_conversation_until_end(encode_sentence, df, token_dict) for encode_sentence in encode_sentences]\n",
    "\n",
    "    # Step 3: Decode the tokens back to words\n",
    "    return [decode_tokens_to_words(encode, token_dict) for encode in extended_conversation_encode]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:22:59.639772600Z",
     "start_time": "2023-10-30T13:22:59.633773500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue With ['boy', 'ate', 'the']\n",
      "     Word  Chance\n",
      "0  pickle     1.0\n",
      "\n",
      "Select token O: pickle\n",
      "\n",
      "Continue With ['ate', 'the', 'pickle']\n",
      "    Word  Chance\n",
      "0      .    0.14\n",
      "1     in    0.76\n",
      "2     at    0.01\n",
      "3     on    0.05\n",
      "4  under    0.04\n",
      "\n",
      "Select token U: .\n",
      "\n",
      "Continue With ['the', 'dog', 'sat']\n",
      "    Word  Chance\n",
      "0      .     0.2\n",
      "1     in     0.2\n",
      "2     on     0.4\n",
      "3  under     0.2\n",
      "\n",
      "Select token T: under\n",
      "\n",
      "Continue With ['dog', 'sat', 'under']\n",
      "  Word  Chance\n",
      "0  the     1.0\n",
      "\n",
      "Select token S: the\n",
      "\n",
      "Continue With ['sat', 'under', 'the']\n",
      "           Word  Chance\n",
      "0           bed    0.33\n",
      "1         chair    0.18\n",
      "2         table    0.42\n",
      "3  refrigerator    0.06\n",
      "\n",
      "Select token R: table\n",
      "\n",
      "Continue With ['under', 'the', 'table']\n",
      "     Word  Chance\n",
      "0     ate    0.10\n",
      "1     lay    0.16\n",
      "2     sat    0.19\n",
      "3       .    0.53\n",
      "4  chased    0.01\n",
      "5      on    0.02\n",
      "\n",
      "Select token U: .\n",
      "\n",
      "Start Conversation With ['the', 'pickle']\n",
      "    Word  Chance\n",
      "0    lay    0.18\n",
      "1    sat    0.14\n",
      "2    jar    0.24\n",
      "3     on    0.21\n",
      "4  under    0.10\n",
      "5     in    0.14\n",
      "\n",
      "Select token N: on\n",
      "\n",
      "Continue With ['the', 'pickle', 'on']\n",
      "  Word  Chance\n",
      "0  the     1.0\n",
      "\n",
      "Select token S: the\n",
      "\n",
      "Continue With ['pickle', 'on', 'the']\n",
      "           Word  Chance\n",
      "0           bed    0.14\n",
      "1         chair    0.06\n",
      "2         floor    0.57\n",
      "3  refrigerator    0.06\n",
      "4         table    0.17\n",
      "\n",
      "Select token P: refrigerator\n",
      "\n",
      "Continue With ['on', 'the', 'refrigerator']\n",
      "  Word  Chance\n",
      "0  lay    0.38\n",
      "1  sat    0.44\n",
      "2    .    0.19\n",
      "\n",
      "Select token L: lay\n",
      "\n",
      "Continue With ['the', 'refrigerator', 'lay']\n",
      "  Word  Chance\n",
      "0    .    0.50\n",
      "1  the    0.45\n",
      "2   on    0.05\n",
      "\n",
      "Select token U: .\n",
      "\n",
      "Start Conversation With ['on']\n",
      "  Word  Chance\n",
      "0  the     1.0\n",
      "\n",
      "Select token S: the\n",
      "\n",
      "Start Conversation With ['on', 'the']\n",
      "           Word  Chance\n",
      "0           bed    0.26\n",
      "1           boy    0.27\n",
      "2         chair    0.13\n",
      "3         floor    0.12\n",
      "4  refrigerator    0.11\n",
      "5         table    0.11\n",
      "\n",
      "Select token D: boy\n",
      "\n",
      "Continue With ['on', 'the', 'boy']\n",
      "    Word  Chance\n",
      "0    lay    0.06\n",
      "1    sat    0.03\n",
      "2      .    0.10\n",
      "3     in    0.32\n",
      "4     on    0.25\n",
      "5  under    0.19\n",
      "6     at    0.04\n",
      "\n",
      "Select token N: on\n",
      "\n",
      "Continue With ['the', 'boy', 'on']\n",
      "  Word  Chance\n",
      "0  the     1.0\n",
      "\n",
      "Select token S: the\n",
      "\n",
      "Continue With ['boy', 'on', 'the']\n",
      "    Word  Chance\n",
      "0    bed    0.29\n",
      "1  floor    0.69\n",
      "2  chair    0.02\n",
      "\n",
      "Select token I: floor\n",
      "\n",
      "Continue With ['on', 'the', 'floor']\n",
      "     Word  Chance\n",
      "0       .    0.34\n",
      "1     ate    0.14\n",
      "2     lay    0.21\n",
      "3     sat    0.21\n",
      "4  chased    0.10\n",
      "\n",
      "Select token B: ate\n",
      "\n",
      "Continue With ['the', 'floor', 'ate']\n",
      "    Word  Chance\n",
      "0      .    0.12\n",
      "1    the    0.77\n",
      "2  under    0.12\n",
      "\n",
      "Select token T: under\n",
      "\n",
      "Continue With ['floor', 'ate', 'under']\n",
      "  Word  Chance\n",
      "0  the     1.0\n",
      "\n",
      "Select token S: the\n",
      "\n",
      "Continue With ['ate', 'under', 'the']\n",
      "    Word  Chance\n",
      "0  chair    0.22\n",
      "1  table    0.44\n",
      "2    bed    0.33\n",
      "\n",
      "Select token R: table\n",
      "\n",
      "Continue With ['under', 'the', 'table']\n",
      "     Word  Chance\n",
      "0     ate    0.10\n",
      "1     lay    0.16\n",
      "2     sat    0.19\n",
      "3       .    0.53\n",
      "4  chased    0.01\n",
      "5      on    0.02\n",
      "\n",
      "Select token Q: sat\n",
      "\n",
      "Continue With ['the', 'table', 'sat']\n",
      "  Word  Chance\n",
      "0  the    0.56\n",
      "1    .    0.24\n",
      "2   on    0.13\n",
      "3   in    0.07\n",
      "\n",
      "Select token S: the\n",
      "\n",
      "Continue With ['table', 'sat', 'the']\n",
      "     Word  Chance\n",
      "0     boy    0.17\n",
      "1     cat    0.30\n",
      "2     jar    0.07\n",
      "3  pickle    0.40\n",
      "4     dog    0.03\n",
      "5   mouse    0.03\n",
      "\n",
      "Select token D: boy\n",
      "\n",
      "Continue With ['sat', 'the', 'boy']\n",
      "  Word  Chance\n",
      "0    .    0.71\n",
      "1   in    0.14\n",
      "2   on    0.14\n",
      "\n",
      "Select token U: .\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "['The boy ate the pickle .',\n 'The dog sat under the table .',\n 'The pickle on the refrigerator lay .',\n 'On the boy on the floor ate under the table sat the boy .']"
     },
     "execution_count": 766,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_extend_decode(['The boy ate the', 'The dog sat', 'The pickle', 'On'], token_dictionary, conversation_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T13:31:46.797241600Z",
     "start_time": "2023-10-30T13:31:46.754647100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
