{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Build the speech model\n",
    "\n",
    "Now that we've created spectrogram images, it's time to build the computer vision model. If you're following along with the different modules in this PyTorch learning path, then you should have a good understanding of how to create a computer vision model (in particular, see the \"Introduction to Computer Vision with PyTorch\" Learn module). You'll be using the `torchvision` package to build your vision model. The convolutional neural network (CNN) layer (`conv2d`) will be used to extract the unique features from the spectrogram image for each speech command.\n",
    "\n",
    "Let's import the packages we need to build the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 建立語音模型\n",
    "\n",
    "現在我們已經創建了頻譜圖像，是時候建立計算機視覺模型了。如果您正在按照這個 PyTorch 學習路徑中的不同模塊進行學習，那麼您應該對如何創建計算機視覺模型有一個良好的理解（特別是請參考 \"使用 PyTorch 進行計算機視覺入門\" 學習模塊）。您將使用 torchvision 套件來建立您的視覺模型。卷積神經網絡（CNN）層（conv2d）將用於從每個語音命令的頻譜圖像中提取獨特的特徵。\n",
    "\n",
    "讓我們導入我們建立模型所需的套件。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "json"
    },
    "ExecuteTime": {
     "end_time": "2023-10-08T11:57:34.550017Z",
     "start_time": "2023-10-08T11:57:29.753565500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load spectrogram images into a data loader for training\n",
    "\n",
    "Here, we provide the path to our image data and use PyTorch's `ImageFolder` dataset helper class to load the images into tensors. We'll also normalize the images by resizing to a dimension of 201 x 81."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 將頻譜圖像加載到用於訓練的數據加載器中\n",
    "\n",
    "在這裡，我們提供了圖像數據的路徑，並使用 PyTorch 的 `ImageFolder` 數據集輔助類別將圖像加載為張量。我們還將對圖像進行歸一化，將其調整為 201 x 81 的維度。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data_path = './data/spectrograms' #looking in subfolder train\n",
    "\n",
    "yes_no_dataset = datasets.ImageFolder(\n",
    "    root=data_path,\n",
    "    transform=transforms.Compose([transforms.Resize((201,81)),\n",
    "                                  transforms.ToTensor()\n",
    "                                  ])\n",
    ")\n",
    "print(yes_no_dataset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 7985\n",
      "    Root location: ./data/spectrograms\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(201, 81), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "json"
    },
    "ExecuteTime": {
     "end_time": "2023-10-08T11:57:45.079022800Z",
     "start_time": "2023-10-08T11:57:45.018434800Z"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "source": [
    "`ImageFolder` automatically creates the image class labels and indices based on the folders for each audio class.  We'll use the `class_to_idx` to view the class mapping for the image dataset.\n",
    "\n",
    "<img alt=\"Folder class index diagram\" src=\"images/4-model-1.png\" align=\"middle\" />"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class_map=yes_no_dataset.class_to_idx\n",
    "\n",
    "print(\"\\nClass category and index of the images: {}\\n\".format(class_map))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class category and index of the images: {'no': 0, 'yes': 1}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T11:57:47.836030100Z",
     "start_time": "2023-10-08T11:57:47.821569600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split the data for training and testing\n",
    "We'll need to split the data to use 80 percent to train the model, and 20 percent to test."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "#split data to test and train\n",
    "#use 80% to train\n",
    "train_size = int(0.8 * len(yes_no_dataset))\n",
    "test_size = len(yes_no_dataset) - train_size\n",
    "yes_no_train_dataset, yes_no_test_dataset = torch.utils.data.random_split(yes_no_dataset, [train_size, test_size])\n",
    "\n",
    "print(\"Training size:\", len(yes_no_train_dataset))\n",
    "print(\"Testing size:\",len(yes_no_test_dataset))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 6388\n",
      "Testing size: 1597\n"
     ]
    }
   ],
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T11:58:17.048707Z",
     "start_time": "2023-10-08T11:58:17.016375800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because the dataset was randomly split, let's count the training data to verify that the data has a fairly even distribution between the images in the `yes` and \n",
    "`no` categories."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "# labels in training set\n",
    "train_classes = [label for _, label in yes_no_train_dataset]\n",
    "Counter(train_classes)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": "Counter({1: 3248, 0: 3140})"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T11:58:38.463403200Z",
     "start_time": "2023-10-08T11:58:29.036722Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the data into the `DataLoader` and specify the batch size of how the data will be divided and loaded in the training iterations. We'll also set the number of workers to specify the number of subprocesses to load the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    yes_no_train_dataset,\n",
    "    batch_size=15,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    yes_no_test_dataset,\n",
    "    batch_size=15,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T11:59:54.615024400Z",
     "start_time": "2023-10-08T11:59:54.605461200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's take a look at what our training tensor looks like:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "td = train_dataloader.dataset[0][0][0][0]\n",
    "print(td)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T11:59:55.929650300Z",
     "start_time": "2023-10-08T11:59:55.861071400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get GPU for training, or use CPU if GPU isn't available."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T11:59:59.474457Z",
     "start_time": "2023-10-08T11:59:58.227763600Z"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "source": [
    "#### Create the convolutional neural network\n",
    "\n",
    "\n",
    "[ ![Diagram showing a convolutional neural network.](./images/4-model-2.png) ](./images/4-model-2.png#lightbox)\n",
    "\n",
    "We'll define our layers and parameters:\n",
    "\n",
    "- `conv2d`: Takes an input of 3 `channels`, which represents RGB colors because our input images are in color. The 32 represents the number of feature map images produced from the convolutional layer. The images are produced after you apply a filter on each image in a channel, with a 5 x 5 kernel size and a stride of 1. `Max pooling` is set with a 2 x 2 kernel size to reduce the dimensions of the filtered images. We apply the `ReLU` activation to replace the negative pixel values to 0.\n",
    "- `conv2d`: Takes the 32 output images from the previous convolutional layer as input. Then, we increase the output number to 64 feature map images, after a filter is applied on the 32 input images, with a 5 x 5 kernel size and a stride of 1. `Max pooling` is set with a 2 x 2 kernel size to reduce the dimensions of the filtered images. We apply the `ReLU` activation to replace the negative pixel values to 0.\n",
    "- `dropout`: Removes some of the features extracted from the `conv2d` layer with the ratio of 0.50, to prevent overfitting.\n",
    "- `flatten`: Converts features from the `conv2d` output image into the linear input layer.\n",
    "- `Linear`: Takes a number of 51136 features as input, and sets the number of outputs from the network to be 50 logits. The next layer will take the 50 inputs and produces 2 logits in the output layer. The `ReLU` activation function will be applied to the neurons across the linear network to replace the negative values to 0. The 2 output values will be used to predict the classification `yes` or `no`.  \n",
    "- `log_Softmax`: An activation function applied to the 2 output values to predict the probability of the audio classification.\n",
    "\n",
    "After defining the CNN, we'll set the device to run it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 創建卷積神經網絡\n",
    "\n",
    "我們將定義我們的層和參數：\n",
    "\n",
    "- `conv2d`：接受 3 個 `通道` 的輸入，表示 RGB 顏色，因為我們的輸入圖像是彩色的。32 表示從卷積層產生的特徵圖圖像數。這些圖像是在每個通道的每個圖像上應用 5 x 5 的內核大小和步長為 1 後生成的。我們設置了 2 x 2 的內核大小進行最大池化，以減小過濾圖像的尺寸。我們應用 `ReLU` 激活函數以將負像素值替換為 0。\n",
    "- `conv2d`：將前一個卷積層的 32 個輸出圖像作為輸入。然後，我們將輸出數量增加到 64 個特徵圖圖像，經過 32 個輸入圖像應用 5 x 5 的內核大小和步長 1 的過濾器後。我們設置了 2 x 2 的內核大小進行最大池化，以減小過濾圖像的尺寸。我們應用 `ReLU` 激活函數以將負像素值替換為 0。\n",
    "- `dropout`：以 0.50 的比率從 `conv2d` 層中刪除一些特徵，以防止過擬合。\n",
    "- `flatten`：將 `conv2d` 輸出圖像的特徵轉換為線性輸入層。\n",
    "- `Linear`：以 51136 個特徵作為輸入，並將網絡的輸出數量設置為 50 個 logits。下一層將接受 50 個輸入並在輸出層中生成 2 個 logits。我們將應用 `ReLU` 激活函數以將線性網絡上的神經元的負值替換為 0。2 個輸出值將用於預測分類 `yes` 或 `no`。\n",
    "- `log_Softmax`：應用於 2 個輸出值以預測音訊分類的概率。\n",
    "\n",
    "在定義完 CNN 後，我們將設置運行它的設備。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class CNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(51136, 50)\n",
    "        self.fc2 = nn.Linear(50, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x,dim=1)  \n",
    "\n",
    "model = CNNet().to(device)"
   ],
   "outputs": [],
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "json"
    },
    "ExecuteTime": {
     "end_time": "2023-10-08T12:00:08.032129800Z",
     "start_time": "2023-10-08T12:00:06.757375600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create train and test functions\n",
    "\n",
    "Now you set the cost function, learning rate, and optimizer. Then you define the train and test functions that you'll use to train and test the model by using the CNN."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 創建訓練和測試函數\n",
    "\n",
    "現在，您設置了成本函數、學習速率和優化器。然後，您定義了將用於使用 CNN 訓練和測試模型的訓練和測試函數。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# cost function used to determine best parameters\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# used to create optimal parameters\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training function\n",
    "\n",
    "def train(dataloader, model, loss, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "        \n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = cost(pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "\n",
    "# Create the validation/test function\n",
    "\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, Y) in enumerate(dataloader):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += cost(pred, Y).item()\n",
    "            correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "\n",
    "    print(f'\\nTest Error:\\nacc: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\\n')"
   ],
   "outputs": [],
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "json"
    },
    "ExecuteTime": {
     "end_time": "2023-10-08T05:49:17.270525700Z",
     "start_time": "2023-10-08T05:49:17.261915400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the model\n",
    "\n",
    "Now let's set the number of epochs, and call our `train` and `test` functions for each iteration. We'll iterate through the training network by the number of epochs.  As we train the model, we'll calculate the loss as it decreases during the training. In addition, we'll display the accuracy as the optimization increases."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 訓練模型\n",
    "\n",
    "現在讓我們設置 epoch 的數量，並為每個迭代調用我們的 `train` 和 `test` 函數。我們將根據 epoch 的數量來迭代訓練網絡。在訓練模型時，我們將計算訓練過程中損失的下降。此外，我們將顯示隨著優化的增加而提高的準確性。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "epochs = 15\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t+1}\\n-------------------------------')\n",
    "    train(train_dataloader, model, cost, optimizer)\n",
    "    test(test_dataloader, model)\n",
    "print('Done!')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.692127  [    0/ 6388]\n",
      "loss: 0.395299  [ 1500/ 6388]\n",
      "loss: 0.253122  [ 3000/ 6388]\n",
      "loss: 0.317790  [ 4500/ 6388]\n",
      "loss: 0.561404  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 89.9%, avg loss: 0.016103\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.234733  [    0/ 6388]\n",
      "loss: 0.351605  [ 1500/ 6388]\n",
      "loss: 0.123859  [ 3000/ 6388]\n",
      "loss: 0.193750  [ 4500/ 6388]\n",
      "loss: 0.217090  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 90.5%, avg loss: 0.014598\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.066517  [    0/ 6388]\n",
      "loss: 0.086646  [ 1500/ 6388]\n",
      "loss: 0.120797  [ 3000/ 6388]\n",
      "loss: 0.072816  [ 4500/ 6388]\n",
      "loss: 0.119996  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 92.0%, avg loss: 0.013056\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.124941  [    0/ 6388]\n",
      "loss: 0.057019  [ 1500/ 6388]\n",
      "loss: 0.118088  [ 3000/ 6388]\n",
      "loss: 0.026632  [ 4500/ 6388]\n",
      "loss: 0.032528  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 93.0%, avg loss: 0.011396\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.121537  [    0/ 6388]\n",
      "loss: 0.134929  [ 1500/ 6388]\n",
      "loss: 0.089382  [ 3000/ 6388]\n",
      "loss: 0.133298  [ 4500/ 6388]\n",
      "loss: 0.061350  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 93.4%, avg loss: 0.011264\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.091900  [    0/ 6388]\n",
      "loss: 0.105359  [ 1500/ 6388]\n",
      "loss: 0.383063  [ 3000/ 6388]\n",
      "loss: 0.164096  [ 4500/ 6388]\n",
      "loss: 0.072453  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 92.4%, avg loss: 0.011683\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.090978  [    0/ 6388]\n",
      "loss: 0.123314  [ 1500/ 6388]\n",
      "loss: 0.113162  [ 3000/ 6388]\n",
      "loss: 0.081644  [ 4500/ 6388]\n",
      "loss: 0.122384  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 93.3%, avg loss: 0.010664\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.187051  [    0/ 6388]\n",
      "loss: 0.091124  [ 1500/ 6388]\n",
      "loss: 0.011458  [ 3000/ 6388]\n",
      "loss: 0.050380  [ 4500/ 6388]\n",
      "loss: 0.016378  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 93.6%, avg loss: 0.009943\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.145997  [    0/ 6388]\n",
      "loss: 0.119765  [ 1500/ 6388]\n",
      "loss: 0.074022  [ 3000/ 6388]\n",
      "loss: 0.077225  [ 4500/ 6388]\n",
      "loss: 0.188207  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 93.6%, avg loss: 0.009978\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.076447  [    0/ 6388]\n",
      "loss: 0.215744  [ 1500/ 6388]\n",
      "loss: 0.185538  [ 3000/ 6388]\n",
      "loss: 0.104665  [ 4500/ 6388]\n",
      "loss: 0.030221  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 93.7%, avg loss: 0.009929\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.064107  [    0/ 6388]\n",
      "loss: 0.191535  [ 1500/ 6388]\n",
      "loss: 0.057042  [ 3000/ 6388]\n",
      "loss: 0.041256  [ 4500/ 6388]\n",
      "loss: 0.047522  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 94.5%, avg loss: 0.009008\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.040953  [    0/ 6388]\n",
      "loss: 0.055865  [ 1500/ 6388]\n",
      "loss: 0.203066  [ 3000/ 6388]\n",
      "loss: 0.138323  [ 4500/ 6388]\n",
      "loss: 0.056579  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 94.2%, avg loss: 0.008757\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.057855  [    0/ 6388]\n",
      "loss: 0.146322  [ 1500/ 6388]\n",
      "loss: 0.056992  [ 3000/ 6388]\n",
      "loss: 0.154130  [ 4500/ 6388]\n",
      "loss: 0.029365  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 94.2%, avg loss: 0.008882\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.247138  [    0/ 6388]\n",
      "loss: 0.168360  [ 1500/ 6388]\n",
      "loss: 0.087981  [ 3000/ 6388]\n",
      "loss: 0.067601  [ 4500/ 6388]\n",
      "loss: 0.083091  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 94.1%, avg loss: 0.009421\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.231350  [    0/ 6388]\n",
      "loss: 0.050163  [ 1500/ 6388]\n",
      "loss: 0.040067  [ 3000/ 6388]\n",
      "loss: 0.111019  [ 4500/ 6388]\n",
      "loss: 0.067607  [ 6000/ 6388]\n",
      "\n",
      "Test Error:\n",
      "acc: 94.4%, avg loss: 0.009468\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "json"
    },
    "ExecuteTime": {
     "end_time": "2023-10-08T05:53:02.908541500Z",
     "start_time": "2023-10-08T05:49:41.156594100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at the summary breakdown of the model architecture. It shows the number of filters used for the feature extraction and image reduction from pooling for each convolutional layer. Next, it shows 51136 input features and the 2 outputs used for classification in the linear layers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "讓我們來查看模型架構的摘要概述。它顯示了用於特徵提取和從池化中減少圖像的每個卷積層的過濾器數量。接下來，它顯示了 51136 個輸入特徵以及用於線性層中的分類的 2 個輸出。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "summary(model, input_size=(15, 3, 201, 81))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCNNet                                    [15, 2]                   --\n├─Conv2d: 1-1                            [15, 32, 197, 77]         2,432\n├─Conv2d: 1-2                            [15, 64, 94, 34]          51,264\n├─Dropout2d: 1-3                         [15, 64, 94, 34]          --\n├─Flatten: 1-4                           [15, 51136]               --\n├─Linear: 1-5                            [15, 50]                  2,556,850\n├─Linear: 1-6                            [15, 2]                   102\n==========================================================================================\nTotal params: 2,610,648\nTrainable params: 2,610,648\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 3.05\n==========================================================================================\nInput size (MB): 2.93\nForward/backward pass size (MB): 82.80\nParams size (MB): 10.44\nEstimated Total Size (MB): 96.17\n=========================================================================================="
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "json"
    },
    "ExecuteTime": {
     "end_time": "2023-10-08T05:53:48.636947800Z",
     "start_time": "2023-10-08T05:53:48.612534500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## Test the model\n",
    " \n",
    "You should have got somewhere between a 93-95 percent accuracy by the 15th epoch. Here we grab a batch from our test data, and see how the model performs on the predicted result and the actual result. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "test_loss, correct = 0, 0\n",
    "class_map = ['no', 'yes']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, (X, Y) in enumerate(test_dataloader):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        pred = model(X)\n",
    "        print(\"Predicted:\\nvalue={}, class_name= {}\\n\".format(pred[0].argmax(0),class_map[pred[0].argmax(0)]))\n",
    "        print(\"Actual:\\nvalue={}, class_name= {}\\n\".format(Y[0],class_map[Y[0]]))\n",
    "        break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:\n",
      "value=0, class_name= no\n",
      "\n",
      "Actual:\n",
      "value=0, class_name= no\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "json"
    },
    "ExecuteTime": {
     "end_time": "2023-10-08T05:54:37.704289200Z",
     "start_time": "2023-10-08T05:54:32.462819600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
