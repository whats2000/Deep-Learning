{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>prediction_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>002824.jpg</td>\n",
       "      <td>[[person, 0.5703840851783752, 8, 13, 488, 427]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>000473.jpg</td>\n",
       "      <td>[[aeroplane, 0.8369326591491699, 425, 123, 451...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>000358.jpg</td>\n",
       "      <td>[[person, 0.7091589570045471, 12, 21, 115, 302...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>006052.jpg</td>\n",
       "      <td>[[cat, 0.36228689551353455, 132, 33, 485, 363]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>004758.jpg</td>\n",
       "      <td>[[dog, 0.6347848176956177, 43, 42, 133, 307]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    image_id                                    prediction_list\n",
       "0   1  002824.jpg    [[person, 0.5703840851783752, 8, 13, 488, 427]]\n",
       "1   2  000473.jpg  [[aeroplane, 0.8369326591491699, 425, 123, 451...\n",
       "2   3  000358.jpg  [[person, 0.7091589570045471, 12, 21, 115, 302...\n",
       "3   4  006052.jpg    [[cat, 0.36228689551353455, 132, 33, 485, 363]]\n",
       "4   5  004758.jpg      [[dog, 0.6347848176956177, 43, 42, 133, 307]]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to the CSV file containing predictions\n",
    "predictions_path = 'result.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "predictions_df = pd.read_csv(predictions_path)\n",
    "\n",
    "# Convert the string representations in the 'prediction_list' column to actual lists\n",
    "predictions_df[\"prediction_list\"] = predictions_df[\"prediction_list\"].apply(ast.literal_eval)\n",
    "\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_CLASSES = (  # always index 0\n",
    "    \"aeroplane\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"boat\",\n",
    "    \"bottle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"chair\",\n",
    "    \"cow\",\n",
    "    \"diningtable\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"motorbike\",\n",
    "    \"person\",\n",
    "    \"pottedplant\",\n",
    "    \"sheep\",\n",
    "    \"sofa\",\n",
    "    \"train\",\n",
    "    \"tvmonitor\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>boxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>002824.jpg</td>\n",
       "      <td>[{'x1': 1, 'y1': 13, 'x2': 500, 'y2': 431, 'cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000473.jpg</td>\n",
       "      <td>[{'x1': 415, 'y1': 120, 'x2': 460, 'y2': 153, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000358.jpg</td>\n",
       "      <td>[{'x1': 89, 'y1': 100, 'x2': 387, 'y2': 284, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>006052.jpg</td>\n",
       "      <td>[{'x1': 129, 'y1': 51, 'x2': 497, 'y2': 374, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>004758.jpg</td>\n",
       "      <td>[{'x1': 44, 'y1': 49, 'x2': 129, 'y2': 308, 'c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_id                                              boxes\n",
       "0  002824.jpg  [{'x1': 1, 'y1': 13, 'x2': 500, 'y2': 431, 'cl...\n",
       "1  000473.jpg  [{'x1': 415, 'y1': 120, 'x2': 460, 'y2': 153, ...\n",
       "2  000358.jpg  [{'x1': 89, 'y1': 100, 'x2': 387, 'y2': 284, '...\n",
       "3  006052.jpg  [{'x1': 129, 'y1': 51, 'x2': 497, 'y2': 374, '...\n",
       "4  004758.jpg  [{'x1': 44, 'y1': 49, 'x2': 129, 'y2': 308, 'c..."
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the ground truth data\n",
    "ground_truth_path = 'data/voc2007test_gt.txt'\n",
    "\n",
    "# let's try reading the file line by line and parsing it manually.\n",
    "ground_truth_data = []\n",
    "\n",
    "with open(ground_truth_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line by spaces and parse the contents\n",
    "        split_line = line.strip().split()\n",
    "        image_id = split_line[0]\n",
    "        boxes = []\n",
    "        for i in range(1, len(split_line), 5):\n",
    "            box = {\n",
    "                \"x1\": int(split_line[i]),\n",
    "                \"y1\": int(split_line[i+1]),\n",
    "                \"x2\": int(split_line[i+2]),\n",
    "                \"y2\": int(split_line[i+3]),\n",
    "                \"class_name\": VOC_CLASSES[int(split_line[i+4])]\n",
    "            }\n",
    "            boxes.append(box)\n",
    "        ground_truth_data.append({\"image_id\": image_id, \"boxes\": boxes})\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "ground_truth_df = pd.DataFrame(ground_truth_data)\n",
    "ground_truth_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'x1': 1, 'y1': 13, 'x2': 500, 'y2': 431, 'class_name': 'person'}]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_df[\"boxes\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['002824.jpg', 14, 0.5703840851783752, 8, 13, 488, 427],\n",
       "  ['000473.jpg', 0, 0.8369326591491699, 425, 123, 451, 148],\n",
       "  ['000358.jpg', 14, 0.7091589570045471, 12, 21, 115, 302],\n",
       "  ['000358.jpg', 6, 0.6020732522010803, 62, 87, 392, 297],\n",
       "  ['006052.jpg', 7, 0.36228689551353455, 132, 33, 485, 363]],\n",
       " [['002824.jpg', 14, 1.0, 1, 13, 500, 431],\n",
       "  ['000473.jpg', 0, 1.0, 415, 120, 460, 153],\n",
       "  ['000358.jpg', 6, 1.0, 89, 100, 387, 284],\n",
       "  ['000358.jpg', 14, 1.0, 23, 33, 110, 287],\n",
       "  ['006052.jpg', 7, 1.0, 129, 51, 497, 374]])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mapping from class names to class indices\n",
    "class_name_to_index = {class_name: index for index, class_name in enumerate(VOC_CLASSES)}\n",
    "\n",
    "def convert_predictions_to_list(df, class_name_to_index):\n",
    "    \"\"\" Convert predictions DataFrame to the required list format for mAP calculation. \"\"\"\n",
    "    pred_boxes_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        image_id = row[\"image_id\"]\n",
    "        for pred in row[\"prediction_list\"]:\n",
    "            class_name, prob_score, x1, y1, x2, y2 = pred\n",
    "            class_index = class_name_to_index[class_name]\n",
    "            pred_boxes_list.append([image_id, class_index, prob_score, x1, y1, x2, y2])\n",
    "    return pred_boxes_list\n",
    "\n",
    "def convert_ground_truth_to_list(df, class_name_to_index):\n",
    "    \"\"\" Convert ground truth DataFrame to the required list format for mAP calculation. \"\"\"\n",
    "    true_boxes_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        image_id = row[\"image_id\"]\n",
    "        for box in row[\"boxes\"]:\n",
    "            class_index = class_name_to_index[box[\"class_name\"]]\n",
    "            x1, y1, x2, y2 = box[\"x1\"], box[\"y1\"], box[\"x2\"], box[\"y2\"]\n",
    "            true_boxes_list.append([image_id, class_index, 1.0 ,x1, y1, x2, y2])\n",
    "    return true_boxes_list\n",
    "\n",
    "# Convert DataFrames to the required list format\n",
    "pred_boxes = convert_predictions_to_list(predictions_df, class_name_to_index)\n",
    "true_boxes = convert_ground_truth_to_list(ground_truth_df, class_name_to_index)\n",
    "\n",
    "# Display first few elements of converted lists for verification\n",
    "pred_boxes[:5], true_boxes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 14, 0.5703840851783752, 8, 13, 488, 427],\n",
       "  [1, 0, 0.8369326591491699, 425, 123, 451, 148],\n",
       "  [2, 14, 0.7091589570045471, 12, 21, 115, 302],\n",
       "  [2, 6, 0.6020732522010803, 62, 87, 392, 297],\n",
       "  [3, 7, 0.36228689551353455, 132, 33, 485, 363]],\n",
       " [[0, 14, 1.0, 1, 13, 500, 431],\n",
       "  [1, 0, 1.0, 415, 120, 460, 153],\n",
       "  [2, 6, 1.0, 89, 100, 387, 284],\n",
       "  [2, 14, 1.0, 23, 33, 110, 287],\n",
       "  [3, 7, 1.0, 129, 51, 497, 374]])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mapping from image_id to a unique numerical index\n",
    "unique_image_ids = ground_truth_df[\"image_id\"].unique()\n",
    "image_id_to_index = {image_id: idx for idx, image_id in enumerate(unique_image_ids)}\n",
    "\n",
    "def update_list_with_index(input_list, image_id_to_index):\n",
    "    \"\"\" Update the given list by replacing image_id with a numerical index based on the provided mapping. \"\"\"\n",
    "    updated_list = []\n",
    "    for item in input_list:\n",
    "        image_id = item[0]\n",
    "        updated_item = [image_id_to_index[image_id]] + item[1:]\n",
    "        updated_list.append(updated_item)\n",
    "    return updated_list\n",
    "\n",
    "# Update pred_boxes and true_boxes with numerical indices\n",
    "pred_boxes_updated = update_list_with_index(pred_boxes, image_id_to_index)\n",
    "true_boxes_updated = update_list_with_index(true_boxes, image_id_to_index)\n",
    "\n",
    "# Display first few elements of the updated lists for verification\n",
    "pred_boxes_updated[:5], true_boxes_updated[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2023 Aladdin Persson\n",
    "# The following code is derived from the YOLOv3 implementation by Aladdin Persson available at\n",
    "# https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/object_detection/YOLOv3/\n",
    "# MIT license\n",
    "\n",
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    This function calculates the Intersection over Union (IoU) given predicted boxes and target boxes.\n",
    "\n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        box_format (str): Format of the boxes, either \"midpoint\" or \"corners\"\n",
    "\n",
    "    Returns:\n",
    "        tensor: Intersection over Union for all examples\n",
    "    \"\"\"\n",
    "\n",
    "    if box_format == \"midpoint\":\n",
    "        # Convert midpoint format to corner format\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    if box_format == \"corners\":\n",
    "        # Use corner format directly\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "\n",
    "    # Calculate the coordinates of the intersection rectangle\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    # Calculate the area of intersection and union\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "    # Calculate the IoU\n",
    "    iou = intersection / (box1_area + box2_area - intersection + 1e-6)\n",
    "\n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2023 Aladdin Persson\n",
    "# The following code is derived from the YOLOv3 implementation by Aladdin Persson available at\n",
    "# https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/object_detection/YOLOv3/\n",
    "# MIT license\n",
    "\n",
    "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\", max_cal=3000):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes\n",
    "\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all bboxes with each bbox\n",
    "        specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
    "        iou_threshold (float): threshold where predicted bbox is considered correct\n",
    "        threshold (float): threshold to remove predicted bboxes (independent of IoU)\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "        max_cal: maximum number of bboxes to calculate NMS for\n",
    "\n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    # Filter out bboxes with probability scores below the threshold\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "\n",
    "    # Sort the bboxes in descending order based on probability scores\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    # Limit the number of bboxes to perform NMS on\n",
    "    bboxes = bboxes[:max_cal]\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        # Remove bboxes that have the same class prediction as the chosen_box\n",
    "        # and have an IoU greater than the specified threshold\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "            or intersection_over_union(\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "                box_format=box_format,\n",
    "            )\n",
    "            < iou_threshold\n",
    "        ]\n",
    "\n",
    "        # Add the chosen_box to the final list of bboxes after NMS\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2023 Aladdin Persson\n",
    "# The following code is derived from the YOLOv3 implementation by Aladdin Persson available at\n",
    "# https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/object_detection/YOLOv3/\n",
    "# MIT license\n",
    "\n",
    "def mean_average_precision(\n",
    "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates the mean Average Precision (mAP) for object detection.\n",
    "\n",
    "    Parameters:\n",
    "        pred_boxes (list): List of lists containing predicted bounding boxes,\n",
    "                           each specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2].\n",
    "        true_boxes (list): List of lists containing ground truth bounding boxes,\n",
    "                           formatted similarly to pred_boxes but without the prob_score.\n",
    "        iou_threshold (float): Threshold for Intersection over Union (IoU) to consider\n",
    "                               a prediction as a correct detection.\n",
    "        box_format (str): Format of the bounding boxes - \"midpoint\" or \"corners\".\n",
    "        num_classes (int): Total number of classes.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean mAP value across all classes given a specific IoU threshold.\n",
    "        dict: mAP values for each class.\n",
    "    \"\"\"\n",
    "\n",
    "    average_precisions = []  # Stores the average precision for each class.\n",
    "    epsilon = 1e-6  # Small value to ensure numerical stability.\n",
    "    class_map = {}  # Dictionary to store AP for each class.\n",
    "\n",
    "    # Iterate over all classes.\n",
    "    for c in tqdm(range(num_classes)):\n",
    "        detections = []  # List to store predictions for the current class.\n",
    "        ground_truths = []  # List to store ground truths for the current class.\n",
    "\n",
    "        # Filter out predictions and ground truths for the current class.\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # Count the number of ground truth boxes for each image.\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # Sort detections by confidence score.\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        TP = torch.zeros((len(detections)))  # True Positives.\n",
    "        FP = torch.zeros((len(detections)))  # False Positives.\n",
    "        total_true_bboxes = len(ground_truths)  # Total ground truths for the class.\n",
    "\n",
    "        # Skip if there are no ground truths for this class.\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        # Calculate TP and FP for each detection.\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            ground_truth_img = [bbox for bbox in ground_truths if bbox[0] == detection[0]]\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "            # Mark as TP or FP based on IoU threshold.\n",
    "            if best_iou > iou_threshold:\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        # Calculate cumulative sums for TP and FP to compute recalls and precisions.\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
    "        \n",
    "        # Add sentinel values at the start and end.\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "\n",
    "        # Calculate AP using the trapezoidal rule (numerical integration).\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "        class_map[c] = average_precisions[-1]\n",
    "\n",
    "    # Calculate mean AP over all classes.\n",
    "    mean_map = sum(average_precisions) / len(average_precisions)\n",
    "\n",
    "    return mean_map, class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:06<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aeroplane AP (EMA): 0.40616244077682495\n",
      "bicycle AP (EMA): 0.6702146530151367\n",
      "bird AP (EMA): 0.48762625455856323\n",
      "boat AP (EMA): 0.28872260451316833\n",
      "bottle AP (EMA): 0.23380839824676514\n",
      "bus AP (EMA): 0.6299288868904114\n",
      "car AP (EMA): 0.6543501019477844\n",
      "cat AP (EMA): 0.7566591501235962\n",
      "chair AP (EMA): 0.3437689244747162\n",
      "cow AP (EMA): 0.5243673920631409\n",
      "diningtable AP (EMA): 0.4062141478061676\n",
      "dog AP (EMA): 0.6922149658203125\n",
      "horse AP (EMA): 0.7172512412071228\n",
      "motorbike AP (EMA): 0.6244627833366394\n",
      "person AP (EMA): 0.5648261308670044\n",
      "pottedplant AP (EMA): 0.2502616047859192\n",
      "sheep AP (EMA): 0.47784918546676636\n",
      "sofa AP (EMA): 0.48125213384628296\n",
      "train AP (EMA): 0.6834219098091125\n",
      "tvmonitor AP (EMA): 0.4964831471443176\n",
      "MAP (EMA): 0.5194922685623169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mapval, map_dict = mean_average_precision(\n",
    "    pred_boxes_updated,\n",
    "    true_boxes_updated,\n",
    "    iou_threshold=0.5,\n",
    "    box_format=\"corners\",\n",
    "    num_classes=20,\n",
    ")\n",
    "\n",
    "for c, ap in map_dict.items():\n",
    "    print(f\"{VOC_CLASSES[int(c)]} AP (EMA): {ap.item()}\")\n",
    "            \n",
    "print(f\"MAP (EMA): {mapval.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
