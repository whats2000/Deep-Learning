{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIS 583 Assignment 4: Self-supervised and transfer learning on CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, please put your name and SID in following format: <br>\n",
    ": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷 M114020035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**    \n",
    "Hi I'm Ren-Di Wu, B104020009."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWMWW8Ab_345"
   },
   "source": [
    "## Google Colab Setup\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vH4wc4iD_6w_",
    "ExecuteTime": {
     "end_time": "2023-11-07T06:24:57.203222200Z",
     "start_time": "2023-11-07T06:24:57.201707200Z"
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Um5DJvBwb6xT"
   },
   "source": [
    "# Data Setup (5 points)\n",
    "\n",
    "The first thing to do is implement a dataset class to load rotated CIFAR10 images with matching labels. Since there is already a CIFAR10 dataset class implemented in `torchvision`, we will extend this class and modify the `__get_item__` method appropriately to load rotated images.\n",
    "\n",
    "Each rotation label should be an integer in the set {0, 1, 2, 3} which correspond to rotations of 0, 90, 180, or 270 degrees respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oHkeNUOKiFbP",
    "ExecuteTime": {
     "end_time": "2023-11-08T07:05:34.923406300Z",
     "start_time": "2023-11-08T07:05:34.916380400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def rotate_img(img, rot):\n",
    "    if rot == 0: # 0 degrees rotation\n",
    "        return img\n",
    "    #######################################################################\n",
    "    #        TODO: Implement rotate_img() - return the rotated img        #                           \n",
    "    #######################################################################\n",
    "    angles = {0: 0, 1: 90, 2: 180, 3: 270}\n",
    "    if rot in angles:\n",
    "        return transforms.functional.rotate(img, angles[rot])\n",
    "    else:\n",
    "        raise ValueError('rotation should be 0, 90, 180, or 270 degrees')\n",
    "\n",
    "    #######################################################################\n",
    "    #                           End of your code                          #\n",
    "    #######################################################################\n",
    "\n",
    "class CIFAR10Rotation(torchvision.datasets.CIFAR10):\n",
    "\n",
    "    def __init__(self, root, train, download, transform) -> None:\n",
    "        super().__init__(root=root, train=train, download=download, transform=transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image, cls_label = super().__getitem__(index)\n",
    "\n",
    "        # randomly select image rotation\n",
    "        rotation_label = random.choice([0, 1, 2, 3])\n",
    "        image_rotated = rotate_img(image, rotation_label)\n",
    "\n",
    "        rotation_label = torch.tensor(rotation_label).long()\n",
    "        return image, image_rotated, rotation_label, torch.tensor(cls_label).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "CCBSpNWpb8uw",
    "ExecuteTime": {
     "end_time": "2023-11-08T07:05:37.739399300Z",
     "start_time": "2023-11-08T07:05:35.499557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = CIFAR10Rotation(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testset = CIFAR10Rotation(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOCWMyGhVOJB"
   },
   "source": [
    "Show some example images and rotated images with labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "A9wN4BJWVMzB",
    "ExecuteTime": {
     "end_time": "2023-11-08T07:05:39.224231900Z",
     "start_time": "2023-11-08T07:05:38.812227400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels:  car   bird  dog   cat  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotation labels:  0     180   270   180  \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "rot_classes = ('0', '90', '180', '270')\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    # unnormalize\n",
    "    img = transforms.Normalize((0, 0, 0), (1/0.2023, 1/0.1994, 1/0.2010))(img)\n",
    "    img = transforms.Normalize((-0.4914, -0.4822, -0.4465), (1, 1, 1))(img)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, rot_images, rot_labels, labels = next(dataiter)\n",
    "\n",
    "# print images and rotated images\n",
    "img_grid = imshow(torchvision.utils.make_grid(images[:4], padding=0))\n",
    "print('Class labels: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
    "img_grid = imshow(torchvision.utils.make_grid(rot_images[:4], padding=0))\n",
    "print('Rotation labels: ', ' '.join(f'{rot_classes[rot_labels[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unCucbHexG4W"
   },
   "source": [
    "# Evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "pptQRpqK0rOl",
    "ExecuteTime": {
     "end_time": "2023-11-08T07:05:39.349276400Z",
     "start_time": "2023-11-08T07:05:39.263253700Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_test(net, testloader, criterion, task):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    avg_test_loss = 0.0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for images, images_rotated, labels, cls_labels in testloader:\n",
    "            if task == 'rotation':\n",
    "              images, labels = images_rotated.to(device), labels.to(device)\n",
    "            elif task == 'classification':\n",
    "              images, labels = images.to(device), cls_labels.to(device)\n",
    "            #######################################################################\n",
    "            # TODO: Calculate outputs by running images through the network       #\n",
    "            # The class with the highest energy is what we choose as prediction   #\n",
    "            #######################################################################\n",
    "            # Calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "\n",
    "            # The class with the highest score is what we choose as our prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            #######################################################################\n",
    "            #                           End of your code                          #\n",
    "            #######################################################################\n",
    "            avg_test_loss += criterion(outputs, labels)  / len(testloader)\n",
    "    print('TESTING:')\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f} %')\n",
    "    print(f'Average loss on the 10000 test images: {avg_test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "hf698c16A9k5",
    "ExecuteTime": {
     "end_time": "2023-11-08T07:05:40.741265500Z",
     "start_time": "2023-11-08T07:05:40.734789900Z"
    }
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, init_lr, decay_epochs=30):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = init_lr * (0.1 ** (epoch // decay_epochs))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lYdnb1Wsta_"
   },
   "source": [
    "# Train a ResNet18 on the rotation task (9 points)\n",
    "\n",
    "In this section, we will train a ResNet18 model **from scratch** on the rotation task. The input is a rotated image and the model predicts the rotation label. See the Data Setup section for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "knAiwdURvBHk",
    "ExecuteTime": {
     "end_time": "2023-11-08T07:05:41.787161Z",
     "start_time": "2023-11-08T07:05:41.766498800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice: You should not use pretrained weights from ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "235MEIUgsv65",
    "ExecuteTime": {
     "end_time": "2023-11-08T07:05:42.979885200Z",
     "start_time": "2023-11-08T07:05:42.573418400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "net = resnet18(weights = None, num_classes=4) # Do not modify this line.\n",
    "net = net.to(device)\n",
    "print(net) # print your model and check the num_classes is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Vuhiw0ZoszAd",
    "ExecuteTime": {
     "end_time": "2023-11-08T07:05:43.324540400Z",
     "start_time": "2023-11-08T07:05:43.308534500Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "################################################################################\n",
    "# TODO: Define loss and optmizer functions                                     #\n",
    "# Try any loss or optimizer function and learning rate to get better result    #\n",
    "# hint: torch.nn and torch.optim                                               #\n",
    "################################################################################\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "################################################################################\n",
    "#                               End of your code                               #\n",
    "################################################################################\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "WleH-YBgs0rq",
    "ExecuteTime": {
     "end_time": "2023-11-08T07:05:44.922491800Z",
     "start_time": "2023-11-08T07:05:44.901481900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Both the self-supervised rotation task and supervised CIFAR10 classification are\n",
    "# trained with the CrossEntropyLoss, so we can use the training loop code.\n",
    "\n",
    "def train(net, criterion, optimizer, num_epochs, decay_epochs, init_lr, task):\n",
    "\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0.0\n",
    "        running_total = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        net.train()\n",
    "\n",
    "        for i, (imgs, imgs_rotated, rotation_label, cls_label) in enumerate(trainloader, 0):\n",
    "            adjust_learning_rate(optimizer, epoch, init_lr, decay_epochs)\n",
    "            ######################################################################################################\n",
    "            # TODO: Set the data to the correct device; Different task will use different inputs and labels      #  \n",
    "            # TODO: Zero the parameter gradients                                                                 #\n",
    "            # TODO: forward + backward + optimize                                                                #\n",
    "            # TODO: Get predicted results                                                                        #\n",
    "            ######################################################################################################\n",
    "            # Set data to the correct device\n",
    "            if task == 'rotation':\n",
    "                inputs, labels = imgs_rotated.to(device), rotation_label.to(device)\n",
    "            elif task == 'classification':\n",
    "                inputs, labels = imgs.to(device), cls_label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Get predicted results for accuracy calculation\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            ######################################################################################################\n",
    "            #                               End of your code                                                     #\n",
    "            ######################################################################################################                                            \n",
    "\n",
    "\n",
    "            # print statistics\n",
    "            print_freq = 100\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # calc acc\n",
    "            running_total += labels.size(0)\n",
    "            running_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if i % print_freq == (print_freq - 1):    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / print_freq:.3f} acc: {100*running_correct / running_total:.2f} time: {time.time() - start_time:.2f}')\n",
    "                running_loss, running_correct, running_total = 0.0, 0.0, 0.0\n",
    "                start_time = time.time()\n",
    "        ######################################################################################################\n",
    "        # TODO: Run the run_test() function after each epoch; Set the model to the evaluation mode.          #\n",
    "        ######################################################################################################\n",
    "        # Evaluate the model after each epoch\n",
    "        net.eval()\n",
    "        run_test(net, testloader, criterion, task)\n",
    "        ######################################################################################################\n",
    "        #                               End of your code                                                     #\n",
    "        ######################################################################################################  \n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "2u4AsfAKtaQS",
    "ExecuteTime": {
     "end_time": "2023-11-08T07:32:29.358087600Z",
     "start_time": "2023-11-08T07:05:45.810629200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.519 acc: 31.64 time: 8.89\n",
      "[1,   200] loss: 1.291 acc: 43.62 time: 7.72\n",
      "[1,   300] loss: 1.217 acc: 48.27 time: 8.00\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 52.22 %\n",
      "Average loss on the 10000 test images: 1.151\n",
      "[2,   100] loss: 1.176 acc: 51.48 time: 7.93\n",
      "[2,   200] loss: 1.166 acc: 52.25 time: 8.22\n",
      "[2,   300] loss: 1.148 acc: 53.42 time: 7.75\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 54.94 %\n",
      "Average loss on the 10000 test images: 1.137\n",
      "[3,   100] loss: 1.135 acc: 54.34 time: 7.60\n",
      "[3,   200] loss: 1.112 acc: 55.20 time: 8.97\n",
      "[3,   300] loss: 1.097 acc: 56.84 time: 8.50\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 57.49 %\n",
      "Average loss on the 10000 test images: 1.109\n",
      "[4,   100] loss: 1.082 acc: 58.09 time: 8.13\n",
      "[4,   200] loss: 1.072 acc: 58.38 time: 8.42\n",
      "[4,   300] loss: 1.059 acc: 59.34 time: 8.67\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 58.75 %\n",
      "Average loss on the 10000 test images: 1.089\n",
      "[5,   100] loss: 1.060 acc: 59.67 time: 8.00\n",
      "[5,   200] loss: 1.042 acc: 59.91 time: 8.14\n",
      "[5,   300] loss: 1.032 acc: 61.42 time: 8.41\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 59.34 %\n",
      "Average loss on the 10000 test images: 1.073\n",
      "[6,   100] loss: 1.025 acc: 61.59 time: 7.69\n",
      "[6,   200] loss: 1.017 acc: 61.91 time: 8.60\n",
      "[6,   300] loss: 1.011 acc: 62.44 time: 8.79\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 61.77 %\n",
      "Average loss on the 10000 test images: 1.019\n",
      "[7,   100] loss: 0.991 acc: 64.09 time: 8.16\n",
      "[7,   200] loss: 1.010 acc: 62.16 time: 8.01\n",
      "[7,   300] loss: 0.993 acc: 63.95 time: 8.07\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 64.34 %\n",
      "Average loss on the 10000 test images: 0.979\n",
      "[8,   100] loss: 0.986 acc: 64.23 time: 7.91\n",
      "[8,   200] loss: 0.972 acc: 64.64 time: 7.75\n",
      "[8,   300] loss: 0.973 acc: 65.02 time: 7.86\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 65.63 %\n",
      "Average loss on the 10000 test images: 0.961\n",
      "[9,   100] loss: 0.953 acc: 66.09 time: 8.12\n",
      "[9,   200] loss: 0.967 acc: 64.71 time: 8.38\n",
      "[9,   300] loss: 0.963 acc: 65.27 time: 8.04\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 66.27 %\n",
      "Average loss on the 10000 test images: 0.967\n",
      "[10,   100] loss: 0.950 acc: 66.51 time: 9.13\n",
      "[10,   200] loss: 0.942 acc: 67.13 time: 7.88\n",
      "[10,   300] loss: 0.931 acc: 67.70 time: 8.36\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 67.05 %\n",
      "Average loss on the 10000 test images: 0.947\n",
      "[11,   100] loss: 0.933 acc: 67.26 time: 7.63\n",
      "[11,   200] loss: 0.925 acc: 67.94 time: 7.87\n",
      "[11,   300] loss: 0.928 acc: 67.54 time: 7.92\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 67.99 %\n",
      "Average loss on the 10000 test images: 0.921\n",
      "[12,   100] loss: 0.911 acc: 68.64 time: 7.82\n",
      "[12,   200] loss: 0.916 acc: 68.32 time: 8.38\n",
      "[12,   300] loss: 0.905 acc: 69.11 time: 7.61\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 69.79 %\n",
      "Average loss on the 10000 test images: 0.900\n",
      "[13,   100] loss: 0.904 acc: 69.50 time: 9.28\n",
      "[13,   200] loss: 0.917 acc: 68.20 time: 8.87\n",
      "[13,   300] loss: 0.895 acc: 69.49 time: 7.67\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 68.77 %\n",
      "Average loss on the 10000 test images: 0.914\n",
      "[14,   100] loss: 0.897 acc: 69.88 time: 7.59\n",
      "[14,   200] loss: 0.896 acc: 70.12 time: 8.20\n",
      "[14,   300] loss: 0.880 acc: 70.23 time: 7.46\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 70.66 %\n",
      "Average loss on the 10000 test images: 0.882\n",
      "[15,   100] loss: 0.880 acc: 70.76 time: 7.97\n",
      "[15,   200] loss: 0.875 acc: 71.27 time: 9.36\n",
      "[15,   300] loss: 0.882 acc: 70.25 time: 7.83\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 72.64 %\n",
      "Average loss on the 10000 test images: 0.852\n",
      "[16,   100] loss: 0.841 acc: 73.30 time: 7.19\n",
      "[16,   200] loss: 0.815 acc: 74.85 time: 7.23\n",
      "[16,   300] loss: 0.825 acc: 73.33 time: 7.53\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 75.06 %\n",
      "Average loss on the 10000 test images: 0.799\n",
      "[17,   100] loss: 0.809 acc: 75.38 time: 8.46\n",
      "[17,   200] loss: 0.816 acc: 74.46 time: 8.08\n",
      "[17,   300] loss: 0.800 acc: 75.15 time: 7.67\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 75.71 %\n",
      "Average loss on the 10000 test images: 0.789\n",
      "[18,   100] loss: 0.804 acc: 74.69 time: 7.15\n",
      "[18,   200] loss: 0.801 acc: 75.22 time: 7.69\n",
      "[18,   300] loss: 0.798 acc: 75.31 time: 7.76\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 76.15 %\n",
      "Average loss on the 10000 test images: 0.783\n",
      "[19,   100] loss: 0.790 acc: 76.23 time: 8.52\n",
      "[19,   200] loss: 0.799 acc: 75.91 time: 8.21\n",
      "[19,   300] loss: 0.787 acc: 76.12 time: 8.24\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 76.47 %\n",
      "Average loss on the 10000 test images: 0.777\n",
      "[20,   100] loss: 0.797 acc: 75.43 time: 7.63\n",
      "[20,   200] loss: 0.789 acc: 76.12 time: 7.96\n",
      "[20,   300] loss: 0.790 acc: 76.02 time: 7.98\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 76.64 %\n",
      "Average loss on the 10000 test images: 0.778\n",
      "[21,   100] loss: 0.792 acc: 75.56 time: 8.10\n",
      "[21,   200] loss: 0.783 acc: 76.22 time: 8.33\n",
      "[21,   300] loss: 0.783 acc: 76.17 time: 7.95\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 76.86 %\n",
      "Average loss on the 10000 test images: 0.775\n",
      "[22,   100] loss: 0.781 acc: 76.47 time: 7.75\n",
      "[22,   200] loss: 0.778 acc: 76.44 time: 7.76\n",
      "[22,   300] loss: 0.785 acc: 76.38 time: 7.54\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 76.58 %\n",
      "Average loss on the 10000 test images: 0.775\n",
      "[23,   100] loss: 0.773 acc: 76.91 time: 8.07\n",
      "[23,   200] loss: 0.779 acc: 76.59 time: 8.04\n",
      "[23,   300] loss: 0.779 acc: 76.55 time: 8.15\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 77.53 %\n",
      "Average loss on the 10000 test images: 0.762\n",
      "[24,   100] loss: 0.775 acc: 76.98 time: 7.74\n",
      "[24,   200] loss: 0.777 acc: 76.95 time: 8.86\n",
      "[24,   300] loss: 0.768 acc: 77.16 time: 8.20\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 77.36 %\n",
      "Average loss on the 10000 test images: 0.762\n",
      "[25,   100] loss: 0.766 acc: 77.12 time: 10.56\n",
      "[25,   200] loss: 0.776 acc: 76.27 time: 9.08\n",
      "[25,   300] loss: 0.765 acc: 77.30 time: 9.72\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 77.56 %\n",
      "Average loss on the 10000 test images: 0.760\n",
      "[26,   100] loss: 0.765 acc: 77.66 time: 8.51\n",
      "[26,   200] loss: 0.764 acc: 77.45 time: 7.95\n",
      "[26,   300] loss: 0.769 acc: 77.43 time: 7.86\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 77.58 %\n",
      "Average loss on the 10000 test images: 0.759\n",
      "[27,   100] loss: 0.757 acc: 77.97 time: 8.76\n",
      "[27,   200] loss: 0.765 acc: 77.66 time: 7.80\n",
      "[27,   300] loss: 0.760 acc: 77.80 time: 7.47\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 77.55 %\n",
      "Average loss on the 10000 test images: 0.755\n",
      "[28,   100] loss: 0.758 acc: 77.82 time: 7.40\n",
      "[28,   200] loss: 0.766 acc: 77.27 time: 7.66\n",
      "[28,   300] loss: 0.756 acc: 77.89 time: 7.83\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 77.86 %\n",
      "Average loss on the 10000 test images: 0.751\n",
      "[29,   100] loss: 0.762 acc: 77.63 time: 7.56\n",
      "[29,   200] loss: 0.748 acc: 78.19 time: 8.19\n",
      "[29,   300] loss: 0.760 acc: 77.75 time: 22.21\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 78.24 %\n",
      "Average loss on the 10000 test images: 0.756\n",
      "[30,   100] loss: 0.751 acc: 78.29 time: 8.08\n",
      "[30,   200] loss: 0.753 acc: 78.18 time: 8.61\n",
      "[30,   300] loss: 0.751 acc: 78.08 time: 9.12\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 78.32 %\n",
      "Average loss on the 10000 test images: 0.751\n",
      "[31,   100] loss: 0.759 acc: 77.42 time: 7.45\n",
      "[31,   200] loss: 0.751 acc: 77.85 time: 7.87\n",
      "[31,   300] loss: 0.746 acc: 78.25 time: 7.75\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 78.08 %\n",
      "Average loss on the 10000 test images: 0.746\n",
      "[32,   100] loss: 0.736 acc: 79.06 time: 7.37\n",
      "[32,   200] loss: 0.745 acc: 78.73 time: 6.78\n",
      "[32,   300] loss: 0.741 acc: 78.97 time: 6.93\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 78.83 %\n",
      "Average loss on the 10000 test images: 0.738\n",
      "[33,   100] loss: 0.738 acc: 79.21 time: 6.89\n",
      "[33,   200] loss: 0.739 acc: 78.87 time: 7.35\n",
      "[33,   300] loss: 0.745 acc: 78.72 time: 7.38\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 79.22 %\n",
      "Average loss on the 10000 test images: 0.738\n",
      "[34,   100] loss: 0.746 acc: 78.68 time: 7.10\n",
      "[34,   200] loss: 0.739 acc: 78.98 time: 7.57\n",
      "[34,   300] loss: 0.747 acc: 78.30 time: 7.66\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 78.86 %\n",
      "Average loss on the 10000 test images: 0.737\n",
      "[35,   100] loss: 0.741 acc: 78.72 time: 7.24\n",
      "[35,   200] loss: 0.736 acc: 79.05 time: 8.24\n",
      "[35,   300] loss: 0.747 acc: 78.48 time: 7.81\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 78.97 %\n",
      "Average loss on the 10000 test images: 0.740\n",
      "[36,   100] loss: 0.744 acc: 78.40 time: 7.21\n",
      "[36,   200] loss: 0.743 acc: 78.82 time: 7.59\n",
      "[36,   300] loss: 0.737 acc: 79.05 time: 7.46\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 78.95 %\n",
      "Average loss on the 10000 test images: 0.741\n",
      "[37,   100] loss: 0.741 acc: 79.12 time: 7.45\n",
      "[37,   200] loss: 0.744 acc: 78.40 time: 7.42\n",
      "[37,   300] loss: 0.739 acc: 78.65 time: 7.37\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 78.80 %\n",
      "Average loss on the 10000 test images: 0.738\n",
      "[38,   100] loss: 0.743 acc: 78.62 time: 7.36\n",
      "[38,   200] loss: 0.737 acc: 79.04 time: 7.60\n",
      "[38,   300] loss: 0.735 acc: 78.81 time: 7.41\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 78.22 %\n",
      "Average loss on the 10000 test images: 0.744\n",
      "[39,   100] loss: 0.743 acc: 78.66 time: 7.36\n",
      "[39,   200] loss: 0.740 acc: 78.92 time: 7.45\n",
      "[39,   300] loss: 0.739 acc: 79.08 time: 7.51\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 78.82 %\n",
      "Average loss on the 10000 test images: 0.739\n",
      "[40,   100] loss: 0.740 acc: 78.87 time: 7.49\n",
      "[40,   200] loss: 0.737 acc: 78.91 time: 7.44\n",
      "[40,   300] loss: 0.735 acc: 78.97 time: 7.51\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 79.16 %\n",
      "Average loss on the 10000 test images: 0.737\n",
      "[41,   100] loss: 0.735 acc: 79.38 time: 7.27\n",
      "[41,   200] loss: 0.742 acc: 78.81 time: 7.40\n",
      "[41,   300] loss: 0.736 acc: 78.86 time: 7.29\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 78.86 %\n",
      "Average loss on the 10000 test images: 0.739\n",
      "[42,   100] loss: 0.738 acc: 79.23 time: 6.97\n",
      "[42,   200] loss: 0.736 acc: 79.12 time: 7.67\n",
      "[42,   300] loss: 0.737 acc: 79.04 time: 7.69\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 79.01 %\n",
      "Average loss on the 10000 test images: 0.735\n",
      "[43,   100] loss: 0.739 acc: 79.16 time: 7.01\n",
      "[43,   200] loss: 0.736 acc: 79.42 time: 7.56\n",
      "[43,   300] loss: 0.735 acc: 79.47 time: 7.55\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 78.81 %\n",
      "Average loss on the 10000 test images: 0.742\n",
      "[44,   100] loss: 0.739 acc: 79.15 time: 7.56\n",
      "[44,   200] loss: 0.734 acc: 79.25 time: 7.82\n",
      "[44,   300] loss: 0.744 acc: 78.51 time: 8.78\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 78.72 %\n",
      "Average loss on the 10000 test images: 0.739\n",
      "[45,   100] loss: 0.740 acc: 78.66 time: 7.94\n",
      "[45,   200] loss: 0.738 acc: 79.08 time: 8.37\n",
      "[45,   300] loss: 0.732 acc: 79.52 time: 7.97\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 79.42 %\n",
      "Average loss on the 10000 test images: 0.731\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(net, criterion, optimizer, num_epochs=45, decay_epochs=15, init_lr=0.01, task='rotation')\n",
    "################################\n",
    "#     TODO: Save the model     #  \n",
    "################################\n",
    "from datetime import datetime\n",
    "\n",
    "# Get current date and time\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "# Save the model\n",
    "model_path = f\"model_{current_time}.pth\"\n",
    "torch.save(net.state_dict(), model_path)\n",
    "################################\n",
    "#      End of your code        #  \n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLLMRTS9rTnk"
   },
   "source": [
    "## Fine-tuning on the pre-trained model (9 points)\n",
    "\n",
    "In this section, we will load the ResNet18 model pre-trained on the rotation task and fine-tune on the classification task. We will freeze all previous layers except for the 'layer4' block and 'fc' layer.\n",
    "\n",
    "**Then we will use the trained model from rotation task as the pretrained weights. Notice, you should not use the pretrained weights from ImageNet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "S4nX4ExlrymI",
    "ExecuteTime": {
     "end_time": "2023-11-05T10:21:42.082962800Z",
     "start_time": "2023-11-05T10:21:41.981960200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "#####################################################\n",
    "#     TODO: Load the pre-trained ResNet18 model     #  \n",
    "#####################################################\n",
    "# load the trained classifier weights\n",
    "ckpt = torch.load('model_2023-11-02-23-22-09.pth')\n",
    "net.load_state_dict(ckpt)\n",
    "print(net) # print your model and check the num_classes is correct\n",
    "####################################################\n",
    "#                End of your code                  #   \n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kD44g-TxwYdU",
    "ExecuteTime": {
     "end_time": "2023-11-05T10:21:42.092563600Z",
     "start_time": "2023-11-05T10:21:42.082962800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=10, bias=True)\n)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################################################\n",
    "#   TODO: Freeze all previous layers; only keep the 'layer4' block and 'fc' layer trainable     #\n",
    "#################################################################################################\n",
    "# Freeze all parameters\n",
    "net.requires_grad_(False)\n",
    "\n",
    "# Adjusting the fully connected layer to have 10 outputs for 10 classes\n",
    "net.fc = nn.Linear(net.fc.in_features, 10)  # Re-define the fc layer\n",
    "\n",
    "# Unfreeze the last block (layer4) and the fully connected (fc) layer\n",
    "net.layer4.requires_grad_(True)\n",
    "net.fc.requires_grad_(True)\n",
    "\n",
    "# Make sure to move the model to the device after modifying it\n",
    "net.to(device)\n",
    "#################################################################################################\n",
    "#                                          End of your code                                     #\n",
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9T5DX0efr4fh",
    "ExecuteTime": {
     "end_time": "2023-11-05T10:21:42.100565900Z",
     "start_time": "2023-11-05T10:21:42.096566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t layer4.0.conv1.weight\n",
      "\t layer4.0.bn1.weight\n",
      "\t layer4.0.bn1.bias\n",
      "\t layer4.0.conv2.weight\n",
      "\t layer4.0.bn2.weight\n",
      "\t layer4.0.bn2.bias\n",
      "\t layer4.0.downsample.0.weight\n",
      "\t layer4.0.downsample.1.weight\n",
      "\t layer4.0.downsample.1.bias\n",
      "\t layer4.1.conv1.weight\n",
      "\t layer4.1.bn1.weight\n",
      "\t layer4.1.bn1.bias\n",
      "\t layer4.1.conv2.weight\n",
      "\t layer4.1.bn2.weight\n",
      "\t layer4.1.bn2.bias\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Print all the trainable parameters\n",
    "params_to_update = net.parameters()\n",
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name,param in net.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xb032dG700ph",
    "ExecuteTime": {
     "end_time": "2023-11-05T10:21:42.110568Z",
     "start_time": "2023-11-05T10:21:42.105564600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "CrossEntropyLoss()"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Define criterion and optimizer\n",
    "# Note that your optimizer only needs to update the parameters that are trainable.\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Optimizer: Update only the parameters of layer4 and fc\n",
    "optimizer = optim.Adam([\n",
    "    {'params': net.layer4.parameters()},\n",
    "    {'params': net.fc.parameters()}\n",
    "], lr=0.001)\n",
    "\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3vLSwOo6sBjl",
    "ExecuteTime": {
     "end_time": "2023-11-05T10:32:50.765546400Z",
     "start_time": "2023-11-05T10:21:42.110568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.956 acc: 30.91 time: 6.90\n",
      "[1,   200] loss: 1.740 acc: 41.34 time: 7.43\n",
      "[1,   300] loss: 1.673 acc: 44.98 time: 7.52\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 47.17 %\n",
      "Average loss on the 10000 test images: 1.626\n",
      "[2,   100] loss: 1.621 acc: 47.38 time: 6.80\n",
      "[2,   200] loss: 1.609 acc: 48.16 time: 7.47\n",
      "[2,   300] loss: 1.587 acc: 49.48 time: 7.58\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 51.27 %\n",
      "Average loss on the 10000 test images: 1.560\n",
      "[3,   100] loss: 1.563 acc: 50.53 time: 6.84\n",
      "[3,   200] loss: 1.564 acc: 51.20 time: 7.45\n",
      "[3,   300] loss: 1.557 acc: 51.13 time: 7.54\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 53.10 %\n",
      "Average loss on the 10000 test images: 1.512\n",
      "[4,   100] loss: 1.560 acc: 50.59 time: 6.72\n",
      "[4,   200] loss: 1.532 acc: 52.75 time: 7.38\n",
      "[4,   300] loss: 1.537 acc: 52.34 time: 7.61\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 54.80 %\n",
      "Average loss on the 10000 test images: 1.483\n",
      "[5,   100] loss: 1.544 acc: 51.27 time: 6.69\n",
      "[5,   200] loss: 1.523 acc: 52.57 time: 7.53\n",
      "[5,   300] loss: 1.533 acc: 51.98 time: 7.55\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 54.93 %\n",
      "Average loss on the 10000 test images: 1.478\n",
      "[6,   100] loss: 1.529 acc: 52.37 time: 6.81\n",
      "[6,   200] loss: 1.515 acc: 52.67 time: 7.49\n",
      "[6,   300] loss: 1.525 acc: 52.69 time: 7.55\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 55.20 %\n",
      "Average loss on the 10000 test images: 1.460\n",
      "[7,   100] loss: 1.518 acc: 52.84 time: 6.78\n",
      "[7,   200] loss: 1.508 acc: 53.48 time: 7.45\n",
      "[7,   300] loss: 1.507 acc: 53.05 time: 7.55\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 55.34 %\n",
      "Average loss on the 10000 test images: 1.471\n",
      "[8,   100] loss: 1.507 acc: 53.05 time: 6.75\n",
      "[8,   200] loss: 1.490 acc: 54.20 time: 7.49\n",
      "[8,   300] loss: 1.515 acc: 53.23 time: 7.53\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 55.34 %\n",
      "Average loss on the 10000 test images: 1.472\n",
      "[9,   100] loss: 1.485 acc: 54.57 time: 6.85\n",
      "[9,   200] loss: 1.497 acc: 53.77 time: 7.51\n",
      "[9,   300] loss: 1.495 acc: 54.12 time: 7.57\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 56.35 %\n",
      "Average loss on the 10000 test images: 1.450\n",
      "[10,   100] loss: 1.499 acc: 54.00 time: 6.75\n",
      "[10,   200] loss: 1.486 acc: 54.22 time: 7.49\n",
      "[10,   300] loss: 1.482 acc: 54.66 time: 7.54\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 56.62 %\n",
      "Average loss on the 10000 test images: 1.436\n",
      "[11,   100] loss: 1.458 acc: 56.16 time: 6.77\n",
      "[11,   200] loss: 1.435 acc: 57.03 time: 7.43\n",
      "[11,   300] loss: 1.446 acc: 56.22 time: 7.53\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 58.78 %\n",
      "Average loss on the 10000 test images: 1.391\n",
      "[12,   100] loss: 1.442 acc: 56.45 time: 6.86\n",
      "[12,   200] loss: 1.433 acc: 57.03 time: 7.47\n",
      "[12,   300] loss: 1.426 acc: 57.58 time: 7.58\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 58.98 %\n",
      "Average loss on the 10000 test images: 1.385\n",
      "[13,   100] loss: 1.425 acc: 57.24 time: 6.81\n",
      "[13,   200] loss: 1.430 acc: 56.62 time: 7.48\n",
      "[13,   300] loss: 1.432 acc: 56.55 time: 7.56\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 59.05 %\n",
      "Average loss on the 10000 test images: 1.386\n",
      "[14,   100] loss: 1.420 acc: 57.39 time: 6.78\n",
      "[14,   200] loss: 1.425 acc: 57.36 time: 7.60\n",
      "[14,   300] loss: 1.428 acc: 57.18 time: 7.56\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 58.72 %\n",
      "Average loss on the 10000 test images: 1.385\n",
      "[15,   100] loss: 1.426 acc: 56.80 time: 6.76\n",
      "[15,   200] loss: 1.416 acc: 57.74 time: 7.45\n",
      "[15,   300] loss: 1.421 acc: 57.34 time: 7.53\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 59.13 %\n",
      "Average loss on the 10000 test images: 1.382\n",
      "[16,   100] loss: 1.424 acc: 57.09 time: 6.79\n",
      "[16,   200] loss: 1.419 acc: 57.91 time: 7.45\n",
      "[16,   300] loss: 1.418 acc: 57.77 time: 7.56\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 59.11 %\n",
      "Average loss on the 10000 test images: 1.379\n",
      "[17,   100] loss: 1.417 acc: 57.41 time: 6.76\n",
      "[17,   200] loss: 1.419 acc: 57.38 time: 7.54\n",
      "[17,   300] loss: 1.421 acc: 57.14 time: 7.56\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 59.33 %\n",
      "Average loss on the 10000 test images: 1.377\n",
      "[18,   100] loss: 1.413 acc: 57.19 time: 6.70\n",
      "[18,   200] loss: 1.423 acc: 57.55 time: 7.39\n",
      "[18,   300] loss: 1.414 acc: 57.57 time: 7.55\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 59.10 %\n",
      "Average loss on the 10000 test images: 1.378\n",
      "[19,   100] loss: 1.408 acc: 57.79 time: 6.69\n",
      "[19,   200] loss: 1.425 acc: 56.95 time: 7.71\n",
      "[19,   300] loss: 1.418 acc: 57.21 time: 7.58\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 59.65 %\n",
      "Average loss on the 10000 test images: 1.377\n",
      "[20,   100] loss: 1.413 acc: 57.74 time: 6.83\n",
      "[20,   200] loss: 1.414 acc: 58.00 time: 7.44\n",
      "[20,   300] loss: 1.406 acc: 57.84 time: 7.53\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 59.55 %\n",
      "Average loss on the 10000 test images: 1.373\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(net, criterion, optimizer, num_epochs=20, decay_epochs=10, init_lr=0.01, task='classification')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghPNhcJBrcNj"
   },
   "source": [
    "## Fine-tuning on the randomly initialized model (9 points)\n",
    "In this section, we will randomly initialize a ResNet18 model and fine-tune on the classification task. We will freeze all previous layers except for the 'layer4' block and 'fc' layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2RfXAh9vxXRB",
    "ExecuteTime": {
     "end_time": "2023-11-05T10:32:50.923150900Z",
     "start_time": "2023-11-05T10:32:50.791551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "#################################################\n",
    "# TODO: Randomly initialize a ResNet18 model    #\n",
    "#################################################\n",
    "# Randomly initialize a ResNet18 model\n",
    "net = resnet18(pretrained=False)\n",
    "print(net) # print your model and check the num_classes is correct\n",
    "#################################################\n",
    "#              End of your code                 #\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fpx-SYAizt4p",
    "ExecuteTime": {
     "end_time": "2023-11-05T10:32:50.983181Z",
     "start_time": "2023-11-05T10:32:50.923150900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=10, bias=True)\n)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################################################\n",
    "# TODO: Freeze all previous layers; only keep the 'layer4' block and 'fc' layer trainable       #\n",
    "# To do this, you should set requires_grad=False for the frozen layers.                         #\n",
    "#################################################################################################\n",
    "# Adjusting the fully connected layer for 10 classes\n",
    "net.fc = nn.Linear(net.fc.in_features, 10)\n",
    "\n",
    "# Freeze all layers in the randomly initialized model\n",
    "net.requires_grad_(False)\n",
    "\n",
    "# Unfreeze the last block (layer4) and the fully connected (fc) layer\n",
    "net.layer4.requires_grad_(True)\n",
    "net.fc.requires_grad_(True)\n",
    "\n",
    "# Move the model to the device\n",
    "net.to(device)\n",
    "#################################################################################################\n",
    "#                                          End of your code                                     #\n",
    "#################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "BUFWizbHxgm2",
    "ExecuteTime": {
     "end_time": "2023-11-05T10:32:50.987448100Z",
     "start_time": "2023-11-05T10:32:50.983181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t layer4.0.conv1.weight\n",
      "\t layer4.0.bn1.weight\n",
      "\t layer4.0.bn1.bias\n",
      "\t layer4.0.conv2.weight\n",
      "\t layer4.0.bn2.weight\n",
      "\t layer4.0.bn2.bias\n",
      "\t layer4.0.downsample.0.weight\n",
      "\t layer4.0.downsample.1.weight\n",
      "\t layer4.0.downsample.1.bias\n",
      "\t layer4.1.conv1.weight\n",
      "\t layer4.1.bn1.weight\n",
      "\t layer4.1.bn1.bias\n",
      "\t layer4.1.conv2.weight\n",
      "\t layer4.1.bn2.weight\n",
      "\t layer4.1.bn2.bias\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "# Print all the trainable parameters\n",
    "params_to_update = net.parameters()\n",
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name,param in net.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BxFrGj091AN_",
    "ExecuteTime": {
     "end_time": "2023-11-05T10:32:50.995535600Z",
     "start_time": "2023-11-05T10:32:50.986434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "CrossEntropyLoss()"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Define criterion and optimizer\n",
    "# Note that your optimizer only needs to update the parameters that are trainable.\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Optimizer: Update only the parameters of layer4 and fc\n",
    "optimizer = optim.Adam([\n",
    "    {'params': net.layer4.parameters()},\n",
    "    {'params': net.fc.parameters()}\n",
    "], lr=0.001)\n",
    "\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "GzRVy0MZxpoL",
    "ExecuteTime": {
     "end_time": "2023-11-05T10:43:59.132027700Z",
     "start_time": "2023-11-05T10:32:50.995535600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.340 acc: 23.17 time: 6.85\n",
      "[1,   200] loss: 2.033 acc: 29.73 time: 7.45\n",
      "[1,   300] loss: 1.982 acc: 31.26 time: 7.58\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 35.74 %\n",
      "Average loss on the 10000 test images: 1.879\n",
      "[2,   100] loss: 1.917 acc: 34.38 time: 6.76\n",
      "[2,   200] loss: 1.914 acc: 34.70 time: 7.45\n",
      "[2,   300] loss: 1.892 acc: 35.75 time: 7.59\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 38.41 %\n",
      "Average loss on the 10000 test images: 1.832\n",
      "[3,   100] loss: 1.879 acc: 35.60 time: 6.79\n",
      "[3,   200] loss: 1.878 acc: 35.94 time: 7.55\n",
      "[3,   300] loss: 1.873 acc: 36.68 time: 7.58\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 38.75 %\n",
      "Average loss on the 10000 test images: 1.825\n",
      "[4,   100] loss: 1.855 acc: 37.27 time: 6.78\n",
      "[4,   200] loss: 1.859 acc: 37.20 time: 7.53\n",
      "[4,   300] loss: 1.843 acc: 38.43 time: 7.58\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 40.19 %\n",
      "Average loss on the 10000 test images: 1.800\n",
      "[5,   100] loss: 1.842 acc: 37.79 time: 6.76\n",
      "[5,   200] loss: 1.842 acc: 38.09 time: 7.43\n",
      "[5,   300] loss: 1.839 acc: 38.61 time: 7.67\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 40.47 %\n",
      "Average loss on the 10000 test images: 1.792\n",
      "[6,   100] loss: 1.825 acc: 39.34 time: 6.75\n",
      "[6,   200] loss: 1.828 acc: 39.43 time: 7.45\n",
      "[6,   300] loss: 1.836 acc: 38.47 time: 7.58\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 40.26 %\n",
      "Average loss on the 10000 test images: 1.795\n",
      "[7,   100] loss: 1.820 acc: 39.27 time: 6.80\n",
      "[7,   200] loss: 1.824 acc: 39.68 time: 7.51\n",
      "[7,   300] loss: 1.816 acc: 39.32 time: 7.54\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 40.99 %\n",
      "Average loss on the 10000 test images: 1.774\n",
      "[8,   100] loss: 1.802 acc: 40.08 time: 6.69\n",
      "[8,   200] loss: 1.822 acc: 39.16 time: 7.55\n",
      "[8,   300] loss: 1.797 acc: 40.40 time: 7.57\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 41.71 %\n",
      "Average loss on the 10000 test images: 1.770\n",
      "[9,   100] loss: 1.802 acc: 40.45 time: 6.68\n",
      "[9,   200] loss: 1.806 acc: 40.41 time: 7.44\n",
      "[9,   300] loss: 1.791 acc: 40.82 time: 7.56\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 41.90 %\n",
      "Average loss on the 10000 test images: 1.763\n",
      "[10,   100] loss: 1.802 acc: 40.55 time: 6.93\n",
      "[10,   200] loss: 1.800 acc: 40.55 time: 7.49\n",
      "[10,   300] loss: 1.787 acc: 41.30 time: 7.56\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 41.91 %\n",
      "Average loss on the 10000 test images: 1.762\n",
      "[11,   100] loss: 1.780 acc: 40.97 time: 6.80\n",
      "[11,   200] loss: 1.761 acc: 42.80 time: 7.48\n",
      "[11,   300] loss: 1.755 acc: 42.66 time: 7.58\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 43.56 %\n",
      "Average loss on the 10000 test images: 1.720\n",
      "[12,   100] loss: 1.749 acc: 42.59 time: 6.71\n",
      "[12,   200] loss: 1.742 acc: 42.97 time: 7.36\n",
      "[12,   300] loss: 1.751 acc: 42.80 time: 7.41\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 44.18 %\n",
      "Average loss on the 10000 test images: 1.708\n",
      "[13,   100] loss: 1.726 acc: 44.16 time: 6.73\n",
      "[13,   200] loss: 1.734 acc: 43.27 time: 7.42\n",
      "[13,   300] loss: 1.746 acc: 42.48 time: 7.58\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 44.52 %\n",
      "Average loss on the 10000 test images: 1.701\n",
      "[14,   100] loss: 1.736 acc: 43.62 time: 6.81\n",
      "[14,   200] loss: 1.730 acc: 43.68 time: 7.44\n",
      "[14,   300] loss: 1.735 acc: 44.02 time: 7.66\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 44.73 %\n",
      "Average loss on the 10000 test images: 1.699\n",
      "[15,   100] loss: 1.719 acc: 44.40 time: 6.75\n",
      "[15,   200] loss: 1.733 acc: 43.72 time: 7.50\n",
      "[15,   300] loss: 1.729 acc: 43.23 time: 7.56\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 44.63 %\n",
      "Average loss on the 10000 test images: 1.698\n",
      "[16,   100] loss: 1.713 acc: 44.14 time: 6.77\n",
      "[16,   200] loss: 1.725 acc: 43.62 time: 7.40\n",
      "[16,   300] loss: 1.722 acc: 43.59 time: 7.58\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 44.95 %\n",
      "Average loss on the 10000 test images: 1.695\n",
      "[17,   100] loss: 1.719 acc: 43.77 time: 6.70\n",
      "[17,   200] loss: 1.720 acc: 44.43 time: 7.48\n",
      "[17,   300] loss: 1.713 acc: 44.30 time: 7.57\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 45.05 %\n",
      "Average loss on the 10000 test images: 1.692\n",
      "[18,   100] loss: 1.726 acc: 43.84 time: 6.76\n",
      "[18,   200] loss: 1.713 acc: 44.27 time: 7.43\n",
      "[18,   300] loss: 1.709 acc: 44.07 time: 7.56\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 45.20 %\n",
      "Average loss on the 10000 test images: 1.690\n",
      "[19,   100] loss: 1.715 acc: 44.08 time: 6.91\n",
      "[19,   200] loss: 1.721 acc: 44.18 time: 7.50\n",
      "[19,   300] loss: 1.704 acc: 45.03 time: 7.56\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 45.41 %\n",
      "Average loss on the 10000 test images: 1.687\n",
      "[20,   100] loss: 1.716 acc: 44.52 time: 6.58\n",
      "[20,   200] loss: 1.707 acc: 44.81 time: 7.23\n",
      "[20,   300] loss: 1.709 acc: 44.55 time: 7.39\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 45.18 %\n",
      "Average loss on the 10000 test images: 1.688\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(net, criterion, optimizer, num_epochs=20, decay_epochs=10, init_lr=0.01, task='classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcN54tcNN15U"
   },
   "source": [
    "## Supervised training on the pre-trained model (9 points)\n",
    "In this section, we will load the ResNet18 model pre-trained on the rotation task and re-train the whole model on the classification task.\n",
    "\n",
    "**Then we will use the trained model from rotation task as the pretrained weights. Notice, you should not use the pretrained weights from ImageNet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "9xR9h_S1N6Xi",
    "ExecuteTime": {
     "end_time": "2023-11-05T10:43:59.400540600Z",
     "start_time": "2023-11-05T10:43:59.133028100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "#####################################################\n",
    "#     TODO: Load the pre-trained ResNet18 model     #  \n",
    "#####################################################\n",
    "# load the trained classifier weights\n",
    "net = resnet18(weights=None, num_classes=4)\n",
    "ckpt = torch.load('model_2023-11-02-23-22-09.pth')\n",
    "net.load_state_dict(ckpt)\n",
    "\n",
    "# Adjusting the fully connected layer for 10 classes\n",
    "net.fc = nn.Linear(net.fc.in_features, 10)\n",
    "net = net.to(device)\n",
    "print(net) # print your model and check the num_classes is correct\n",
    "#####################################################\n",
    "#                End of your code                   #   \n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "gGozc2cM0ADw",
    "ExecuteTime": {
     "end_time": "2023-11-05T10:43:59.405565800Z",
     "start_time": "2023-11-05T10:43:59.400540600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "CrossEntropyLoss()"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Define criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Optimizer: Update only the parameters of layer4 and fc\n",
    "optimizer = optim.Adam(lr=0.001, params=net.parameters())\n",
    "\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "JGWW7gzCz_Bu",
    "ExecuteTime": {
     "end_time": "2023-11-05T10:54:53.903062Z",
     "start_time": "2023-11-05T10:43:59.406557600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.654 acc: 45.31 time: 7.22\n",
      "[1,   200] loss: 1.385 acc: 60.48 time: 7.52\n",
      "[1,   300] loss: 1.303 acc: 63.76 time: 7.40\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 66.45 %\n",
      "Average loss on the 10000 test images: 1.259\n",
      "[2,   100] loss: 1.220 acc: 67.86 time: 6.97\n",
      "[2,   200] loss: 1.184 acc: 69.91 time: 7.37\n",
      "[2,   300] loss: 1.174 acc: 69.82 time: 7.43\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 71.50 %\n",
      "Average loss on the 10000 test images: 1.152\n",
      "[3,   100] loss: 1.115 acc: 72.78 time: 7.13\n",
      "[3,   200] loss: 1.103 acc: 73.30 time: 7.47\n",
      "[3,   300] loss: 1.099 acc: 73.53 time: 7.53\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 74.93 %\n",
      "Average loss on the 10000 test images: 1.073\n",
      "[4,   100] loss: 1.061 acc: 74.88 time: 7.07\n",
      "[4,   200] loss: 1.036 acc: 76.88 time: 7.41\n",
      "[4,   300] loss: 1.056 acc: 75.31 time: 7.43\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 75.81 %\n",
      "Average loss on the 10000 test images: 1.050\n",
      "[5,   100] loss: 1.018 acc: 77.01 time: 7.30\n",
      "[5,   200] loss: 1.014 acc: 77.60 time: 7.41\n",
      "[5,   300] loss: 1.009 acc: 77.68 time: 7.39\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 75.40 %\n",
      "Average loss on the 10000 test images: 1.093\n",
      "[6,   100] loss: 0.999 acc: 78.44 time: 7.24\n",
      "[6,   200] loss: 0.991 acc: 78.22 time: 7.71\n",
      "[6,   300] loss: 0.982 acc: 78.62 time: 7.43\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 77.35 %\n",
      "Average loss on the 10000 test images: 1.029\n",
      "[7,   100] loss: 0.970 acc: 79.24 time: 6.89\n",
      "[7,   200] loss: 0.950 acc: 80.32 time: 7.37\n",
      "[7,   300] loss: 0.959 acc: 80.05 time: 7.43\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 79.53 %\n",
      "Average loss on the 10000 test images: 0.977\n",
      "[8,   100] loss: 0.939 acc: 80.77 time: 7.22\n",
      "[8,   200] loss: 0.949 acc: 80.33 time: 7.43\n",
      "[8,   300] loss: 0.941 acc: 80.60 time: 7.42\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 78.81 %\n",
      "Average loss on the 10000 test images: 0.978\n",
      "[9,   100] loss: 0.921 acc: 81.83 time: 7.02\n",
      "[9,   200] loss: 0.929 acc: 81.27 time: 7.42\n",
      "[9,   300] loss: 0.926 acc: 80.97 time: 7.44\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 80.25 %\n",
      "Average loss on the 10000 test images: 0.952\n",
      "[10,   100] loss: 0.897 acc: 82.62 time: 7.02\n",
      "[10,   200] loss: 0.910 acc: 82.29 time: 7.52\n",
      "[10,   300] loss: 0.918 acc: 81.82 time: 7.41\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 78.80 %\n",
      "Average loss on the 10000 test images: 0.990\n",
      "[11,   100] loss: 0.867 acc: 83.83 time: 6.99\n",
      "[11,   200] loss: 0.843 acc: 84.74 time: 7.38\n",
      "[11,   300] loss: 0.824 acc: 85.61 time: 7.44\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 82.90 %\n",
      "Average loss on the 10000 test images: 0.886\n",
      "[12,   100] loss: 0.818 acc: 85.87 time: 7.08\n",
      "[12,   200] loss: 0.823 acc: 85.91 time: 7.38\n",
      "[12,   300] loss: 0.809 acc: 86.55 time: 7.50\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 83.32 %\n",
      "Average loss on the 10000 test images: 0.879\n",
      "[13,   100] loss: 0.802 acc: 86.68 time: 7.15\n",
      "[13,   200] loss: 0.809 acc: 86.38 time: 7.42\n",
      "[13,   300] loss: 0.806 acc: 86.41 time: 7.43\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 83.33 %\n",
      "Average loss on the 10000 test images: 0.875\n",
      "[14,   100] loss: 0.786 acc: 87.25 time: 7.02\n",
      "[14,   200] loss: 0.798 acc: 86.84 time: 7.42\n",
      "[14,   300] loss: 0.794 acc: 86.92 time: 7.40\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 83.62 %\n",
      "Average loss on the 10000 test images: 0.873\n",
      "[15,   100] loss: 0.786 acc: 87.10 time: 7.30\n",
      "[15,   200] loss: 0.788 acc: 87.40 time: 7.40\n",
      "[15,   300] loss: 0.787 acc: 87.50 time: 7.45\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 83.60 %\n",
      "Average loss on the 10000 test images: 0.871\n",
      "[16,   100] loss: 0.785 acc: 87.31 time: 7.24\n",
      "[16,   200] loss: 0.784 acc: 87.60 time: 7.42\n",
      "[16,   300] loss: 0.784 acc: 87.48 time: 7.43\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 83.99 %\n",
      "Average loss on the 10000 test images: 0.871\n",
      "[17,   100] loss: 0.781 acc: 87.65 time: 7.14\n",
      "[17,   200] loss: 0.782 acc: 87.60 time: 7.44\n",
      "[17,   300] loss: 0.777 acc: 87.50 time: 7.44\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 83.78 %\n",
      "Average loss on the 10000 test images: 0.871\n",
      "[18,   100] loss: 0.768 acc: 88.23 time: 7.34\n",
      "[18,   200] loss: 0.777 acc: 87.88 time: 7.53\n",
      "[18,   300] loss: 0.786 acc: 87.36 time: 7.40\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 84.00 %\n",
      "Average loss on the 10000 test images: 0.869\n",
      "[19,   100] loss: 0.770 acc: 88.26 time: 7.18\n",
      "[19,   200] loss: 0.780 acc: 87.74 time: 7.55\n",
      "[19,   300] loss: 0.768 acc: 88.41 time: 7.41\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 84.40 %\n",
      "Average loss on the 10000 test images: 0.868\n",
      "[20,   100] loss: 0.762 acc: 88.37 time: 6.99\n",
      "[20,   200] loss: 0.756 acc: 88.73 time: 7.42\n",
      "[20,   300] loss: 0.768 acc: 88.14 time: 7.44\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 84.01 %\n",
      "Average loss on the 10000 test images: 0.869\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(net, criterion, optimizer, num_epochs=20, decay_epochs=10, init_lr=0.01, task='classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjVTp9jhefTi"
   },
   "source": [
    "## Supervised training on the randomly initialized model (9 points)\n",
    "In this section, we will randomly initialize a ResNet18 model and re-train the whole model on the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uEjy8TBieeLK",
    "ExecuteTime": {
     "end_time": "2023-11-05T10:54:54.042791Z",
     "start_time": "2023-11-05T10:54:53.902646800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "#################################################\n",
    "# TODO: Randomly initialize a ResNet18 model    #\n",
    "#################################################\n",
    "net = resnet18(weights=None, num_classes=10)\n",
    "net = net.to(device)\n",
    "print(net) # print your model and check the num_classes is correct\n",
    "#################################################\n",
    "#              End of your code                 #\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "jEY90pK_0ZAm",
    "ExecuteTime": {
     "end_time": "2023-11-05T10:54:54.048024100Z",
     "start_time": "2023-11-05T10:54:54.043791100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "CrossEntropyLoss()"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Define criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# Optimizer: Update only the parameters of layer4 and fc\n",
    "optimizer = optim.Adam(lr=0.001, params=net.parameters())\n",
    "\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "lMDwelhY0auO",
    "ExecuteTime": {
     "end_time": "2023-11-05T11:05:59.789229100Z",
     "start_time": "2023-11-05T10:54:54.049025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.279 acc: 23.41 time: 7.28\n",
      "[1,   200] loss: 1.926 acc: 34.48 time: 7.56\n",
      "[1,   300] loss: 1.802 acc: 39.67 time: 7.42\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 46.79 %\n",
      "Average loss on the 10000 test images: 1.657\n",
      "[2,   100] loss: 1.657 acc: 46.05 time: 7.21\n",
      "[2,   200] loss: 1.618 acc: 48.73 time: 7.45\n",
      "[2,   300] loss: 1.559 acc: 51.73 time: 7.45\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 56.04 %\n",
      "Average loss on the 10000 test images: 1.464\n",
      "[3,   100] loss: 1.441 acc: 57.27 time: 7.24\n",
      "[3,   200] loss: 1.408 acc: 58.66 time: 7.42\n",
      "[3,   300] loss: 1.371 acc: 60.68 time: 7.41\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 59.50 %\n",
      "Average loss on the 10000 test images: 1.420\n",
      "[4,   100] loss: 1.303 acc: 63.56 time: 7.22\n",
      "[4,   200] loss: 1.285 acc: 64.72 time: 7.45\n",
      "[4,   300] loss: 1.290 acc: 64.44 time: 7.43\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 69.16 %\n",
      "Average loss on the 10000 test images: 1.188\n",
      "[5,   100] loss: 1.225 acc: 67.01 time: 7.15\n",
      "[5,   200] loss: 1.215 acc: 67.70 time: 7.40\n",
      "[5,   300] loss: 1.203 acc: 68.19 time: 7.42\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 69.70 %\n",
      "Average loss on the 10000 test images: 1.169\n",
      "[6,   100] loss: 1.160 acc: 70.73 time: 7.24\n",
      "[6,   200] loss: 1.141 acc: 71.52 time: 7.42\n",
      "[6,   300] loss: 1.149 acc: 71.20 time: 7.41\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 71.83 %\n",
      "Average loss on the 10000 test images: 1.141\n",
      "[7,   100] loss: 1.127 acc: 72.51 time: 7.31\n",
      "[7,   200] loss: 1.105 acc: 72.95 time: 7.61\n",
      "[7,   300] loss: 1.100 acc: 73.11 time: 7.62\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 73.71 %\n",
      "Average loss on the 10000 test images: 1.096\n",
      "[8,   100] loss: 1.063 acc: 74.60 time: 7.54\n",
      "[8,   200] loss: 1.078 acc: 74.24 time: 7.58\n",
      "[8,   300] loss: 1.083 acc: 74.03 time: 7.59\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 70.94 %\n",
      "Average loss on the 10000 test images: 1.164\n",
      "[9,   100] loss: 1.054 acc: 75.57 time: 7.50\n",
      "[9,   200] loss: 1.053 acc: 75.84 time: 7.60\n",
      "[9,   300] loss: 1.037 acc: 76.56 time: 7.61\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 73.78 %\n",
      "Average loss on the 10000 test images: 1.083\n",
      "[10,   100] loss: 1.015 acc: 77.30 time: 7.45\n",
      "[10,   200] loss: 1.023 acc: 76.73 time: 7.63\n",
      "[10,   300] loss: 1.013 acc: 77.48 time: 7.69\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 74.53 %\n",
      "Average loss on the 10000 test images: 1.075\n",
      "[11,   100] loss: 0.961 acc: 79.02 time: 7.12\n",
      "[11,   200] loss: 0.917 acc: 81.23 time: 7.41\n",
      "[11,   300] loss: 0.915 acc: 81.72 time: 7.49\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 81.13 %\n",
      "Average loss on the 10000 test images: 0.926\n",
      "[12,   100] loss: 0.893 acc: 82.27 time: 7.28\n",
      "[12,   200] loss: 0.895 acc: 82.38 time: 7.53\n",
      "[12,   300] loss: 0.889 acc: 82.80 time: 7.58\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 81.45 %\n",
      "Average loss on the 10000 test images: 0.923\n",
      "[13,   100] loss: 0.887 acc: 82.98 time: 7.44\n",
      "[13,   200] loss: 0.873 acc: 83.23 time: 7.59\n",
      "[13,   300] loss: 0.871 acc: 83.56 time: 7.67\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 81.85 %\n",
      "Average loss on the 10000 test images: 0.913\n",
      "[14,   100] loss: 0.860 acc: 84.00 time: 7.29\n",
      "[14,   200] loss: 0.857 acc: 84.05 time: 7.61\n",
      "[14,   300] loss: 0.886 acc: 82.98 time: 7.54\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 81.84 %\n",
      "Average loss on the 10000 test images: 0.909\n",
      "[15,   100] loss: 0.851 acc: 84.61 time: 7.28\n",
      "[15,   200] loss: 0.850 acc: 84.63 time: 7.59\n",
      "[15,   300] loss: 0.859 acc: 84.07 time: 7.61\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 81.93 %\n",
      "Average loss on the 10000 test images: 0.907\n",
      "[16,   100] loss: 0.862 acc: 83.92 time: 7.13\n",
      "[16,   200] loss: 0.837 acc: 84.87 time: 7.54\n",
      "[16,   300] loss: 0.853 acc: 84.54 time: 7.59\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 81.75 %\n",
      "Average loss on the 10000 test images: 0.904\n",
      "[17,   100] loss: 0.842 acc: 85.03 time: 7.28\n",
      "[17,   200] loss: 0.842 acc: 84.63 time: 7.56\n",
      "[17,   300] loss: 0.836 acc: 84.95 time: 7.61\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 82.46 %\n",
      "Average loss on the 10000 test images: 0.899\n",
      "[18,   100] loss: 0.831 acc: 85.48 time: 7.58\n",
      "[18,   200] loss: 0.829 acc: 85.35 time: 7.56\n",
      "[18,   300] loss: 0.832 acc: 85.23 time: 7.55\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 82.41 %\n",
      "Average loss on the 10000 test images: 0.897\n",
      "[19,   100] loss: 0.825 acc: 85.76 time: 7.39\n",
      "[19,   200] loss: 0.823 acc: 85.77 time: 7.66\n",
      "[19,   300] loss: 0.827 acc: 85.70 time: 7.57\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 82.20 %\n",
      "Average loss on the 10000 test images: 0.897\n",
      "[20,   100] loss: 0.816 acc: 86.10 time: 7.11\n",
      "[20,   200] loss: 0.812 acc: 86.27 time: 7.52\n",
      "[20,   300] loss: 0.834 acc: 85.05 time: 7.59\n",
      "TESTING:\n",
      "Accuracy of the network on the 10000 test images: 82.69 %\n",
      "Average loss on the 10000 test images: 0.893\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(net, criterion, optimizer, num_epochs=20, decay_epochs=10, init_lr=0.01, task='classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write report (37 points)\n",
    "\n",
    "本次作業主要有3個tasks需要大家完成，在A4.pdf中有希望大家達成的baseline **(不能低於baseline最多2%，沒有達到不會給全部分數)**，report的撰寫請大家根據以下要求完成，就請大家將嘗試的結果寫在report裡，祝大家順利！\n",
    "\n",
    "1. (13 points) Train a ResNet18 on the Rotation task and report the test performance. Discuss why such a task helps in learning features that are generalizable to other visual tasks.\n",
    "\n",
    "2. (12 points) Initializing from the Rotation model or from random weights, fine-tune only the weights of the final block of convolutional layers and linear layer on the supervised CIFAR10 classification task. Report the test results and compare the performance of these two models. Provide your observations and insights. You can also discuss how the performance of pre-trained models affects downstream tasks, the performance of fine-tuning different numbers of layers, and so on.\n",
    "\n",
    "3. (12 points) Initializing from the Rotation model or from random weights, train the full network on the supervised CIFAR10 classification task. Report the test results and compare the performance of these two models. Provide your observations and insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit (13 points)\n",
    "\n",
    "上面基本的code跟report最高可以拿到87分，這個加分部分並沒有要求同學們一定要做，若同學們想要獲得更高的分數可以根據以下的加分要求來獲得加分。\n",
    "\n",
    "- In Figure 5(b) from the Gidaris et al. paper, the authors show a plot of CIFAR10 classification performance vs. number of training examples per category for a supervised CIFAR10 model vs. a RotNet model with the final layers fine-tuned on CIFAR10. The plot shows that pre-training on the Rotation task can be advantageous when only a small amount of labeled data is available. Using your RotNet fine-tuning code and supervised CIFAR10 training code from the main assignment, try to create a similar plot by performing supervised fine-tuning/training on only a subset of CIFAR10.\n",
    "- Use a more advanced model than ResNet18 to try to get higher accuracy on the rotation prediction task, as well as for transfer to supervised CIFAR10 classification.\n",
    "  \n",
    "- If you have a good amount of compute at your disposal, try to train a rotation prediction model on the larger ImageNette dataset (still smaller than ImageNet, though).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a balanced subset of the training set (Extra Credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 26\u001B[0m\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Subset(dataset, subset_indices)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# Create a balanced subset of the training set with 20 samples per class\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m balanced_train_set_20 \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_balanced_subset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mnum_samples_per_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m balanced_train_loader_20 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(balanced_train_set_20,\n\u001B[0;32m     29\u001B[0m                                                     batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m     30\u001B[0m                                                     shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     31\u001B[0m                                                     num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "Cell \u001B[1;32mIn[11], line 15\u001B[0m, in \u001B[0;36mcreate_balanced_subset\u001B[1;34m(dataset, num_samples_per_class)\u001B[0m\n\u001B[0;32m     12\u001B[0m class_indices \u001B[38;5;241m=\u001B[39m [[] \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m)]\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# Gather indices for each class\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx, (_, target) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataset):\n\u001B[0;32m     16\u001B[0m     class_indices[target]\u001B[38;5;241m.\u001B[39mappend(idx)\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Select num_samples_per_class indices for each class\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "def create_balanced_subset(dataset, num_samples_per_class):\n",
    "    \"\"\"\n",
    "    Create a balanced subset of the dataset with a specific number of samples per class.\n",
    "\n",
    "    :param dataset: The dataset from which to create the subset.\n",
    "    :param num_samples_per_class: The number of samples to include from each class.\n",
    "    :return: A Subset of the original dataset.\n",
    "    \"\"\"\n",
    "    class_indices = [[] for _ in range(10)]\n",
    "\n",
    "    # Gather indices for each class\n",
    "    for idx, (_, target) in enumerate(dataset):\n",
    "        class_indices[target].append(idx)\n",
    "\n",
    "    # Select num_samples_per_class indices for each class\n",
    "    subset_indices = []\n",
    "    for indices in class_indices:\n",
    "        subset_indices.extend(np.random.choice(indices, num_samples_per_class, replace=False))\n",
    "\n",
    "    return Subset(dataset, subset_indices)\n",
    "\n",
    "# Create a balanced subset of the training set with 20 samples per class\n",
    "balanced_trainset_20 = create_balanced_subset(trainset,\n",
    "                                            num_samples_per_class=20)\n",
    "balanced_trainloader_20 = torch.utils.data.DataLoader(balanced_trainset_20,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=True,\n",
    "                                                    num_workers=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T05:59:27.401934300Z",
     "start_time": "2023-11-08T05:59:27.078936100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1qQ3NEaCUln7yDSksrEOnDwrt3u9dYNr4",
     "timestamp": 1677623843954
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aaa478f9632825e83f6a2247407c7a2930de96a6810af7910643e423346524f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
