{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6Z1Snuk7rIK"
   },
   "source": [
    "# MIS 583 Assignment 6: Text Sentiment Classification with Prompt Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSwr9MgZogRZ"
   },
   "source": [
    "Before we start, please put your name and SID in following format: <br>\n",
    ": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷, M114020035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DzsjuDhlz_k"
   },
   "source": [
    "**Your Answer:**   \n",
    "Hi I'm 鄔仁迪, B104020009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d-Zzebq7rIM"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kc9gd_Wk7rIN"
   },
   "source": [
    "**Sentiment Classification** is an automated process of identifying opinions in text and labeling them as positive or negative based on the emotions customers express within them.\n",
    "\n",
    "In Task 1, you need to fine-tune a pre-trained language model (e.g., BERT) to predict the sentiment of given tweets.\n",
    "\n",
    "In Task 2, we employ prompts to enable the model to perform sentiment analysis through in-context learning, eliminating the need for additional training.\n",
    "\n",
    "In Task 3, you will use the method called LM-BFF to utilize the model in generating the optimal template and verbalizer autonomously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice \n",
    "**You are not allow to use the model like GPT family or pre-trained weight using SST-2 and twitter dataset!!!!!!!!!!!!!!!!!**\n",
    "\n",
    "You can use BERT and RoBERTa encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giUId1Naqacs",
    "tags": []
   },
   "source": [
    "##  Versions of used packages\n",
    "\n",
    "We will check PyTorch version to make sure everything work properly.  \n",
    "We use `python==3.7.14`, `torch==1.12.1+cu113` and `torchvision==0.13.1+cu113`.  \n",
    "This is the default version in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Vuw-gNvjqcYe",
    "ExecuteTime": {
     "end_time": "2023-12-13T16:38:50.028736800Z",
     "start_time": "2023-12-13T16:38:32.305392700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]\n",
      "torch 2.1.0+cu118\n",
      "torchvision 0.16.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "print('python', sys.version.split('\\n')[0])\n",
    "print('torch', torch.__version__)\n",
    "print('torchvision', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Text Sentiment Classification (40 points)\n",
    "\n",
    "In this task, you need to fine-tune a pre-trained language model (e.g., BERT or RoBERTa encoder) to predict the sentiment of given tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a4s_a5D7rIR"
   },
   "source": [
    "## Loading Model and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPUkTbnL7rIR"
   },
   "source": [
    "First, let's talk about the model. The Hugging Face team has created an amazing framework called \"transformers\" for NLP tasks. It includes many state-of-the-art machine learning models for PyTorch, TensorFlow, and JAX.\n",
    "\n",
    "To start with this package, follow [this link to installation and a basic tutorial](https://pytorch.org/hub/huggingface_pytorch-transformers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rK0ouXa09pDU",
    "ExecuteTime": {
     "end_time": "2023-12-15T16:26:28.590639300Z",
     "start_time": "2023-12-15T16:26:28.589638900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'!echo happy installation\\n!pip -V\\n!pip install grpcio\\n!pip install google-auth\\n!pip install protobuf==3.9.2\\n!pip install pyprind\\n!pip install tqdm boto3 requests regex sentencepiece sacremoses'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you might need some additional installations there\n",
    "\"\"\"!echo happy installation\n",
    "!pip -V\n",
    "!pip install grpcio\n",
    "!pip install google-auth\n",
    "!pip install protobuf==3.9.2\n",
    "!pip install pyprind\n",
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dmGCAevi7rIS",
    "ExecuteTime": {
     "end_time": "2023-12-15T16:26:38.039868700Z",
     "start_time": "2023-12-15T16:26:28.589638900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "#########################################################################\n",
    "#            Loading tokenizer and model from transformer               #\n",
    "#########################################################################\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, AutoConfig\n",
    "\n",
    "bert_type = 'roberta-base'\n",
    "\n",
    "# ---------- 1. load from torch.hub ----------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# create a Bert-extended task (classification)\n",
    "model = RobertaForSequenceClassification.from_pretrained(bert_type)\n",
    "\n",
    "# finetune from the output from bert to your task\n",
    "# Replace the out_proj layer in the classifier with a new Linear layer for 3 classes\n",
    "model.classifier.out_proj = nn.Linear(in_features=768, out_features=3, bias=True)\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiMThsYeDa2O"
   },
   "source": [
    "## How to Get Data\n",
    "\n",
    "Please open the file `twitter_sentiment.zip`, creat shortcut to your Google Drive.\n",
    "\n",
    "1. open [LINK of Google Drive](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
    "3. Select the location where you want to place the shortcut.\n",
    "4. Click Add shortcut.\n",
    "\n",
    "After above procedures, we have a shortcut of zip file of dataset.  \n",
    "We can access this in colab after granting the permission of Google Drive.\n",
    "\n",
    "---\n",
    "\n",
    "請先到共用雲端硬碟將檔案 `twitter_sentiment.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lZnFgi5i_2oA",
    "ExecuteTime": {
     "end_time": "2023-12-15T16:26:38.077877800Z",
     "start_time": "2023-12-15T16:26:38.031872Z"
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqO8DiB6VRQZ"
   },
   "source": [
    "## Unzip Data\n",
    "\n",
    "解壓縮 `twitter_sentiment.zip` 後可以發現裡面有三個csv檔。\n",
    "\n",
    "- `train.csv`, `test.csv` and `val.csv`\n",
    "\n",
    "Training set 有 **10248** 筆資料.  \n",
    "Validation set 有 **1317** 筆資料.  \n",
    "Testing set 有 **3075** 筆資料.  \n",
    "\n",
    "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OSlTMdxf8Zd7",
    "ExecuteTime": {
     "end_time": "2023-12-15T16:26:38.112868800Z",
     "start_time": "2023-12-15T16:26:38.045868300Z"
    }
   },
   "outputs": [],
   "source": [
    "# !unzip -qq ./drive/MyDrive/twitter_sentiment.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wf5GXTme7rIT",
    "ExecuteTime": {
     "end_time": "2023-12-15T16:26:38.112868800Z",
     "start_time": "2023-12-15T16:26:38.073870300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to extract text and label from csv file\n",
    "def get_texts(f_name='./twitter_sentiment', mode='train'):\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "\n",
    "    f_path = os.path.join(f_name, '{}.csv'.format(mode))\n",
    "    with open(f_path, encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for line in reader:\n",
    "            text_list.append(line['text'])\n",
    "            if mode != 'test':\n",
    "                label_list.append(int(line['sentiment_label']))\n",
    "\n",
    "    return text_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6fpY0ZrK7rIV",
    "ExecuteTime": {
     "end_time": "2023-12-15T16:26:38.113869400Z",
     "start_time": "2023-12-15T16:26:38.081871200Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, f_name='./twitter_sentiment', mode='train'):\n",
    "        self.mode = mode\n",
    "\n",
    "        text_list, label_list = get_texts(f_name, mode)\n",
    "        print('mode', mode, 'has', len(text_list), 'datas')\n",
    "        text_list = tokenizer(text_list,\n",
    "                             truncation=True, padding=True,\n",
    "                             return_tensors='pt')\n",
    "\n",
    "        self.text_list = text_list['input_ids']\n",
    "        self.mask_list = text_list['attention_mask']\n",
    "\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_list[idx]\n",
    "        mask = self.mask_list[idx]\n",
    "        if self.mode == 'test':\n",
    "            return text, mask\n",
    "        label = torch.tensor(self.label_list[idx])\n",
    "        return text, mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nCmM4FSw7rIW",
    "ExecuteTime": {
     "end_time": "2023-12-15T16:26:39.154910800Z",
     "start_time": "2023-12-15T16:26:38.094870600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode train has 10248 datas\n",
      "mode val has 1317 datas\n",
      "mode test has 3075 datas\n"
     ]
    }
   ],
   "source": [
    "dataset_train = TwitterDataset(mode='train')\n",
    "dataset_val = TwitterDataset(mode='val')\n",
    "dataset_test = TwitterDataset(mode='test')\n",
    "\n",
    "batch_size = 24\n",
    "train_data = DataLoader(dataset_train, batch_size=batch_size,\n",
    "                       shuffle=True)\n",
    "val_data = DataLoader(dataset_val, batch_size=batch_size // 2,\n",
    "                       shuffle=False)\n",
    "test_data = DataLoader(dataset_test, batch_size=batch_size // 2,\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bqkvofHc7rIY",
    "ExecuteTime": {
     "end_time": "2023-12-15T16:26:39.233908400Z",
     "start_time": "2023-12-15T16:26:39.154910800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['<s>', '@', 'united', 'ĠI', 'Ġhave', 'Ġnever', 'Ġbeen', 'Ġmislead', 'Ġby', 'Ġa', 'Ġcompany', 'Ġas', 'Ġmany', 'Ġtimes', 'Ġas', 'ĠI', 'Ġhave', 'Ġthis', 'Ġweek', 'Ġby', 'ĠUnited', 'ĠAirlines', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "token to s <s>@united I have never been mislead by a company as many times as I have this week by United Airlines!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(dataset_train[0][0])\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DxZrfCqW7rIY",
    "ExecuteTime": {
     "end_time": "2023-12-15T16:26:41.140909700Z",
     "start_time": "2023-12-15T16:26:39.187909200Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# 定義標籤平滑化的KL損失函數 Paper: https://arxiv.org/pdf/2312.06522.pdf\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = -1\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n",
    "# 設定標籤平滑化水平\n",
    "\"\"\"\n",
    "LS2: smoothing=0.03（即3%平滑化）\n",
    "LS3: smoothing=0.075（即7.5%平滑化）\n",
    "LS4: smoothing=0.15（即15%平滑化）\n",
    "LS5: smoothing=0.3（即30%平滑化）\n",
    "\"\"\"\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpwgE2Gd7rIZ"
   },
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zlaiAZAD7rIa",
    "ExecuteTime": {
     "end_time": "2023-12-15T16:26:41.145908900Z",
     "start_time": "2023-12-15T16:26:41.140909700Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(raw_preds, y):\n",
    "    preds = raw_preds.argmax(dim=1)\n",
    "    acc = (preds == y).sum()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dmc_Gms97rIa",
    "ExecuteTime": {
     "end_time": "2023-12-15T16:26:42.228916700Z",
     "start_time": "2023-12-15T16:26:41.152909900Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Testing process                              #\n",
    "        #########################################################################\n",
    "        # 1. Clean the gradients of optimizer\n",
    "        # 2. Put correct variables into model\n",
    "        # 3. Get prediction\n",
    "        # 4. Evalutate by criterion and accuracy\n",
    "\n",
    "        # Clean the gradients of optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(text, attention_mask=mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.logits, label)\n",
    "\n",
    "        # Compute accuracy using the provided function\n",
    "        acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "def test(model, data, criterion, log_loss=False):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Training process                             #\n",
    "        #########################################################################\n",
    "        # 1. Put correct variables into model\n",
    "        # 2. Get prediction\n",
    "        # 3. Evalutate by criterion and accuracy\n",
    "\n",
    "        # No gradient calculation for evaluation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(text, attention_mask=mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.logits, label)\n",
    "\n",
    "            # Compute accuracy\n",
    "            acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if log_loss:\n",
    "            val_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "# class for monitoring train and test acc/loss\n",
    "class Meter:\n",
    "    def __init__(self):\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def update(self, train_loss, train_acc, val_loss, val_acc):\n",
    "        self.train_loss_list.append(train_loss)\n",
    "        self.train_acc_list.append(train_acc)\n",
    "        self.val_loss_list.append(val_loss)\n",
    "        self.val_acc_list.append(val_acc)\n",
    "\n",
    "    def plot(self):\n",
    "        x = range(len(self.train_loss_list))\n",
    "        plt.plot(x, self.train_loss_list)\n",
    "        plt.plot(x, self.val_loss_list, color='r')\n",
    "        plt.legend(['train_loss', 'val_loss'])\n",
    "        plt.show()\n",
    "        plt.plot(x, self.train_acc_list)\n",
    "        plt.plot(x, self.val_acc_list, color='r')\n",
    "        plt.legend(['train_acc', 'val_acc'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExZyrKd57rIb"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bVDe-fRe7rIc",
    "ExecuteTime": {
     "end_time": "2023-12-15T16:41:30.830481Z",
     "start_time": "2023-12-15T16:26:46.910436300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:41<00:00,  2.64it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 33.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 0.027199020622346106 train_acc: 0.7897150663544106\n",
      "Epoch 1 val_loss:  0.046113631651993496 val_acc : 0.85041761579347\n",
      "---------- e 1 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:54<00:00,  2.44it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 33.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss: 0.02207990615082662 train_acc: 0.867096018735363\n",
      "Epoch 2 val_loss:  0.04733145128350052 val_acc : 0.8451025056947609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:55<00:00,  2.43it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 33.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss: 0.020277466177126096 train_acc: 0.8934426229508197\n",
      "Epoch 3 val_loss:  0.04593462546213884 val_acc : 0.8580106302201974\n",
      "---------- e 3 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:56<00:00,  2.42it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 33.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss: 0.01847000606794268 train_acc: 0.9193989071038251\n",
      "Epoch 4 val_loss:  0.04958810766447468 val_acc : 0.85041761579347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:56<00:00,  2.42it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 33.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss: 0.017050756811862994 train_acc: 0.9388173302107728\n",
      "Epoch 5 val_loss:  0.04923145485449308 val_acc : 0.8526955201214882\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#                          Hyper-parameters                             #\n",
    "#########################################################################\n",
    "max_epoch = 5\n",
    "log_interval = 1\n",
    "best_acc = 0\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################\n",
    "\n",
    "m = Meter()\n",
    "\n",
    "for epoch in range(1, max_epoch + 1):\n",
    "    train_loss, train_acc = train(model, train_data, optimizer, criterion)\n",
    "    val_loss, val_acc = test(model, val_data, criterion, log_loss=True)\n",
    "\n",
    "    if epoch % log_interval == 0:\n",
    "        print('Epoch {} train_loss: {} train_acc: {}'.format(\n",
    "            epoch, train_loss, train_acc\n",
    "        ))\n",
    "        print('Epoch {} val_loss:  {} val_acc : {}'.format(\n",
    "            epoch, val_loss, val_acc\n",
    "        ))\n",
    "\n",
    "    m.update(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "    # model checkpoint\n",
    "    if val_acc > best_acc:\n",
    "        best_model = model\n",
    "        best_acc = val_acc\n",
    "        print('-'*10, 'e', epoch, 'save best model', '-'*10)\n",
    "        torch.save(model.state_dict(), 'ckpts/e{}.pt'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SmtW58OR7rIc",
    "ExecuteTime": {
     "end_time": "2023-12-15T16:41:35.800474200Z",
     "start_time": "2023-12-15T16:41:35.501415800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot them out\n",
    "m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcJcHf7n7rId"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/f072e95f51bc48978225941dba218241).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = model\n",
    "best_model.load_state_dict(torch.load('ckpts/e3.pt'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T16:41:54.728791100Z",
     "start_time": "2023-12-15T16:41:53.576739200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Sf5UTlMZ7rId",
    "ExecuteTime": {
     "end_time": "2023-12-15T16:42:02.583018300Z",
     "start_time": "2023-12-15T16:41:56.148858600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257/257 [00:06<00:00, 40.03it/s]\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "\n",
    "total_out = []\n",
    "for text, mask in tqdm(test_data, total=len(test_data)):\n",
    "    text = text.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    output = best_model(text, mask)\n",
    "    pred = output.logits\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    total_out.append(pred)\n",
    "\n",
    "total_out = torch.cat(total_out).cpu().numpy().tolist()\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(total_out):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:41:35.870336300Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Save best model\n",
    "torch.save(best_model.state_dict(), 'ckpts/best_roberta_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: In-Context learning (32 points)\n",
    "\n",
    "In this task, you will learn how to perform sentiment classification using prompts without the need for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:41:35.882334700Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyprind\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertTokenizer, BertForMaskedLM\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.591798400Z",
     "start_time": "2023-12-11T06:59:54.213731500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#         TODO: Design your own template(prefix) and verbalizer         #\n",
    "#########################################################################\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.prefix = 'This sentence is [MASK].'  # you can modify this line\n",
    "        self.verbalizer = {\n",
    "            'disappointing': 0,\n",
    "            'neutral': 1,\n",
    "            'perfect': 2,\n",
    "        }\n",
    "\n",
    "        self.max_seq_length = 512\n",
    "        self.batch_size = 32\n",
    "\n",
    "\n",
    "config = Config()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bert_type = 'bert-base-uncased'\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(bert_type, num_labels=3)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(bert_type)\n",
    "\n",
    "bert = model.from_pretrained(bert_type, config=bert_config).to(device)\n",
    "\n",
    "#######################################################################\n",
    "#                        End of your code                             #\n",
    "#######################################################################\n",
    "\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.601784800Z",
     "start_time": "2023-12-11T07:00:00.592783400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to obtaion verbalizer ids\n",
    "def obtain_verbalizer_ids(verbalizer, tokenizer):\n",
    "    verbalizer_ids = tokenizer.convert_tokens_to_ids(list(verbalizer.keys()))\n",
    "    index2ids = {i: verbalizer_ids[i] for i in range(len(verbalizer_ids))}\n",
    "    return verbalizer_ids, index2ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.605878500Z",
     "start_time": "2023-12-11T07:00:00.597783400Z"
    }
   },
   "outputs": [],
   "source": [
    "verbalizer_ids, index2ids = obtain_verbalizer_ids(config.verbalizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.615507200Z",
     "start_time": "2023-12-11T07:00:00.607878800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to concatenate prefix and text\n",
    "def concatenate_prefix(texts, config):\n",
    "    ##################################################\n",
    "    #   TODO: concatenate your own prefix and text   #                               \n",
    "    ##################################################\n",
    "    prefix_texts = [config.prefix + \" \" + text for text in texts]\n",
    "    ##################################################\n",
    "    #                 End of your code               #                               \n",
    "    ##################################################\n",
    "    return prefix_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.694028Z",
     "start_time": "2023-12-11T07:00:00.617508700Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(config):\n",
    "    # ['texts', 'labels']\n",
    "    df = pd.read_csv('./twitter_sentiment/train.csv')\n",
    "    original_texts = df['text'].tolist()\n",
    "    labels = df['sentiment_label'].tolist()\n",
    "\n",
    "    texts = concatenate_prefix(original_texts, config)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "texts, labels = load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.706026300Z",
     "start_time": "2023-12-11T07:00:00.696029900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['[CLS]', 'it', 'was', '[MASK]', 'sentence', '.', '@', 'united', 'i', 'have', 'never', 'been', 'mis', '##lea', '##d', 'by', 'a', 'company', 'as', 'many', 'times', 'as', 'i', 'have', 'this', 'week', 'by', 'united', 'airlines', '!', '[SEP]']\n",
      "token to s [CLS] it was [MASK] sentence . @ united i have never been mislead by a company as many times as i have this week by united airlines ! [SEP]\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(tokenizer.encode(texts[0], add_special_tokens=True))\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.718528600Z",
     "start_time": "2023-12-11T07:00:00.712026600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Batching of texts and labels for training or processing in batches\n",
    "def pack_batch(texts, labels, batch_size):\n",
    "    \"\"\"\n",
    "    :param texts: list\n",
    "    :param labels: list\n",
    "    :param batch_size: int\n",
    "    :return batch_X: list\n",
    "            [[text11, text12, ...], [text21, text22, ...], ...]\n",
    "    :return batch_y: list\n",
    "            [[label11, label12, ...], [label21, label22, ...], ...]\n",
    "    :return batch_count: int\n",
    "    \"\"\"\n",
    "    assert len(texts) == len(labels)\n",
    "\n",
    "    if len(texts) % batch_size != 0:\n",
    "        flag = False\n",
    "        batch_count = int(len(texts) / batch_size) + 1\n",
    "    else:\n",
    "        flag = True\n",
    "        batch_count = int(len(texts) / batch_size)\n",
    "\n",
    "    batch_X, batch_y = [], []\n",
    "\n",
    "    if flag:\n",
    "        for i in range(batch_count):\n",
    "            batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "            batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "    else:\n",
    "        for i in range(batch_count):\n",
    "            if i == batch_count - 1:\n",
    "                batch_X.append(texts[i * batch_size:])\n",
    "                batch_y.append(labels[i * batch_size:])\n",
    "            else:\n",
    "                batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "                batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "\n",
    "    return batch_X, batch_y, batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.754528500Z",
     "start_time": "2023-12-11T07:00:00.720528100Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_X, batch_y, batch_count = pack_batch(texts, labels, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:04:58.906945500Z",
     "start_time": "2023-12-11T07:00:00.739529800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[100 %] Time elapsed: 00:04:58 | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.456967 | precision: 0.606231 | recall: 0.456967 | f1: 0.463890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:04:58\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    pper = pyprind.ProgPercent(batch_count)\n",
    "    for i in range(batch_count):\n",
    "        inputs = batch_X[i]\n",
    "        labels = batch_y[i]\n",
    "\n",
    "        tokens = tokenizer.batch_encode_plus(inputs, add_special_tokens=True,\n",
    "                                             max_length=config.max_seq_length,\n",
    "                                             padding='max_length', truncation=True)\n",
    "        \n",
    "        ids = torch.tensor(tokens['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(tokens['attention_mask']).to(device)\n",
    "\n",
    "        # Shape: (batch_size, max_seq_length, vocab_size)\n",
    "        logits = bert(ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        mask_token_index = (ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "\n",
    "        # Find [MASK] logits\n",
    "        # shape: (batch_size, vocab_size)\n",
    "        masked_logits = logits[mask_token_index[0], mask_token_index[1], :]\n",
    "\n",
    "        # Extract the logits of the word in the verbalizer at the [MASK] position\n",
    "        # shape: (batch_size, verbalizer_size)\n",
    "        verbalizer_logits = masked_logits[:, verbalizer_ids]\n",
    "\n",
    "        # Construct a pseudo-distribution from the logits in these verbalizers\n",
    "        pseudo_distribution = softmax(verbalizer_logits)\n",
    "\n",
    "        #################################################################################\n",
    "        #   1. Find the index with the maximum probability in the pseudo-distribution   #\n",
    "        #   2. Convert the index to the corresponding word ID                           #\n",
    "        #   3. Convert the ID to a token                                                #\n",
    "        #   4. Find the label corresponding to the token                                #\n",
    "        #################################################################################\n",
    "        # 1. Find the index with the maximum probability in the pseudo-distribution\n",
    "        pred_indices = torch.argmax(pseudo_distribution, dim=1)\n",
    "\n",
    "        # 2. Convert the index to the corresponding word ID\n",
    "        pred_ids = [verbalizer_ids[index] for index in pred_indices]\n",
    "\n",
    "        # 3. Convert the ID to a token\n",
    "        pred_tokens = tokenizer.convert_ids_to_tokens(pred_ids)\n",
    "\n",
    "        # 4. Find the label corresponding to the token\n",
    "        pred_labels = [config.verbalizer[token] if token in config.verbalizer else -1 for token in pred_tokens]\n",
    "        pred_labels = np.array(pred_labels)\n",
    "        #################################################################################\n",
    "        #                             End of your code                                  #\n",
    "        #################################################################################\n",
    "\n",
    "        predict_all = np.append(predict_all, pred_labels)\n",
    "        labels_all = np.append(labels_all, labels)\n",
    "\n",
    "        pper.update()\n",
    "\n",
    "    acc = accuracy_score(labels_all, predict_all)\n",
    "    p = precision_score(labels_all, predict_all, average=\"weighted\")\n",
    "    r = recall_score(labels_all, predict_all, average=\"weighted\")\n",
    "    f1 = f1_score(labels_all, predict_all, average=\"weighted\")\n",
    "\n",
    "    print('accuracy: %f | precision: %f | recall: %f | f1: %f' % (acc, p, r, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:04:59.175549700Z",
     "start_time": "2023-12-11T07:04:58.906945500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10243</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10244</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10245</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10246</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10247</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10248 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1\n",
       "0      0  2\n",
       "1      0  2\n",
       "2      2  2\n",
       "3      1  0\n",
       "4      0  2\n",
       "...   .. ..\n",
       "10243  0  2\n",
       "10244  0  1\n",
       "10245  0  0\n",
       "10246  2  2\n",
       "10247  1  1\n",
       "\n",
       "[10248 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([labels_all, predict_all]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: LM-BFF (45 points)\n",
    "\n",
    "https://arxiv.org/pdf/2012.15723.pdf\n",
    "\n",
    "Unlike the previous task, LM-BFF can generate templates and verbalizers automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請先到共用雲端硬碟將檔案 `SST-2.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/14MDYFasXU94dUE9DjgfcZE61iTRI2007/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install openprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T03:31:13.543193800Z",
     "start_time": "2023-12-09T03:30:21.808076200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openprompt\n",
      "  Downloading openprompt-1.0.1-py3-none-any.whl (146 kB)\n",
      "     ---------------------------------------- 0.0/146.4 kB ? eta -:--:--\n",
      "     ---------------- ---------------------- 61.4/146.4 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 146.4/146.4 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting transformers>=4.10.0 (from openprompt)\n",
      "  Obtaining dependency information for transformers>=4.10.0 from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 123.5/123.5 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting sentencepiece==0.1.96 (from openprompt)\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 0.1/1.1 MB 2.2 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.2/1.1 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.3/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.4/1.1 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.5/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 0.6/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.6/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 0.7/1.1 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 0.8/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 0.9/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.0/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.1/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.1/1.1 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting tqdm>=4.62.2 (from openprompt)\n",
      "  Obtaining dependency information for tqdm>=4.62.2 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting tensorboardX (from openprompt)\n",
      "  Obtaining dependency information for tensorboardX from https://files.pythonhosted.org/packages/44/71/f3e7c9b2ab67e28c572ab4e9d5fa3499e0d252650f96d8a3a03e26677f53/tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nltk (from openprompt)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting yacs (from openprompt)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting dill (from openprompt)\n",
      "  Obtaining dependency information for dill from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting datasets (from openprompt)\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting rouge==1.0.0 (from openprompt)\n",
      "  Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting pyarrow (from openprompt)\n",
      "  Obtaining dependency information for pyarrow from https://files.pythonhosted.org/packages/28/82/9adfafaf0de581a39a1c86002cafd1a55a1255c9e0d362dc3e970aab0656/pyarrow-14.0.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading pyarrow-14.0.1-cp39-cp39-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openprompt) (1.8.0)\n",
      "Requirement already satisfied: six in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rouge==1.0.0->openprompt) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\eddie\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.62.2->openprompt) (0.4.4)\n",
      "Collecting filelock (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/fc/85/0d1038f068900896a8590d6d0da198b90d31f731a39166a432aa2b92249b/regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (2.25.1)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/9f/90/a6821e7757d2db194c16cbca78c80e206f30f6cc62c7f15fb27428f8c6dd/tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/4e/96/f4ee4434d8b6452fe7d5d44df2e72d1c6b2add1c3a5fb5c81aae83cb90c6/safetensors-0.4.1-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.1-cp39-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->openprompt)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets->openprompt) (1.4.1)\n",
      "Collecting xxhash (from datasets->openprompt)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/33/db/e07aa80e39a7ee82e9ca6f5a0fa9bf0b4465934272116a1b578eaee7f4b3/xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->openprompt)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/c6/c9/820b5ab056f4ada76fbe05bd481a948f287957d6cbfd59e2dd2618b408c1/multiprocess-0.70.15-py39-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets->openprompt)\n",
      "  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets->openprompt) (3.7.4.post0)\n",
      "Requirement already satisfied: click in c:\\users\\eddie\\appdata\\roaming\\python\\python39\\site-packages (from nltk->openprompt) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->openprompt) (1.1.0)\n",
      "Collecting protobuf>=3.20 (from tensorboardX->openprompt)\n",
      "  Obtaining dependency information for protobuf>=3.20 from https://files.pythonhosted.org/packages/b6/4b/f4f3334784576822d7817a664b757030ebb35b981978baf9c2eb3c5f33a8/protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (21.2.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (3.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (5.2.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (1.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers>=4.10.0->openprompt) (3.0.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets->openprompt) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets->openprompt) (2021.3)\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 41.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/7.9 MB 3.2 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.2/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.4/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.4/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.6/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.7/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.8/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.9/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.0/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.1/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.2/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.3/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.4/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.5/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.6/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.7/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.8/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.9/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.2/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.3/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.4/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.5/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.6/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.7/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.8/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.9/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.2/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.3/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.4/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.6/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.7/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.9/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.0/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.1/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.2/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.3/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.5/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.6/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.9/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.0/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.1/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.2/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.3/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.5/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.6/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.7/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.9/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.0/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.1/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.3/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.4/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.5/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.6/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.7/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.8/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.9/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.0/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.3/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.4/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.5/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.6/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.7/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.8/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "   ---------------------------------------- 0.0/521.2 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 92.2/521.2 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 174.1/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 276.5/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 368.6/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 471.0/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 521.2/521.2 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 61.4/115.3 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 115.3/115.3 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow-14.0.1-cp39-cp39-win_amd64.whl (24.6 MB)\n",
      "   ---------------------------------------- 0.0/24.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/24.6 MB 3.2 MB/s eta 0:00:08\n",
      "   ---------------------------------------- 0.2/24.6 MB 2.6 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 0.3/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.4/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.5/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.6/24.6 MB 2.2 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.7/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.8/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.8/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.9/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.0/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.1/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.2/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.3/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.4/24.6 MB 1.9 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.4/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.7/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.8/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.9/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.0/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.1/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.2/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.3/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.4/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.5/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.6/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.7/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.8/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.9/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 3.0/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.2/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.3/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.4/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.5/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.6/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.7/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.8/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.9/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.0/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.2/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.3/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.4/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.5/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.6/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.8/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.9/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.0/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.1/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.2/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.3/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.4/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.5/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.6/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.8/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.9/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.0/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.1/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.2/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.3/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.4/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.5/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.6/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.7/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 6.8/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 6.9/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.0/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.1/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.2/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.3/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.5/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.6/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.7/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.8/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.9/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.0/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.1/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.2/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.3/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.5/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 8.6/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.7/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.8/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.3/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.4/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.5/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.6/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.7/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.8/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 9.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.0/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.3/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.4/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 10.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.6/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.7/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.8/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.9/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 11.0/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.1/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.2/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.4/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.6/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.7/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 11.8/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 11.9/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.0/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.1/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.2/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.7/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.8/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.0/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.1/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.3/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.7/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.8/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.1/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.3/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 14.8/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 14.9/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.0/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.1/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.2/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.3/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.4/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.5/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.8/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.9/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.0/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.1/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.2/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.3/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.4/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.5/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 16.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 16.8/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 16.9/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.0/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.1/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.3/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.4/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.5/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.6/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.7/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.8/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 17.9/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.0/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.1/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.3/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.4/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.5/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.6/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.7/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.7/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.8/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 19.0/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.1/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.2/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.3/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.4/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.5/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.7/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.8/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.0/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.1/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.2/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.3/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.4/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.5/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.7/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.9/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.0/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.1/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.2/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.3/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.4/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.5/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.9/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.0/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.1/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.2/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.3/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.4/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.5/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.7/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.8/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.2/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.3/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.4/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.5/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.6/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.7/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.8/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.2/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.5/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.6/24.6 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.7 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 92.2/101.7 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 101.7/101.7 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 0.0/311.7 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 112.6/311.7 kB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 194.6/311.7 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  307.2/311.7 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 311.7/311.7 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 112.6/413.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 194.6/413.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 307.2/413.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 368.6/413.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 413.4/413.4 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 92.2/269.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 174.1/269.6 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/269.6 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 269.6/269.6 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp39-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.8 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 112.6/277.8 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 194.6/277.8 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 277.8/277.8 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.2 MB 1.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 1.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.4/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.3 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 92.2/133.3 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 133.3/133.3 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl (29 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 71.7/166.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 166.4/166.4 kB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, yacs, xxhash, tqdm, safetensors, rouge, regex, pyarrow-hotfix, pyarrow, protobuf, fsspec, filelock, dill, tensorboardX, nltk, multiprocess, huggingface-hub, tokenizers, transformers, datasets, openprompt\n",
      "Successfully installed datasets-2.15.0 dill-0.3.7 filelock-3.13.1 fsspec-2023.10.0 huggingface-hub-0.19.4 multiprocess-0.70.15 nltk-3.8.1 openprompt-1.0.1 protobuf-4.25.1 pyarrow-14.0.1 pyarrow-hotfix-0.6 regex-2023.10.3 rouge-1.0.0 safetensors-0.4.1 sentencepiece-0.1.96 tensorboardX-2.6.2.2 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.35.2 xxhash-3.4.1 yacs-0.1.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install openprompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import openprompt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.181341Z",
     "start_time": "2023-12-10T03:35:52.654085900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.194342100Z",
     "start_time": "2023-12-10T03:36:10.182341900Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda = True\n",
    "auto_t = True # Whether to perform automatic template generation\n",
    "auto_v = True # Whether to perform automatic verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.226340200Z",
     "start_time": "2023-12-10T03:36:10.195343500Z"
    }
   },
   "outputs": [],
   "source": [
    "from openprompt.data_utils.text_classification_dataset import SST2Processor\n",
    "dataset = {'train': SST2Processor().get_train_examples(\"SST-2/\"),\n",
    "           'validation': SST2Processor().get_dev_examples(\"SST-2/\"),\n",
    "           'test': SST2Processor().get_test_examples(\"SST-2/\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.607530300Z",
     "start_time": "2023-12-10T03:36:10.228340900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: {\n",
      "  \"guid\": \"train-0\",\n",
      "  \"label\": 0,\n",
      "  \"meta\": {\n",
      "    \"labelword\": \"terrible\"\n",
      "  },\n",
      "  \"text_a\": \"nothing happens , and it happens to flat characters .\",\n",
      "  \"text_b\": \"\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print('load model...')\n",
    "from openprompt.plms import load_plm\n",
    "# load mlm model for main tasks\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")\n",
    "\n",
    "# load generation model for template generation\n",
    "template_generate_model, template_generate_tokenizer, template_generate_model_config, template_tokenizer_wrapper = load_plm('t5', 't5-large')\n",
    "\n",
    "from openprompt.prompts import ManualVerbalizer, ManualTemplate\n",
    "verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "#   TODO: You need to switch LMBFFTemplateGenerationTemplate or ManualTemplate to                                 #\n",
    "#         compare auto generate template and manual generate template                                             #\n",
    "###################################################################################################################\n",
    "from openprompt.prompts.prompt_generator import LMBFFTemplateGenerationTemplate\n",
    "########################################\n",
    "#   LMBFFTemplateGenerationTemplate    #\n",
    "########################################\n",
    "import random\n",
    "\n",
    "# number of demonstrations\n",
    "num_demonstrations = 1  # try different number\n",
    "\n",
    "demonstrations = []\n",
    "\n",
    "for _ in range(num_demonstrations):\n",
    "    # random choice training set example with label 0 \n",
    "    random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n",
    "\n",
    "    # random choice training set example with label 1\n",
    "    random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n",
    "    \n",
    "    demonstration = f'{random_example_1.text_a} It was terrible. {random_example_2.text_a} It was great.'\n",
    "    demonstrations.append(demonstration)\n",
    "\n",
    "# You can modify the demonstrations and try different combinations\n",
    "template_text = '{\"placeholder\": \"text_a\"} {\"mask\"} {\"meta\": \"labelword\"} {\"mask\"}.' + ' '.join(demonstrations)\n",
    "template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "\n",
    "#############################################\n",
    "#   End of LMBFFTemplateGenerationTemplate  #\n",
    "#############################################\n",
    "\n",
    "########################################\n",
    "#          ManualTemplate              #\n",
    "########################################\n",
    "\n",
    "#template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} It was {\"mask\"}.')\n",
    "\n",
    "#############################################\n",
    "#          End of ManualTemplate            # \n",
    "#############################################\n",
    "\n",
    "###################################################################################################################\n",
    "#                                           End of your code                                                      #\n",
    "###################################################################################################################\n",
    "\n",
    "\n",
    "# view wrapped example\n",
    "wrapped_example = template.wrap_one_example(dataset['train'][0])\n",
    "print(\"dataset:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.673523500Z",
     "start_time": "2023-12-10T03:36:46.625523700Z"
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "\n",
    "# Returns the best evaluation score achieved during training\n",
    "def fit(model, train_dataloader, val_dataloader, loss_func, optimizer, epochs=5):\n",
    "    best_score = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_dataloader, loss_func, optimizer)\n",
    "        score = evaluate(model, val_dataloader)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "        print(f\"Epoch {epoch+1}: Train loss={train_loss}, Eval score={score}\")\n",
    "    return best_score\n",
    "\n",
    "# Trains the model on the training data and computes the training loss\n",
    "def train_epoch(model, train_dataloader, loss_func, optimizer):\n",
    "    model.train()\n",
    "    loss_all = []\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits #\n",
    "        # 2. Get labels                                     #\n",
    "        # 3. Evalutate using loss_func                      #\n",
    "        # 4. Append loss to loss_all                        #\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits\n",
    "        logits = model(batch=inputs)\n",
    "\n",
    "        # 2. Get labels\n",
    "        labels = inputs['label']\n",
    "\n",
    "        # 3. Evalutate using loss_func\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Append loss to loss_all\n",
    "        loss_all.append(loss.item())\n",
    "        #####################################################\n",
    "        #                 End of your code                  #\n",
    "        #####################################################\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return np.mean(loss_all)\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    with torch.no_grad():\n",
    "        for step, inputs in enumerate(val_dataloader):\n",
    "            if cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits #\n",
    "            # 2. Get labels                                     #\n",
    "            # 3. Extend labels to list                          #\n",
    "            # 4. Get predictions and extend preds to list       #\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits\n",
    "            logits = model(batch=inputs)\n",
    "\n",
    "            # 2. Get labels\n",
    "            labels = inputs['label']\n",
    "\n",
    "            # 3. Extend labels to list\n",
    "            alllabels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # 4. Get predictions and extend preds to list\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            allpreds.extend(preds.cpu().numpy())\n",
    "            #####################################################\n",
    "            #                 End of your code                  #\n",
    "            #####################################################\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generated template from TemplateGenerator and find the best template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:13:36.594464200Z",
     "start_time": "2023-12-10T03:36:46.673523500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 1454.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:37<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"placeholder\": \"text_a\"} It was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} it was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001B[A\n",
      "tokenizing: 32it [00:00, 202.53it/s]A\n",
      "\n",
      "tokenizing: 32it [00:00, 727.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.562330920593297, Eval score=0.5\n",
      "Epoch 2: Train loss=1.029085214715451, Eval score=0.5\n",
      "Epoch 3: Train loss=0.671642615867313, Eval score=0.71875\n",
      "Epoch 4: Train loss=0.2566610455396585, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [15:59<1:03:57, 959.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.2469192902863142, Eval score=0.78125\n",
      "Current template: {\"placeholder\": \"text_a\"} it was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' it was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 379.37it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.511358927668084, Eval score=0.5\n",
      "Epoch 2: Train loss=0.7635227439459413, Eval score=0.5\n",
      "Epoch 3: Train loss=0.3154368309772053, Eval score=0.53125\n",
      "Epoch 4: Train loss=0.8977778584977472, Eval score=0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [36:43<56:21, 1127.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.7606429383413342, Eval score=0.5\n",
      "Current template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 571.42it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1180.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.765889931773188, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.5693292848736746, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.041312409681268036, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.2322409824218994, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [56:36<38:33, 1156.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.22881872234574985, Eval score=0.84375\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 1066.65it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1523.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.7746381501195, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9883591458201408, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8319057337939739, Eval score=0.625\n",
      "Epoch 4: Train loss=0.508084288907412, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [1:16:21<19:27, 1167.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.39746711112820776, Eval score=0.53125\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 864.68it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1454.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.6042319842349002, Eval score=0.5\n",
      "Epoch 2: Train loss=1.069442194304429, Eval score=0.5\n",
      "Epoch 3: Train loss=0.47684725234284997, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.2768472472046142, Eval score=0.59375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:36:08<00:00, 1153.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.07557142606344769, Eval score=0.625\n",
      "Final best template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class ManualTemplateWithoutParse(ManualTemplate):\n",
    "    \"\"\"The generated template from TemplateGenerator is a list of dict of parsed template_text. So no further parsing is needed.\"\"\"\n",
    "    def on_text_set(self):\n",
    "        pass\n",
    "\n",
    "# Template generation\n",
    "if auto_t:\n",
    "    print('performing auto_t...')\n",
    "\n",
    "    if cuda:\n",
    "        template_generate_model = template_generate_model.cuda()\n",
    "    template_generator = T5TemplateGenerator(template_generate_model, template_generate_tokenizer, template_tokenizer_wrapper, verbalizer, beam_width=5) # Beam_width is set to 5 here for efficiency; to improve performance, try a larger number.\n",
    "\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=template_generate_tokenizer, tokenizer_wrapper_class=template_tokenizer_wrapper, batch_size=len(dataset['train']), decoder_max_length=128, max_seq_length=128, shuffle=False, teacher_forcing=False) # Register all data at once\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        template_generator._register_buffer(data)\n",
    "\n",
    "    template_generate_model.eval()\n",
    "    print('generating...')\n",
    "    template_texts = template_generator._get_templates()\n",
    "\n",
    "    original_template = template.text\n",
    "    template_texts = [template_generator.convert_template(template_text, original_template) for template_text in template_texts]\n",
    "    # template_generator._show_template()\n",
    "    template_generator.release_memory()\n",
    "    # Generate a number of candidate template text\n",
    "    print(template_texts)\n",
    "    \n",
    "    # Iterate over each candidate and select the best one\n",
    "    best_metrics = 0.0\n",
    "    best_template_text = None\n",
    "    for template_text in tqdm(template_texts):\n",
    "        verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "        template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "        print(f\"Current template: {template_text}\\n\\nWrapped example: {template.wrap_one_example(dataset['train'][0])}\")\n",
    "\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "        \n",
    "        #######################################################\n",
    "        # TODO: Use score to Find your best template_text     #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_template_text = template_text\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # Use the best template\n",
    "    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "    template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=best_template_text)\n",
    "    print(\"Final best template:\", best_template_text)\n",
    "    print()\n",
    "    print(\"Wrapped example:\", template.wrap_one_example(dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verbalizer template from VerbalizerGenerator and find the best verbalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T11:58:04.838076700Z",
     "start_time": "2023-12-10T05:13:36.638466100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_v...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 1391.30it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current label_words: ['terrible', 'beautiful']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001B[A\n",
      "tokenizing: 3it [00:00, 21.48it/s]\u001B[A\n",
      "tokenizing: 6it [00:01,  5.15it/s]\u001B[A\n",
      "tokenizing: 8it [00:01,  6.27it/s]\u001B[A\n",
      "tokenizing: 32it [00:01, 23.45it/s]\u001B[A\n",
      "\n",
      "tokenizing: 32it [00:00, 969.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.1593105591260544, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.5407680265489034, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.17979280870349612, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.006995583800744498, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [21:05<6:40:40, 1265.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.002051841642924046, Eval score=0.84375\n",
      "current label_words: ['horrible', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 695.63it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1388.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.5843039118335565, Eval score=0.84375\n",
      "Epoch 2: Train loss=0.0798521326687478, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.0026221817748819376, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.0010607897513068565, Eval score=0.78125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [42:02<6:18:07, 1260.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0004625524882726495, Eval score=0.78125\n",
      "current label_words: ['horrible', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 762.07it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1333.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.0936600251085586, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.849827597849071, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.414547988853883, Eval score=0.75\n",
      "Epoch 4: Train loss=0.0500250239797424, Eval score=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [1:02:46<5:55:04, 1253.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.004471211226245941, Eval score=0.8125\n",
      "current label_words: ['sad', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 415.60it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 941.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4185917818103917, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.5065413688316767, Eval score=0.875\n",
      "Epoch 3: Train loss=0.012464412650388113, Eval score=0.875\n",
      "Epoch 4: Train loss=0.002708091426967485, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [1:23:35<5:33:44, 1251.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0004699288858773798, Eval score=0.875\n",
      "current label_words: ['bad', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 551.73it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=3.1230944280390602, Eval score=0.5\n",
      "Epoch 2: Train loss=1.270681380527094, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8551086119841784, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.6224154182709754, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [1:45:07<5:16:30, 1266.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.22766433987999335, Eval score=0.8125\n",
      "current label_words: ['horrible', 'remarkable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 426.62it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1031.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.2276666148868003, Eval score=0.6875\n",
      "Epoch 2: Train loss=0.4889321715090773, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2634941844644345, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.25736280560994373, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [2:07:36<5:01:58, 1294.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.3903749104914027, Eval score=0.625\n",
      "current label_words: ['terrible', 'brilliant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 493.33it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.128300400949797, Eval score=0.5\n",
      "Epoch 2: Train loss=1.0674331626432831, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.6011688623111695, Eval score=0.875\n",
      "Epoch 4: Train loss=0.6810656589186692, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [2:28:49<4:38:53, 1287.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.1570308672144165, Eval score=0.84375\n",
      "current label_words: ['terrible', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 451.65it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1033.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.1662085960851982, Eval score=0.84375\n",
      "Epoch 2: Train loss=0.046194135922632995, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.20158991879031873, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.009880203132524912, Eval score=0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [2:51:16<4:21:16, 1306.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.27703922392970526, Eval score=0.90625\n",
      "current label_words: ['disappointing', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 415.57it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1032.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.3897865504303581, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.6985758165101288, Eval score=0.5625\n",
      "Epoch 3: Train loss=0.20094251398768392, Eval score=0.875\n",
      "Epoch 4: Train loss=0.021359096241212683, Eval score=0.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [3:11:17<3:53:28, 1273.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0014102687238164435, Eval score=0.90625\n",
      "current label_words: ['strange', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 410.23it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4916955009493904, Eval score=0.5\n",
      "Epoch 2: Train loss=0.5601507005485473, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.050391235166898696, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.04447055474645367, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [3:31:10<3:28:06, 1248.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.5673459974156145, Eval score=0.78125\n",
      "current label_words: ['terrible', 'good']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 363.21it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.9351653824437118, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9128216816243366, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.16854792425147025, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.004711857527581742, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [3:50:34<3:03:23, 1222.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0015127657302400621, Eval score=0.84375\n",
      "current label_words: ['bad', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 524.59it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.8022562539517715, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9302898038731655, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.43625156552297994, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.04196147369020764, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [4:10:06<2:40:57, 1207.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.012102704274127518, Eval score=0.65625\n",
      "current label_words: ['short', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 412.44it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1203.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.8797737168388267, Eval score=0.75\n",
      "Epoch 2: Train loss=0.3757321042244257, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2311989847357836, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.43366863449273296, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [4:30:04<2:20:30, 1204.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.620734619528541, Eval score=0.59375\n",
      "current label_words: ['disappointing', 'delightful']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 426.69it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1143.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.324861448905718, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.5381804386270232, Eval score=0.5625\n",
      "Epoch 3: Train loss=0.36967586419450527, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.12384688404563349, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [4:49:18<1:58:55, 1189.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.18691727038913086, Eval score=0.6875\n",
      "current label_words: ['bad', 'brilliant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 490.61it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1103.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.5005056243027184, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.7745201710495166, Eval score=0.875\n",
      "Epoch 3: Train loss=0.13810731278863386, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.004411970109288177, Eval score=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [5:08:29<1:38:08, 1177.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.00313117326692236, Eval score=0.78125\n",
      "current label_words: ['bad', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 492.79it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.182302282977588, Eval score=0.5\n",
      "Epoch 2: Train loss=0.5332906207768247, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.08191546555644891, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.35411459776105403, Eval score=0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [5:26:58<1:17:08, 1157.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.13951104862388775, Eval score=0.46875\n",
      "current label_words: ['horrible', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 466.99it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1219.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4121702450024713, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.32750329683221935, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.01347528245059948, Eval score=0.90625\n",
      "Epoch 4: Train loss=0.3011642301885331, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [5:45:44<57:23, 1147.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.022153147599055956, Eval score=0.875\n",
      "current label_words: ['fine', 'remarkable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 492.08it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1185.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.7259275259780793, Eval score=0.5\n",
      "Epoch 2: Train loss=0.7986743192886934, Eval score=0.65625\n",
      "Epoch 3: Train loss=0.6269949703128077, Eval score=0.5\n",
      "Epoch 4: Train loss=0.16056611831118062, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [6:04:43<38:10, 1145.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.003870869544044808, Eval score=0.78125\n",
      "current label_words: ['disappointing', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 454.61it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1183.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4480454477061357, Eval score=0.5\n",
      "Epoch 2: Train loss=0.8791331336833537, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.4247932309117459, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.2258107879970339, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [6:23:21<18:56, 1137.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0734178872121447, Eval score=0.875\n",
      "current label_words: ['terrible', 'fine']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 489.28it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1164.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.6715138442009447, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.6137157797584223, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.38552796499482156, Eval score=0.875\n",
      "Epoch 4: Train loss=0.34600197029577373, Eval score=0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [6:41:58<00:00, 1205.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.6120385159649686, Eval score=0.78125\n",
      "final best label words: ['terrible', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verbalizer generation\n",
    "from openprompt.prompts.prompt_generator import RobertaVerbalizerGenerator\n",
    "if auto_v:\n",
    "    print('performing auto_v...')\n",
    "    # Load generation model for verbalizer generation\n",
    "    if cuda:\n",
    "        plm = plm.cuda()\n",
    "    verbalizer_generator = RobertaVerbalizerGenerator(model=plm, tokenizer=tokenizer, candidate_num=20, label_word_num_per_class=20)\n",
    "    # To improve performance, try larger numbers\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, batch_size=32)\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        verbalizer_generator.register_buffer(data)\n",
    "    label_words_list = verbalizer_generator.generate()\n",
    "    verbalizer_generator.release_memory()\n",
    "\n",
    "    # Iterate over each candidate and select the best one\n",
    "    current_verbalizer = copy.deepcopy(verbalizer)\n",
    "    best_metrics = 0.0\n",
    "    best_label_words = None\n",
    "    for label_words in tqdm(label_words_list):\n",
    "        print(f\"current label_words: {label_words}\")\n",
    "        current_verbalizer.label_words = label_words\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, current_verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "\n",
    "        #######################################################\n",
    "        # TODO: Use score to find your best_label_word        #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_label_words = label_words\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # use the best verbalizer\n",
    "    print(\"final best label words:\", best_label_words)\n",
    "    verbalizer = ManualVerbalizer(tokenizer, num_classes=2, label_words=best_label_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:18:43.541949400Z",
     "start_time": "2023-12-10T11:58:04.849042700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 383.01it/s]\n",
      "tokenizing: 32it [00:00, 1142.78it/s]\n",
      "tokenizing: 872it [00:00, 1095.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.2980121186483302, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.38737686289732665, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.08258779709103692, Eval score=0.875\n",
      "Epoch 4: Train loss=0.3668047572554656, Eval score=0.84375\n",
      "Epoch 5: Train loss=0.00465579945631589, Eval score=0.84375\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "test_dataloader = PromptDataLoader(dataset['test'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "\n",
    "model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# It's always good practice to set no decay to bias and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/5b8876ed26fd495b8353ad7ce94b6f65)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:46:57.884699300Z",
     "start_time": "2023-12-10T12:18:43.474013600Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "allpreds = []\n",
    "for step, inputs in tqdm(enumerate(test_dataloader)):\n",
    "    if cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = model(inputs)\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(allpreds):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report (15 points)\n",
    "\n",
    "- Task 1: Compare **two** different models you employed and provide a brief discussion of your implementation.\n",
    "\n",
    "- Task 2: You need to try at least **three** different templates and verbalizers to compare how your prompts work with the model. Report your performance in zero-shot, one-shot, and few-shot scenarios, with examples drawn from the training set.\n",
    "\n",
    "- Task 3: Try at least three different manually crafted templates to compare them with auto-generated templates. Evaluate the performance with different numbers of demonstrations and plot the graph from Figure 3 in the paper (https://arxiv.org/pdf/2012.15723.pdf). Also, report your best template and verbalizer. . \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
