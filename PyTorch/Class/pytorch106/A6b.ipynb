{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6Z1Snuk7rIK"
   },
   "source": [
    "# MIS 583 Assignment 6: Text Sentiment Classification with Prompt Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSwr9MgZogRZ"
   },
   "source": [
    "Before we start, please put your name and SID in following format: <br>\n",
    ": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷, M114020035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DzsjuDhlz_k"
   },
   "source": [
    "**Your Answer:**   \n",
    "Hi I'm 鄔仁迪, B104020009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d-Zzebq7rIM"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kc9gd_Wk7rIN"
   },
   "source": [
    "**Sentiment Classification** is an automated process of identifying opinions in text and labeling them as positive or negative based on the emotions customers express within them.\n",
    "\n",
    "In Task 1, you need to fine-tune a pre-trained language model (e.g., BERT) to predict the sentiment of given tweets.\n",
    "\n",
    "In Task 2, we employ prompts to enable the model to perform sentiment analysis through in-context learning, eliminating the need for additional training.\n",
    "\n",
    "In Task 3, you will use the method called LM-BFF to utilize the model in generating the optimal template and verbalizer autonomously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice \n",
    "**You are not allow to use the model like GPT family or pre-trained weight using SST-2 and twitter dataset!!!!!!!!!!!!!!!!!**\n",
    "\n",
    "You can use BERT and RoBERTa encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giUId1Naqacs",
    "tags": []
   },
   "source": [
    "##  Versions of used packages\n",
    "\n",
    "We will check PyTorch version to make sure everything work properly.  \n",
    "We use `python==3.7.14`, `torch==1.12.1+cu113` and `torchvision==0.13.1+cu113`.  \n",
    "This is the default version in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Vuw-gNvjqcYe",
    "ExecuteTime": {
     "end_time": "2023-12-15T17:00:38.812904500Z",
     "start_time": "2023-12-15T17:00:33.332154800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.9.7 (tags/v3.9.7:1016ef3, Aug 30 2021, 20:19:38) [MSC v.1929 64 bit (AMD64)]\n",
      "torch 2.1.1+cu118\n",
      "torchvision 0.16.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "print('python', sys.version.split('\\n')[0])\n",
    "print('torch', torch.__version__)\n",
    "print('torchvision', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Text Sentiment Classification (40 points)\n",
    "\n",
    "In this task, you need to fine-tune a pre-trained language model (e.g., BERT or RoBERTa encoder) to predict the sentiment of given tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a4s_a5D7rIR"
   },
   "source": [
    "## Loading Model and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPUkTbnL7rIR"
   },
   "source": [
    "First, let's talk about the model. The Hugging Face team has created an amazing framework called \"transformers\" for NLP tasks. It includes many state-of-the-art machine learning models for PyTorch, TensorFlow, and JAX.\n",
    "\n",
    "To start with this package, follow [this link to installation and a basic tutorial](https://pytorch.org/hub/huggingface_pytorch-transformers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rK0ouXa09pDU",
    "ExecuteTime": {
     "end_time": "2023-12-15T17:00:38.871898200Z",
     "start_time": "2023-12-15T17:00:38.814905100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'!echo happy installation\\n!pip -V\\n!pip install grpcio\\n!pip install google-auth\\n!pip install protobuf==3.9.2\\n!pip install pyprind\\n!pip install tqdm boto3 requests regex sentencepiece sacremoses'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you might need some additional installations there\n",
    "\"\"\"!echo happy installation\n",
    "!pip -V\n",
    "!pip install grpcio\n",
    "!pip install google-auth\n",
    "!pip install protobuf==3.9.2\n",
    "!pip install pyprind\n",
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dmGCAevi7rIS",
    "ExecuteTime": {
     "end_time": "2023-12-15T17:00:42.741564100Z",
     "start_time": "2023-12-15T17:00:38.830897200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "#########################################################################\n",
    "#            Loading tokenizer and model from transformer               #\n",
    "#########################################################################\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, AutoConfig\n",
    "\n",
    "bert_type = 'roberta-base'\n",
    "\n",
    "# ---------- 1. load from torch.hub ----------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# create a Bert-extended task (classification)\n",
    "model = RobertaForSequenceClassification.from_pretrained(bert_type)\n",
    "\n",
    "# finetune from the output from bert to your task\n",
    "# Replace the out_proj layer in the classifier with a new Linear layer for 3 classes\n",
    "model.classifier.out_proj = nn.Linear(in_features=768, out_features=3, bias=True)\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiMThsYeDa2O"
   },
   "source": [
    "## How to Get Data\n",
    "\n",
    "Please open the file `twitter_sentiment.zip`, creat shortcut to your Google Drive.\n",
    "\n",
    "1. open [LINK of Google Drive](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
    "3. Select the location where you want to place the shortcut.\n",
    "4. Click Add shortcut.\n",
    "\n",
    "After above procedures, we have a shortcut of zip file of dataset.  \n",
    "We can access this in colab after granting the permission of Google Drive.\n",
    "\n",
    "---\n",
    "\n",
    "請先到共用雲端硬碟將檔案 `twitter_sentiment.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lZnFgi5i_2oA",
    "ExecuteTime": {
     "end_time": "2023-12-15T17:00:42.765564900Z",
     "start_time": "2023-12-15T17:00:42.700565300Z"
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqO8DiB6VRQZ"
   },
   "source": [
    "## Unzip Data\n",
    "\n",
    "解壓縮 `twitter_sentiment.zip` 後可以發現裡面有三個csv檔。\n",
    "\n",
    "- `train.csv`, `test.csv` and `val.csv`\n",
    "\n",
    "Training set 有 **10248** 筆資料.  \n",
    "Validation set 有 **1317** 筆資料.  \n",
    "Testing set 有 **3075** 筆資料.  \n",
    "\n",
    "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OSlTMdxf8Zd7",
    "ExecuteTime": {
     "end_time": "2023-12-15T17:00:42.766565200Z",
     "start_time": "2023-12-15T17:00:42.718564800Z"
    }
   },
   "outputs": [],
   "source": [
    "# !unzip -qq ./drive/MyDrive/twitter_sentiment.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wf5GXTme7rIT",
    "ExecuteTime": {
     "end_time": "2023-12-15T17:00:42.767563900Z",
     "start_time": "2023-12-15T17:00:42.741564100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to extract text and label from csv file\n",
    "def get_texts(f_name='./twitter_sentiment', mode='train'):\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "\n",
    "    f_path = os.path.join(f_name, '{}.csv'.format(mode))\n",
    "    with open(f_path, encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for line in reader:\n",
    "            text_list.append(line['text'])\n",
    "            if mode != 'test':\n",
    "                label_list.append(int(line['sentiment_label']))\n",
    "\n",
    "    return text_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6fpY0ZrK7rIV",
    "ExecuteTime": {
     "end_time": "2023-12-15T17:00:42.768564500Z",
     "start_time": "2023-12-15T17:00:42.750565300Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, f_name='./twitter_sentiment', mode='train'):\n",
    "        self.mode = mode\n",
    "\n",
    "        text_list, label_list = get_texts(f_name, mode)\n",
    "        print('mode', mode, 'has', len(text_list), 'datas')\n",
    "        text_list = tokenizer(text_list,\n",
    "                             truncation=True, padding=True,\n",
    "                             return_tensors='pt')\n",
    "\n",
    "        self.text_list = text_list['input_ids']\n",
    "        self.mask_list = text_list['attention_mask']\n",
    "\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_list[idx]\n",
    "        mask = self.mask_list[idx]\n",
    "        if self.mode == 'test':\n",
    "            return text, mask\n",
    "        label = torch.tensor(self.label_list[idx])\n",
    "        return text, mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nCmM4FSw7rIW",
    "ExecuteTime": {
     "end_time": "2023-12-15T17:00:44.206108300Z",
     "start_time": "2023-12-15T17:00:42.761564300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode train has 10248 datas\n",
      "mode val has 1317 datas\n",
      "mode test has 3075 datas\n"
     ]
    }
   ],
   "source": [
    "dataset_train = TwitterDataset(mode='train')\n",
    "dataset_val = TwitterDataset(mode='val')\n",
    "dataset_test = TwitterDataset(mode='test')\n",
    "\n",
    "batch_size = 24\n",
    "train_data = DataLoader(dataset_train, batch_size=batch_size,\n",
    "                       shuffle=True)\n",
    "val_data = DataLoader(dataset_val, batch_size=batch_size // 2,\n",
    "                       shuffle=False)\n",
    "test_data = DataLoader(dataset_test, batch_size=batch_size // 2,\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bqkvofHc7rIY",
    "ExecuteTime": {
     "end_time": "2023-12-15T17:00:44.249663300Z",
     "start_time": "2023-12-15T17:00:44.210107700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['<s>', '@', 'united', 'ĠI', 'Ġhave', 'Ġnever', 'Ġbeen', 'Ġmislead', 'Ġby', 'Ġa', 'Ġcompany', 'Ġas', 'Ġmany', 'Ġtimes', 'Ġas', 'ĠI', 'Ġhave', 'Ġthis', 'Ġweek', 'Ġby', 'ĠUnited', 'ĠAirlines', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "token to s <s>@united I have never been mislead by a company as many times as I have this week by United Airlines!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(dataset_train[0][0])\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DxZrfCqW7rIY",
    "ExecuteTime": {
     "end_time": "2023-12-15T17:00:45.697642500Z",
     "start_time": "2023-12-15T17:00:44.226108100Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# 定義標籤平滑化的KL損失函數 Paper: https://arxiv.org/pdf/2312.06522.pdf\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = -1\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n",
    "# 設定標籤平滑化水平\n",
    "\"\"\"\n",
    "LS2: smoothing=0.03（即3%平滑化）\n",
    "LS3: smoothing=0.075（即7.5%平滑化）\n",
    "LS4: smoothing=0.15（即15%平滑化）\n",
    "LS5: smoothing=0.3（即30%平滑化）\n",
    "\"\"\"\n",
    "criterion = LabelSmoothingLoss(classes=3, smoothing=0.03)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpwgE2Gd7rIZ"
   },
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zlaiAZAD7rIa",
    "ExecuteTime": {
     "end_time": "2023-12-15T17:00:45.712695200Z",
     "start_time": "2023-12-15T17:00:45.699804900Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(raw_preds, y):\n",
    "    preds = raw_preds.argmax(dim=1)\n",
    "    acc = (preds == y).sum()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dmc_Gms97rIa",
    "ExecuteTime": {
     "end_time": "2023-12-15T17:00:46.665695Z",
     "start_time": "2023-12-15T17:00:45.712695200Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Testing process                              #\n",
    "        #########################################################################\n",
    "        # 1. Clean the gradients of optimizer\n",
    "        # 2. Put correct variables into model\n",
    "        # 3. Get prediction\n",
    "        # 4. Evalutate by criterion and accuracy\n",
    "\n",
    "        # Clean the gradients of optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(text, attention_mask=mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.logits, label)\n",
    "\n",
    "        # Compute accuracy using the provided function\n",
    "        acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "def test(model, data, criterion, log_loss=False):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Training process                             #\n",
    "        #########################################################################\n",
    "        # 1. Put correct variables into model\n",
    "        # 2. Get prediction\n",
    "        # 3. Evalutate by criterion and accuracy\n",
    "\n",
    "        # No gradient calculation for evaluation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(text, attention_mask=mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.logits, label)\n",
    "\n",
    "            # Compute accuracy\n",
    "            acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if log_loss:\n",
    "            val_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "# class for monitoring train and test acc/loss\n",
    "class Meter:\n",
    "    def __init__(self):\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def update(self, train_loss, train_acc, val_loss, val_acc):\n",
    "        self.train_loss_list.append(train_loss)\n",
    "        self.train_acc_list.append(train_acc)\n",
    "        self.val_loss_list.append(val_loss)\n",
    "        self.val_acc_list.append(val_acc)\n",
    "\n",
    "    def plot(self):\n",
    "        x = range(len(self.train_loss_list))\n",
    "        plt.plot(x, self.train_loss_list)\n",
    "        plt.plot(x, self.val_loss_list, color='r')\n",
    "        plt.legend(['train_loss', 'val_loss'])\n",
    "        plt.show()\n",
    "        plt.plot(x, self.train_acc_list)\n",
    "        plt.plot(x, self.val_acc_list, color='r')\n",
    "        plt.legend(['train_acc', 'val_acc'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExZyrKd57rIb"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bVDe-fRe7rIc",
    "ExecuteTime": {
     "end_time": "2023-12-15T17:15:26.812852600Z",
     "start_time": "2023-12-15T17:00:46.669695400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:42<00:00,  2.63it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 33.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 0.02388261373908747 train_acc: 0.7963505074160812\n",
      "Epoch 1 val_loss:  0.04029032417156521 val_acc : 0.8367501898253606\n",
      "---------- e 1 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:53<00:00,  2.46it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 33.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss: 0.01844993618569227 train_acc: 0.8672911787665886\n",
      "Epoch 2 val_loss:  0.038894431477436625 val_acc : 0.8625664388762339\n",
      "---------- e 2 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:55<00:00,  2.43it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 33.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss: 0.01592709450378351 train_acc: 0.8978337236533958\n",
      "Epoch 3 val_loss:  0.04148225830238519 val_acc : 0.8572513287775246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:53<00:00,  2.46it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 33.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss: 0.013915341097358425 train_acc: 0.922911787665886\n",
      "Epoch 4 val_loss:  0.0421566495455479 val_acc : 0.8549734244495064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:54<00:00,  2.44it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 33.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss: 0.011985595757541575 train_acc: 0.9444769711163153\n",
      "Epoch 5 val_loss:  0.042629429955960405 val_acc : 0.8572513287775246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#                          Hyper-parameters                             #\n",
    "#########################################################################\n",
    "max_epoch = 5\n",
    "log_interval = 1\n",
    "best_acc = 0\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################\n",
    "\n",
    "m = Meter()\n",
    "\n",
    "for epoch in range(1, max_epoch + 1):\n",
    "    train_loss, train_acc = train(model, train_data, optimizer, criterion)\n",
    "    val_loss, val_acc = test(model, val_data, criterion, log_loss=True)\n",
    "\n",
    "    if epoch % log_interval == 0:\n",
    "        print('Epoch {} train_loss: {} train_acc: {}'.format(\n",
    "            epoch, train_loss, train_acc\n",
    "        ))\n",
    "        print('Epoch {} val_loss:  {} val_acc : {}'.format(\n",
    "            epoch, val_loss, val_acc\n",
    "        ))\n",
    "\n",
    "    m.update(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "    # model checkpoint\n",
    "    if val_acc > best_acc:\n",
    "        best_model = model\n",
    "        best_acc = val_acc\n",
    "        print('-'*10, 'e', epoch, 'save best model', '-'*10)\n",
    "        torch.save(model.state_dict(), 'ckpts/e{}.pt'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SmtW58OR7rIc",
    "ExecuteTime": {
     "end_time": "2023-12-15T17:15:27.044848800Z",
     "start_time": "2023-12-15T17:15:26.818851400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot them out\n",
    "m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcJcHf7n7rId"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/f072e95f51bc48978225941dba218241).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = model\n",
    "best_model.load_state_dict(torch.load('ckpts/e2.pt'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T17:20:09.480889100Z",
     "start_time": "2023-12-15T17:20:08.292888900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Sf5UTlMZ7rId",
    "ExecuteTime": {
     "end_time": "2023-12-15T17:20:16.819584100Z",
     "start_time": "2023-12-15T17:20:10.468888100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257/257 [00:06<00:00, 40.61it/s]\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "\n",
    "total_out = []\n",
    "for text, mask in tqdm(test_data, total=len(test_data)):\n",
    "    text = text.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    output = best_model(text, mask)\n",
    "    pred = output.logits\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    total_out.append(pred)\n",
    "\n",
    "total_out = torch.cat(total_out).cpu().numpy().tolist()\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(total_out):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:41:35.870336300Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Save best model\n",
    "torch.save(best_model.state_dict(), 'ckpts/best_roberta_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: In-Context learning (32 points)\n",
    "\n",
    "In this task, you will learn how to perform sentiment classification using prompts without the need for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:41:35.882334700Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyprind\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertTokenizer, BertForMaskedLM\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.591798400Z",
     "start_time": "2023-12-11T06:59:54.213731500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#         TODO: Design your own template(prefix) and verbalizer         #\n",
    "#########################################################################\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.prefix = 'This sentence is [MASK].'  # you can modify this line\n",
    "        self.verbalizer = {\n",
    "            'disappointing': 0,\n",
    "            'neutral': 1,\n",
    "            'perfect': 2,\n",
    "        }\n",
    "\n",
    "        self.max_seq_length = 512\n",
    "        self.batch_size = 32\n",
    "\n",
    "\n",
    "config = Config()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bert_type = 'bert-base-uncased'\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(bert_type, num_labels=3)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(bert_type)\n",
    "\n",
    "bert = model.from_pretrained(bert_type, config=bert_config).to(device)\n",
    "\n",
    "#######################################################################\n",
    "#                        End of your code                             #\n",
    "#######################################################################\n",
    "\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.601784800Z",
     "start_time": "2023-12-11T07:00:00.592783400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to obtaion verbalizer ids\n",
    "def obtain_verbalizer_ids(verbalizer, tokenizer):\n",
    "    verbalizer_ids = tokenizer.convert_tokens_to_ids(list(verbalizer.keys()))\n",
    "    index2ids = {i: verbalizer_ids[i] for i in range(len(verbalizer_ids))}\n",
    "    return verbalizer_ids, index2ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.605878500Z",
     "start_time": "2023-12-11T07:00:00.597783400Z"
    }
   },
   "outputs": [],
   "source": [
    "verbalizer_ids, index2ids = obtain_verbalizer_ids(config.verbalizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.615507200Z",
     "start_time": "2023-12-11T07:00:00.607878800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to concatenate prefix and text\n",
    "def concatenate_prefix(texts, config):\n",
    "    ##################################################\n",
    "    #   TODO: concatenate your own prefix and text   #                               \n",
    "    ##################################################\n",
    "    prefix_texts = [config.prefix + \" \" + text for text in texts]\n",
    "    ##################################################\n",
    "    #                 End of your code               #                               \n",
    "    ##################################################\n",
    "    return prefix_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.694028Z",
     "start_time": "2023-12-11T07:00:00.617508700Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(config):\n",
    "    # ['texts', 'labels']\n",
    "    df = pd.read_csv('./twitter_sentiment/train.csv')\n",
    "    original_texts = df['text'].tolist()\n",
    "    labels = df['sentiment_label'].tolist()\n",
    "\n",
    "    texts = concatenate_prefix(original_texts, config)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "texts, labels = load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.706026300Z",
     "start_time": "2023-12-11T07:00:00.696029900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['[CLS]', 'it', 'was', '[MASK]', 'sentence', '.', '@', 'united', 'i', 'have', 'never', 'been', 'mis', '##lea', '##d', 'by', 'a', 'company', 'as', 'many', 'times', 'as', 'i', 'have', 'this', 'week', 'by', 'united', 'airlines', '!', '[SEP]']\n",
      "token to s [CLS] it was [MASK] sentence . @ united i have never been mislead by a company as many times as i have this week by united airlines ! [SEP]\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(tokenizer.encode(texts[0], add_special_tokens=True))\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.718528600Z",
     "start_time": "2023-12-11T07:00:00.712026600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Batching of texts and labels for training or processing in batches\n",
    "def pack_batch(texts, labels, batch_size):\n",
    "    \"\"\"\n",
    "    :param texts: list\n",
    "    :param labels: list\n",
    "    :param batch_size: int\n",
    "    :return batch_X: list\n",
    "            [[text11, text12, ...], [text21, text22, ...], ...]\n",
    "    :return batch_y: list\n",
    "            [[label11, label12, ...], [label21, label22, ...], ...]\n",
    "    :return batch_count: int\n",
    "    \"\"\"\n",
    "    assert len(texts) == len(labels)\n",
    "\n",
    "    if len(texts) % batch_size != 0:\n",
    "        flag = False\n",
    "        batch_count = int(len(texts) / batch_size) + 1\n",
    "    else:\n",
    "        flag = True\n",
    "        batch_count = int(len(texts) / batch_size)\n",
    "\n",
    "    batch_X, batch_y = [], []\n",
    "\n",
    "    if flag:\n",
    "        for i in range(batch_count):\n",
    "            batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "            batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "    else:\n",
    "        for i in range(batch_count):\n",
    "            if i == batch_count - 1:\n",
    "                batch_X.append(texts[i * batch_size:])\n",
    "                batch_y.append(labels[i * batch_size:])\n",
    "            else:\n",
    "                batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "                batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "\n",
    "    return batch_X, batch_y, batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.754528500Z",
     "start_time": "2023-12-11T07:00:00.720528100Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_X, batch_y, batch_count = pack_batch(texts, labels, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:04:58.906945500Z",
     "start_time": "2023-12-11T07:00:00.739529800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[100 %] Time elapsed: 00:04:58 | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.456967 | precision: 0.606231 | recall: 0.456967 | f1: 0.463890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:04:58\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    pper = pyprind.ProgPercent(batch_count)\n",
    "    for i in range(batch_count):\n",
    "        inputs = batch_X[i]\n",
    "        labels = batch_y[i]\n",
    "\n",
    "        tokens = tokenizer.batch_encode_plus(inputs, add_special_tokens=True,\n",
    "                                             max_length=config.max_seq_length,\n",
    "                                             padding='max_length', truncation=True)\n",
    "        \n",
    "        ids = torch.tensor(tokens['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(tokens['attention_mask']).to(device)\n",
    "\n",
    "        # Shape: (batch_size, max_seq_length, vocab_size)\n",
    "        logits = bert(ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        mask_token_index = (ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "\n",
    "        # Find [MASK] logits\n",
    "        # shape: (batch_size, vocab_size)\n",
    "        masked_logits = logits[mask_token_index[0], mask_token_index[1], :]\n",
    "\n",
    "        # Extract the logits of the word in the verbalizer at the [MASK] position\n",
    "        # shape: (batch_size, verbalizer_size)\n",
    "        verbalizer_logits = masked_logits[:, verbalizer_ids]\n",
    "\n",
    "        # Construct a pseudo-distribution from the logits in these verbalizers\n",
    "        pseudo_distribution = softmax(verbalizer_logits)\n",
    "\n",
    "        #################################################################################\n",
    "        #   1. Find the index with the maximum probability in the pseudo-distribution   #\n",
    "        #   2. Convert the index to the corresponding word ID                           #\n",
    "        #   3. Convert the ID to a token                                                #\n",
    "        #   4. Find the label corresponding to the token                                #\n",
    "        #################################################################################\n",
    "        # 1. Find the index with the maximum probability in the pseudo-distribution\n",
    "        pred_indices = torch.argmax(pseudo_distribution, dim=1)\n",
    "\n",
    "        # 2. Convert the index to the corresponding word ID\n",
    "        pred_ids = [verbalizer_ids[index] for index in pred_indices]\n",
    "\n",
    "        # 3. Convert the ID to a token\n",
    "        pred_tokens = tokenizer.convert_ids_to_tokens(pred_ids)\n",
    "\n",
    "        # 4. Find the label corresponding to the token\n",
    "        pred_labels = [config.verbalizer[token] if token in config.verbalizer else -1 for token in pred_tokens]\n",
    "        pred_labels = np.array(pred_labels)\n",
    "        #################################################################################\n",
    "        #                             End of your code                                  #\n",
    "        #################################################################################\n",
    "\n",
    "        predict_all = np.append(predict_all, pred_labels)\n",
    "        labels_all = np.append(labels_all, labels)\n",
    "\n",
    "        pper.update()\n",
    "\n",
    "    acc = accuracy_score(labels_all, predict_all)\n",
    "    p = precision_score(labels_all, predict_all, average=\"weighted\")\n",
    "    r = recall_score(labels_all, predict_all, average=\"weighted\")\n",
    "    f1 = f1_score(labels_all, predict_all, average=\"weighted\")\n",
    "\n",
    "    print('accuracy: %f | precision: %f | recall: %f | f1: %f' % (acc, p, r, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:04:59.175549700Z",
     "start_time": "2023-12-11T07:04:58.906945500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10243</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10244</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10245</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10246</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10247</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10248 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1\n",
       "0      0  2\n",
       "1      0  2\n",
       "2      2  2\n",
       "3      1  0\n",
       "4      0  2\n",
       "...   .. ..\n",
       "10243  0  2\n",
       "10244  0  1\n",
       "10245  0  0\n",
       "10246  2  2\n",
       "10247  1  1\n",
       "\n",
       "[10248 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([labels_all, predict_all]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: LM-BFF (45 points)\n",
    "\n",
    "https://arxiv.org/pdf/2012.15723.pdf\n",
    "\n",
    "Unlike the previous task, LM-BFF can generate templates and verbalizers automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請先到共用雲端硬碟將檔案 `SST-2.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/14MDYFasXU94dUE9DjgfcZE61iTRI2007/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install openprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T03:31:13.543193800Z",
     "start_time": "2023-12-09T03:30:21.808076200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openprompt\n",
      "  Downloading openprompt-1.0.1-py3-none-any.whl (146 kB)\n",
      "     ---------------------------------------- 0.0/146.4 kB ? eta -:--:--\n",
      "     ---------------- ---------------------- 61.4/146.4 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 146.4/146.4 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting transformers>=4.10.0 (from openprompt)\n",
      "  Obtaining dependency information for transformers>=4.10.0 from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 123.5/123.5 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting sentencepiece==0.1.96 (from openprompt)\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 0.1/1.1 MB 2.2 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.2/1.1 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.3/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.4/1.1 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.5/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 0.6/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.6/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 0.7/1.1 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 0.8/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 0.9/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.0/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.1/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.1/1.1 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting tqdm>=4.62.2 (from openprompt)\n",
      "  Obtaining dependency information for tqdm>=4.62.2 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting tensorboardX (from openprompt)\n",
      "  Obtaining dependency information for tensorboardX from https://files.pythonhosted.org/packages/44/71/f3e7c9b2ab67e28c572ab4e9d5fa3499e0d252650f96d8a3a03e26677f53/tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nltk (from openprompt)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting yacs (from openprompt)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting dill (from openprompt)\n",
      "  Obtaining dependency information for dill from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting datasets (from openprompt)\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting rouge==1.0.0 (from openprompt)\n",
      "  Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting pyarrow (from openprompt)\n",
      "  Obtaining dependency information for pyarrow from https://files.pythonhosted.org/packages/28/82/9adfafaf0de581a39a1c86002cafd1a55a1255c9e0d362dc3e970aab0656/pyarrow-14.0.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading pyarrow-14.0.1-cp39-cp39-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openprompt) (1.8.0)\n",
      "Requirement already satisfied: six in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rouge==1.0.0->openprompt) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\eddie\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.62.2->openprompt) (0.4.4)\n",
      "Collecting filelock (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/fc/85/0d1038f068900896a8590d6d0da198b90d31f731a39166a432aa2b92249b/regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (2.25.1)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/9f/90/a6821e7757d2db194c16cbca78c80e206f30f6cc62c7f15fb27428f8c6dd/tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/4e/96/f4ee4434d8b6452fe7d5d44df2e72d1c6b2add1c3a5fb5c81aae83cb90c6/safetensors-0.4.1-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.1-cp39-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->openprompt)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets->openprompt) (1.4.1)\n",
      "Collecting xxhash (from datasets->openprompt)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/33/db/e07aa80e39a7ee82e9ca6f5a0fa9bf0b4465934272116a1b578eaee7f4b3/xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->openprompt)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/c6/c9/820b5ab056f4ada76fbe05bd481a948f287957d6cbfd59e2dd2618b408c1/multiprocess-0.70.15-py39-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets->openprompt)\n",
      "  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets->openprompt) (3.7.4.post0)\n",
      "Requirement already satisfied: click in c:\\users\\eddie\\appdata\\roaming\\python\\python39\\site-packages (from nltk->openprompt) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->openprompt) (1.1.0)\n",
      "Collecting protobuf>=3.20 (from tensorboardX->openprompt)\n",
      "  Obtaining dependency information for protobuf>=3.20 from https://files.pythonhosted.org/packages/b6/4b/f4f3334784576822d7817a664b757030ebb35b981978baf9c2eb3c5f33a8/protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (21.2.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (3.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (5.2.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (1.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers>=4.10.0->openprompt) (3.0.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets->openprompt) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets->openprompt) (2021.3)\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 41.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/7.9 MB 3.2 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.2/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.4/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.4/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.6/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.7/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.8/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.9/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.0/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.1/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.2/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.3/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.4/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.5/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.6/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.7/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.8/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.9/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.2/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.3/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.4/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.5/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.6/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.7/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.8/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.9/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.2/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.3/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.4/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.6/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.7/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.9/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.0/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.1/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.2/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.3/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.5/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.6/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.9/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.0/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.1/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.2/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.3/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.5/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.6/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.7/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.9/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.0/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.1/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.3/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.4/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.5/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.6/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.7/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.8/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.9/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.0/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.3/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.4/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.5/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.6/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.7/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.8/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "   ---------------------------------------- 0.0/521.2 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 92.2/521.2 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 174.1/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 276.5/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 368.6/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 471.0/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 521.2/521.2 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 61.4/115.3 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 115.3/115.3 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow-14.0.1-cp39-cp39-win_amd64.whl (24.6 MB)\n",
      "   ---------------------------------------- 0.0/24.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/24.6 MB 3.2 MB/s eta 0:00:08\n",
      "   ---------------------------------------- 0.2/24.6 MB 2.6 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 0.3/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.4/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.5/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.6/24.6 MB 2.2 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.7/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.8/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.8/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.9/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.0/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.1/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.2/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.3/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.4/24.6 MB 1.9 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.4/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.7/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.8/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.9/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.0/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.1/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.2/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.3/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.4/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.5/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.6/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.7/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.8/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.9/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 3.0/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.2/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.3/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.4/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.5/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.6/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.7/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.8/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.9/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.0/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.2/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.3/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.4/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.5/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.6/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.8/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.9/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.0/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.1/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.2/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.3/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.4/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.5/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.6/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.8/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.9/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.0/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.1/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.2/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.3/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.4/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.5/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.6/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.7/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 6.8/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 6.9/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.0/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.1/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.2/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.3/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.5/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.6/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.7/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.8/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.9/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.0/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.1/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.2/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.3/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.5/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 8.6/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.7/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.8/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.3/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.4/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.5/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.6/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.7/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.8/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 9.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.0/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.3/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.4/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 10.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.6/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.7/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.8/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.9/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 11.0/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.1/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.2/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.4/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.6/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.7/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 11.8/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 11.9/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.0/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.1/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.2/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.7/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.8/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.0/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.1/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.3/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.7/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.8/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.1/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.3/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 14.8/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 14.9/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.0/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.1/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.2/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.3/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.4/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.5/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.8/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.9/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.0/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.1/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.2/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.3/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.4/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.5/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 16.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 16.8/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 16.9/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.0/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.1/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.3/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.4/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.5/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.6/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.7/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.8/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 17.9/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.0/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.1/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.3/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.4/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.5/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.6/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.7/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.7/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.8/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 19.0/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.1/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.2/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.3/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.4/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.5/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.7/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.8/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.0/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.1/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.2/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.3/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.4/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.5/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.7/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.9/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.0/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.1/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.2/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.3/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.4/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.5/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.9/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.0/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.1/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.2/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.3/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.4/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.5/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.7/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.8/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.2/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.3/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.4/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.5/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.6/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.7/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.8/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.2/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.5/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.6/24.6 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.7 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 92.2/101.7 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 101.7/101.7 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 0.0/311.7 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 112.6/311.7 kB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 194.6/311.7 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  307.2/311.7 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 311.7/311.7 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 112.6/413.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 194.6/413.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 307.2/413.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 368.6/413.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 413.4/413.4 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 92.2/269.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 174.1/269.6 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/269.6 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 269.6/269.6 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp39-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.8 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 112.6/277.8 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 194.6/277.8 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 277.8/277.8 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.2 MB 1.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 1.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.4/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.3 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 92.2/133.3 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 133.3/133.3 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl (29 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 71.7/166.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 166.4/166.4 kB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, yacs, xxhash, tqdm, safetensors, rouge, regex, pyarrow-hotfix, pyarrow, protobuf, fsspec, filelock, dill, tensorboardX, nltk, multiprocess, huggingface-hub, tokenizers, transformers, datasets, openprompt\n",
      "Successfully installed datasets-2.15.0 dill-0.3.7 filelock-3.13.1 fsspec-2023.10.0 huggingface-hub-0.19.4 multiprocess-0.70.15 nltk-3.8.1 openprompt-1.0.1 protobuf-4.25.1 pyarrow-14.0.1 pyarrow-hotfix-0.6 regex-2023.10.3 rouge-1.0.0 safetensors-0.4.1 sentencepiece-0.1.96 tensorboardX-2.6.2.2 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.35.2 xxhash-3.4.1 yacs-0.1.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install openprompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import openprompt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.181341Z",
     "start_time": "2023-12-10T03:35:52.654085900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.194342100Z",
     "start_time": "2023-12-10T03:36:10.182341900Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda = True\n",
    "auto_t = True # Whether to perform automatic template generation\n",
    "auto_v = True # Whether to perform automatic verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.226340200Z",
     "start_time": "2023-12-10T03:36:10.195343500Z"
    }
   },
   "outputs": [],
   "source": [
    "from openprompt.data_utils.text_classification_dataset import SST2Processor\n",
    "dataset = {'train': SST2Processor().get_train_examples(\"SST-2/\"),\n",
    "           'validation': SST2Processor().get_dev_examples(\"SST-2/\"),\n",
    "           'test': SST2Processor().get_test_examples(\"SST-2/\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.607530300Z",
     "start_time": "2023-12-10T03:36:10.228340900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: {\n",
      "  \"guid\": \"train-0\",\n",
      "  \"label\": 0,\n",
      "  \"meta\": {\n",
      "    \"labelword\": \"terrible\"\n",
      "  },\n",
      "  \"text_a\": \"nothing happens , and it happens to flat characters .\",\n",
      "  \"text_b\": \"\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print('load model...')\n",
    "from openprompt.plms import load_plm\n",
    "# load mlm model for main tasks\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")\n",
    "\n",
    "# load generation model for template generation\n",
    "template_generate_model, template_generate_tokenizer, template_generate_model_config, template_tokenizer_wrapper = load_plm('t5', 't5-large')\n",
    "\n",
    "from openprompt.prompts import ManualVerbalizer, ManualTemplate\n",
    "verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "#   TODO: You need to switch LMBFFTemplateGenerationTemplate or ManualTemplate to                                 #\n",
    "#         compare auto generate template and manual generate template                                             #\n",
    "###################################################################################################################\n",
    "from openprompt.prompts.prompt_generator import LMBFFTemplateGenerationTemplate\n",
    "########################################\n",
    "#   LMBFFTemplateGenerationTemplate    #\n",
    "########################################\n",
    "import random\n",
    "\n",
    "# number of demonstrations\n",
    "num_demonstrations = 1  # try different number\n",
    "\n",
    "demonstrations = []\n",
    "\n",
    "for _ in range(num_demonstrations):\n",
    "    # random choice training set example with label 0 \n",
    "    random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n",
    "\n",
    "    # random choice training set example with label 1\n",
    "    random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n",
    "    \n",
    "    demonstration = f'{random_example_1.text_a} It was terrible. {random_example_2.text_a} It was great.'\n",
    "    demonstrations.append(demonstration)\n",
    "\n",
    "# You can modify the demonstrations and try different combinations\n",
    "template_text = '{\"placeholder\": \"text_a\"} {\"mask\"} {\"meta\": \"labelword\"} {\"mask\"}.' + ' '.join(demonstrations)\n",
    "template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "\n",
    "#############################################\n",
    "#   End of LMBFFTemplateGenerationTemplate  #\n",
    "#############################################\n",
    "\n",
    "########################################\n",
    "#          ManualTemplate              #\n",
    "########################################\n",
    "\n",
    "#template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} It was {\"mask\"}.')\n",
    "\n",
    "#############################################\n",
    "#          End of ManualTemplate            # \n",
    "#############################################\n",
    "\n",
    "###################################################################################################################\n",
    "#                                           End of your code                                                      #\n",
    "###################################################################################################################\n",
    "\n",
    "\n",
    "# view wrapped example\n",
    "wrapped_example = template.wrap_one_example(dataset['train'][0])\n",
    "print(\"dataset:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.673523500Z",
     "start_time": "2023-12-10T03:36:46.625523700Z"
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "\n",
    "# Returns the best evaluation score achieved during training\n",
    "def fit(model, train_dataloader, val_dataloader, loss_func, optimizer, epochs=5):\n",
    "    best_score = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_dataloader, loss_func, optimizer)\n",
    "        score = evaluate(model, val_dataloader)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "        print(f\"Epoch {epoch+1}: Train loss={train_loss}, Eval score={score}\")\n",
    "    return best_score\n",
    "\n",
    "# Trains the model on the training data and computes the training loss\n",
    "def train_epoch(model, train_dataloader, loss_func, optimizer):\n",
    "    model.train()\n",
    "    loss_all = []\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits #\n",
    "        # 2. Get labels                                     #\n",
    "        # 3. Evalutate using loss_func                      #\n",
    "        # 4. Append loss to loss_all                        #\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits\n",
    "        logits = model(batch=inputs)\n",
    "\n",
    "        # 2. Get labels\n",
    "        labels = inputs['label']\n",
    "\n",
    "        # 3. Evalutate using loss_func\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Append loss to loss_all\n",
    "        loss_all.append(loss.item())\n",
    "        #####################################################\n",
    "        #                 End of your code                  #\n",
    "        #####################################################\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return np.mean(loss_all)\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    with torch.no_grad():\n",
    "        for step, inputs in enumerate(val_dataloader):\n",
    "            if cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits #\n",
    "            # 2. Get labels                                     #\n",
    "            # 3. Extend labels to list                          #\n",
    "            # 4. Get predictions and extend preds to list       #\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits\n",
    "            logits = model(batch=inputs)\n",
    "\n",
    "            # 2. Get labels\n",
    "            labels = inputs['label']\n",
    "\n",
    "            # 3. Extend labels to list\n",
    "            alllabels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # 4. Get predictions and extend preds to list\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            allpreds.extend(preds.cpu().numpy())\n",
    "            #####################################################\n",
    "            #                 End of your code                  #\n",
    "            #####################################################\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generated template from TemplateGenerator and find the best template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:13:36.594464200Z",
     "start_time": "2023-12-10T03:36:46.673523500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 1454.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:37<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"placeholder\": \"text_a\"} It was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} it was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001B[A\n",
      "tokenizing: 32it [00:00, 202.53it/s]A\n",
      "\n",
      "tokenizing: 32it [00:00, 727.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.562330920593297, Eval score=0.5\n",
      "Epoch 2: Train loss=1.029085214715451, Eval score=0.5\n",
      "Epoch 3: Train loss=0.671642615867313, Eval score=0.71875\n",
      "Epoch 4: Train loss=0.2566610455396585, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [15:59<1:03:57, 959.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.2469192902863142, Eval score=0.78125\n",
      "Current template: {\"placeholder\": \"text_a\"} it was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' it was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 379.37it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.511358927668084, Eval score=0.5\n",
      "Epoch 2: Train loss=0.7635227439459413, Eval score=0.5\n",
      "Epoch 3: Train loss=0.3154368309772053, Eval score=0.53125\n",
      "Epoch 4: Train loss=0.8977778584977472, Eval score=0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [36:43<56:21, 1127.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.7606429383413342, Eval score=0.5\n",
      "Current template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 571.42it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1180.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.765889931773188, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.5693292848736746, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.041312409681268036, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.2322409824218994, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [56:36<38:33, 1156.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.22881872234574985, Eval score=0.84375\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 1066.65it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1523.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.7746381501195, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9883591458201408, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8319057337939739, Eval score=0.625\n",
      "Epoch 4: Train loss=0.508084288907412, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [1:16:21<19:27, 1167.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.39746711112820776, Eval score=0.53125\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 864.68it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1454.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.6042319842349002, Eval score=0.5\n",
      "Epoch 2: Train loss=1.069442194304429, Eval score=0.5\n",
      "Epoch 3: Train loss=0.47684725234284997, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.2768472472046142, Eval score=0.59375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:36:08<00:00, 1153.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.07557142606344769, Eval score=0.625\n",
      "Final best template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class ManualTemplateWithoutParse(ManualTemplate):\n",
    "    \"\"\"The generated template from TemplateGenerator is a list of dict of parsed template_text. So no further parsing is needed.\"\"\"\n",
    "    def on_text_set(self):\n",
    "        pass\n",
    "\n",
    "# Template generation\n",
    "if auto_t:\n",
    "    print('performing auto_t...')\n",
    "\n",
    "    if cuda:\n",
    "        template_generate_model = template_generate_model.cuda()\n",
    "    template_generator = T5TemplateGenerator(template_generate_model, template_generate_tokenizer, template_tokenizer_wrapper, verbalizer, beam_width=5) # Beam_width is set to 5 here for efficiency; to improve performance, try a larger number.\n",
    "\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=template_generate_tokenizer, tokenizer_wrapper_class=template_tokenizer_wrapper, batch_size=len(dataset['train']), decoder_max_length=128, max_seq_length=128, shuffle=False, teacher_forcing=False) # Register all data at once\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        template_generator._register_buffer(data)\n",
    "\n",
    "    template_generate_model.eval()\n",
    "    print('generating...')\n",
    "    template_texts = template_generator._get_templates()\n",
    "\n",
    "    original_template = template.text\n",
    "    template_texts = [template_generator.convert_template(template_text, original_template) for template_text in template_texts]\n",
    "    # template_generator._show_template()\n",
    "    template_generator.release_memory()\n",
    "    # Generate a number of candidate template text\n",
    "    print(template_texts)\n",
    "    \n",
    "    # Iterate over each candidate and select the best one\n",
    "    best_metrics = 0.0\n",
    "    best_template_text = None\n",
    "    for template_text in tqdm(template_texts):\n",
    "        verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "        template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "        print(f\"Current template: {template_text}\\n\\nWrapped example: {template.wrap_one_example(dataset['train'][0])}\")\n",
    "\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "        \n",
    "        #######################################################\n",
    "        # TODO: Use score to Find your best template_text     #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_template_text = template_text\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # Use the best template\n",
    "    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "    template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=best_template_text)\n",
    "    print(\"Final best template:\", best_template_text)\n",
    "    print()\n",
    "    print(\"Wrapped example:\", template.wrap_one_example(dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verbalizer template from VerbalizerGenerator and find the best verbalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T11:58:04.838076700Z",
     "start_time": "2023-12-10T05:13:36.638466100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_v...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 1391.30it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current label_words: ['terrible', 'beautiful']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001B[A\n",
      "tokenizing: 3it [00:00, 21.48it/s]\u001B[A\n",
      "tokenizing: 6it [00:01,  5.15it/s]\u001B[A\n",
      "tokenizing: 8it [00:01,  6.27it/s]\u001B[A\n",
      "tokenizing: 32it [00:01, 23.45it/s]\u001B[A\n",
      "\n",
      "tokenizing: 32it [00:00, 969.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.1593105591260544, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.5407680265489034, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.17979280870349612, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.006995583800744498, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [21:05<6:40:40, 1265.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.002051841642924046, Eval score=0.84375\n",
      "current label_words: ['horrible', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 695.63it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1388.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.5843039118335565, Eval score=0.84375\n",
      "Epoch 2: Train loss=0.0798521326687478, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.0026221817748819376, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.0010607897513068565, Eval score=0.78125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [42:02<6:18:07, 1260.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0004625524882726495, Eval score=0.78125\n",
      "current label_words: ['horrible', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 762.07it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1333.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.0936600251085586, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.849827597849071, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.414547988853883, Eval score=0.75\n",
      "Epoch 4: Train loss=0.0500250239797424, Eval score=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [1:02:46<5:55:04, 1253.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.004471211226245941, Eval score=0.8125\n",
      "current label_words: ['sad', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 415.60it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 941.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4185917818103917, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.5065413688316767, Eval score=0.875\n",
      "Epoch 3: Train loss=0.012464412650388113, Eval score=0.875\n",
      "Epoch 4: Train loss=0.002708091426967485, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [1:23:35<5:33:44, 1251.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0004699288858773798, Eval score=0.875\n",
      "current label_words: ['bad', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 551.73it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=3.1230944280390602, Eval score=0.5\n",
      "Epoch 2: Train loss=1.270681380527094, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8551086119841784, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.6224154182709754, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [1:45:07<5:16:30, 1266.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.22766433987999335, Eval score=0.8125\n",
      "current label_words: ['horrible', 'remarkable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 426.62it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1031.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.2276666148868003, Eval score=0.6875\n",
      "Epoch 2: Train loss=0.4889321715090773, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2634941844644345, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.25736280560994373, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [2:07:36<5:01:58, 1294.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.3903749104914027, Eval score=0.625\n",
      "current label_words: ['terrible', 'brilliant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 493.33it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.128300400949797, Eval score=0.5\n",
      "Epoch 2: Train loss=1.0674331626432831, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.6011688623111695, Eval score=0.875\n",
      "Epoch 4: Train loss=0.6810656589186692, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [2:28:49<4:38:53, 1287.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.1570308672144165, Eval score=0.84375\n",
      "current label_words: ['terrible', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 451.65it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1033.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.1662085960851982, Eval score=0.84375\n",
      "Epoch 2: Train loss=0.046194135922632995, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.20158991879031873, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.009880203132524912, Eval score=0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [2:51:16<4:21:16, 1306.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.27703922392970526, Eval score=0.90625\n",
      "current label_words: ['disappointing', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 415.57it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1032.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.3897865504303581, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.6985758165101288, Eval score=0.5625\n",
      "Epoch 3: Train loss=0.20094251398768392, Eval score=0.875\n",
      "Epoch 4: Train loss=0.021359096241212683, Eval score=0.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [3:11:17<3:53:28, 1273.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0014102687238164435, Eval score=0.90625\n",
      "current label_words: ['strange', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 410.23it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4916955009493904, Eval score=0.5\n",
      "Epoch 2: Train loss=0.5601507005485473, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.050391235166898696, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.04447055474645367, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [3:31:10<3:28:06, 1248.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.5673459974156145, Eval score=0.78125\n",
      "current label_words: ['terrible', 'good']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 363.21it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.9351653824437118, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9128216816243366, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.16854792425147025, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.004711857527581742, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [3:50:34<3:03:23, 1222.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0015127657302400621, Eval score=0.84375\n",
      "current label_words: ['bad', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 524.59it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.8022562539517715, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9302898038731655, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.43625156552297994, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.04196147369020764, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [4:10:06<2:40:57, 1207.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.012102704274127518, Eval score=0.65625\n",
      "current label_words: ['short', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 412.44it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1203.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.8797737168388267, Eval score=0.75\n",
      "Epoch 2: Train loss=0.3757321042244257, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2311989847357836, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.43366863449273296, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [4:30:04<2:20:30, 1204.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.620734619528541, Eval score=0.59375\n",
      "current label_words: ['disappointing', 'delightful']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 426.69it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1143.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.324861448905718, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.5381804386270232, Eval score=0.5625\n",
      "Epoch 3: Train loss=0.36967586419450527, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.12384688404563349, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [4:49:18<1:58:55, 1189.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.18691727038913086, Eval score=0.6875\n",
      "current label_words: ['bad', 'brilliant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 490.61it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1103.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.5005056243027184, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.7745201710495166, Eval score=0.875\n",
      "Epoch 3: Train loss=0.13810731278863386, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.004411970109288177, Eval score=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [5:08:29<1:38:08, 1177.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.00313117326692236, Eval score=0.78125\n",
      "current label_words: ['bad', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 492.79it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.182302282977588, Eval score=0.5\n",
      "Epoch 2: Train loss=0.5332906207768247, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.08191546555644891, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.35411459776105403, Eval score=0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [5:26:58<1:17:08, 1157.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.13951104862388775, Eval score=0.46875\n",
      "current label_words: ['horrible', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 466.99it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1219.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4121702450024713, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.32750329683221935, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.01347528245059948, Eval score=0.90625\n",
      "Epoch 4: Train loss=0.3011642301885331, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [5:45:44<57:23, 1147.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.022153147599055956, Eval score=0.875\n",
      "current label_words: ['fine', 'remarkable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 492.08it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1185.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.7259275259780793, Eval score=0.5\n",
      "Epoch 2: Train loss=0.7986743192886934, Eval score=0.65625\n",
      "Epoch 3: Train loss=0.6269949703128077, Eval score=0.5\n",
      "Epoch 4: Train loss=0.16056611831118062, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [6:04:43<38:10, 1145.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.003870869544044808, Eval score=0.78125\n",
      "current label_words: ['disappointing', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 454.61it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1183.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4480454477061357, Eval score=0.5\n",
      "Epoch 2: Train loss=0.8791331336833537, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.4247932309117459, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.2258107879970339, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [6:23:21<18:56, 1137.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0734178872121447, Eval score=0.875\n",
      "current label_words: ['terrible', 'fine']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 489.28it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1164.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.6715138442009447, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.6137157797584223, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.38552796499482156, Eval score=0.875\n",
      "Epoch 4: Train loss=0.34600197029577373, Eval score=0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [6:41:58<00:00, 1205.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.6120385159649686, Eval score=0.78125\n",
      "final best label words: ['terrible', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verbalizer generation\n",
    "from openprompt.prompts.prompt_generator import RobertaVerbalizerGenerator\n",
    "if auto_v:\n",
    "    print('performing auto_v...')\n",
    "    # Load generation model for verbalizer generation\n",
    "    if cuda:\n",
    "        plm = plm.cuda()\n",
    "    verbalizer_generator = RobertaVerbalizerGenerator(model=plm, tokenizer=tokenizer, candidate_num=20, label_word_num_per_class=20)\n",
    "    # To improve performance, try larger numbers\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, batch_size=32)\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        verbalizer_generator.register_buffer(data)\n",
    "    label_words_list = verbalizer_generator.generate()\n",
    "    verbalizer_generator.release_memory()\n",
    "\n",
    "    # Iterate over each candidate and select the best one\n",
    "    current_verbalizer = copy.deepcopy(verbalizer)\n",
    "    best_metrics = 0.0\n",
    "    best_label_words = None\n",
    "    for label_words in tqdm(label_words_list):\n",
    "        print(f\"current label_words: {label_words}\")\n",
    "        current_verbalizer.label_words = label_words\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, current_verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "\n",
    "        #######################################################\n",
    "        # TODO: Use score to find your best_label_word        #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_label_words = label_words\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # use the best verbalizer\n",
    "    print(\"final best label words:\", best_label_words)\n",
    "    verbalizer = ManualVerbalizer(tokenizer, num_classes=2, label_words=best_label_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:18:43.541949400Z",
     "start_time": "2023-12-10T11:58:04.849042700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 383.01it/s]\n",
      "tokenizing: 32it [00:00, 1142.78it/s]\n",
      "tokenizing: 872it [00:00, 1095.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.2980121186483302, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.38737686289732665, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.08258779709103692, Eval score=0.875\n",
      "Epoch 4: Train loss=0.3668047572554656, Eval score=0.84375\n",
      "Epoch 5: Train loss=0.00465579945631589, Eval score=0.84375\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "test_dataloader = PromptDataLoader(dataset['test'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "\n",
    "model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# It's always good practice to set no decay to bias and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/5b8876ed26fd495b8353ad7ce94b6f65)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:46:57.884699300Z",
     "start_time": "2023-12-10T12:18:43.474013600Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "allpreds = []\n",
    "for step, inputs in tqdm(enumerate(test_dataloader)):\n",
    "    if cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = model(inputs)\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(allpreds):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report (15 points)\n",
    "\n",
    "- Task 1: Compare **two** different models you employed and provide a brief discussion of your implementation.\n",
    "\n",
    "- Task 2: You need to try at least **three** different templates and verbalizers to compare how your prompts work with the model. Report your performance in zero-shot, one-shot, and few-shot scenarios, with examples drawn from the training set.\n",
    "\n",
    "- Task 3: Try at least three different manually crafted templates to compare them with auto-generated templates. Evaluate the performance with different numbers of demonstrations and plot the graph from Figure 3 in the paper (https://arxiv.org/pdf/2012.15723.pdf). Also, report your best template and verbalizer. . \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
