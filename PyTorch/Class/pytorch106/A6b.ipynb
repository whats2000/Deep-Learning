{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6Z1Snuk7rIK"
   },
   "source": [
    "# MIS 583 Assignment 6: Text Sentiment Classification with Prompt Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSwr9MgZogRZ"
   },
   "source": [
    "Before we start, please put your name and SID in following format: <br>\n",
    ": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷, M114020035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DzsjuDhlz_k"
   },
   "source": [
    "**Your Answer:**   \n",
    "Hi I'm 鄔仁迪, B104020009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d-Zzebq7rIM"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kc9gd_Wk7rIN"
   },
   "source": [
    "**Sentiment Classification** is an automated process of identifying opinions in text and labeling them as positive or negative based on the emotions customers express within them.\n",
    "\n",
    "In Task 1, you need to fine-tune a pre-trained language model (e.g., BERT) to predict the sentiment of given tweets.\n",
    "\n",
    "In Task 2, we employ prompts to enable the model to perform sentiment analysis through in-context learning, eliminating the need for additional training.\n",
    "\n",
    "In Task 3, you will use the method called LM-BFF to utilize the model in generating the optimal template and verbalizer autonomously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice \n",
    "**You are not allow to use the model like GPT family or pre-trained weight using SST-2 and twitter dataset!!!!!!!!!!!!!!!!!**\n",
    "\n",
    "You can use BERT and RoBERTa encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giUId1Naqacs",
    "tags": []
   },
   "source": [
    "##  Versions of used packages\n",
    "\n",
    "We will check PyTorch version to make sure everything work properly.  \n",
    "We use `python==3.7.14`, `torch==1.12.1+cu113` and `torchvision==0.13.1+cu113`.  \n",
    "This is the default version in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:34.469380700Z",
     "start_time": "2023-12-15T17:31:29.590207100Z"
    },
    "id": "Vuw-gNvjqcYe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]\n",
      "torch 2.1.0+cu118\n",
      "torchvision 0.16.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "print('python', sys.version.split('\\n')[0])\n",
    "print('torch', torch.__version__)\n",
    "print('torchvision', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Text Sentiment Classification (40 points)\n",
    "\n",
    "In this task, you need to fine-tune a pre-trained language model (e.g., BERT or RoBERTa encoder) to predict the sentiment of given tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a4s_a5D7rIR"
   },
   "source": [
    "## Loading Model and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPUkTbnL7rIR"
   },
   "source": [
    "First, let's talk about the model. The Hugging Face team has created an amazing framework called \"transformers\" for NLP tasks. It includes many state-of-the-art machine learning models for PyTorch, TensorFlow, and JAX.\n",
    "\n",
    "To start with this package, follow [this link to installation and a basic tutorial](https://pytorch.org/hub/huggingface_pytorch-transformers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:34.531387200Z",
     "start_time": "2023-12-15T17:31:34.473380300Z"
    },
    "id": "rK0ouXa09pDU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!echo happy installation\\n!pip -V\\n!pip install grpcio\\n!pip install google-auth\\n!pip install protobuf==3.9.2\\n!pip install pyprind\\n!pip install tqdm boto3 requests regex sentencepiece sacremoses'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you might need some additional installations there\n",
    "\"\"\"!echo happy installation\n",
    "!pip -V\n",
    "!pip install grpcio\n",
    "!pip install google-auth\n",
    "!pip install protobuf==3.9.2\n",
    "!pip install pyprind\n",
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:39.975451200Z",
     "start_time": "2023-12-15T17:31:34.487379500Z"
    },
    "id": "dmGCAevi7rIS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "#########################################################################\n",
    "#            Loading tokenizer and model from transformer               #\n",
    "#########################################################################\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, AutoConfig\n",
    "\n",
    "bert_type = 'roberta-base'\n",
    "\n",
    "# ---------- 1. load from torch.hub ----------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# create a Bert-extended task (classification)\n",
    "model = RobertaForSequenceClassification.from_pretrained(bert_type)\n",
    "\n",
    "# finetune from the output from bert to your task\n",
    "# Replace the out_proj layer in the classifier with a new Linear layer for 3 classes\n",
    "model.classifier.out_proj = nn.Linear(in_features=768, out_features=3, bias=True)\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiMThsYeDa2O"
   },
   "source": [
    "## How to Get Data\n",
    "\n",
    "Please open the file `twitter_sentiment.zip`, creat shortcut to your Google Drive.\n",
    "\n",
    "1. open [LINK of Google Drive](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
    "3. Select the location where you want to place the shortcut.\n",
    "4. Click Add shortcut.\n",
    "\n",
    "After above procedures, we have a shortcut of zip file of dataset.  \n",
    "We can access this in colab after granting the permission of Google Drive.\n",
    "\n",
    "---\n",
    "\n",
    "請先到共用雲端硬碟將檔案 `twitter_sentiment.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:40.000454300Z",
     "start_time": "2023-12-15T17:31:39.933450400Z"
    },
    "id": "lZnFgi5i_2oA"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqO8DiB6VRQZ"
   },
   "source": [
    "## Unzip Data\n",
    "\n",
    "解壓縮 `twitter_sentiment.zip` 後可以發現裡面有三個csv檔。\n",
    "\n",
    "- `train.csv`, `test.csv` and `val.csv`\n",
    "\n",
    "Training set 有 **10248** 筆資料.  \n",
    "Validation set 有 **1317** 筆資料.  \n",
    "Testing set 有 **3075** 筆資料.  \n",
    "\n",
    "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:40.001450500Z",
     "start_time": "2023-12-15T17:31:39.950449600Z"
    },
    "id": "OSlTMdxf8Zd7"
   },
   "outputs": [],
   "source": [
    "# !unzip -qq ./drive/MyDrive/twitter_sentiment.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:40.001450500Z",
     "start_time": "2023-12-15T17:31:39.967450100Z"
    },
    "id": "wf5GXTme7rIT"
   },
   "outputs": [],
   "source": [
    "# Utility function to extract text and label from csv file\n",
    "def get_texts(f_name='./twitter_sentiment', mode='train'):\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "\n",
    "    f_path = os.path.join(f_name, '{}.csv'.format(mode))\n",
    "    with open(f_path, encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for line in reader:\n",
    "            text_list.append(line['text'])\n",
    "            if mode != 'test':\n",
    "                label_list.append(int(line['sentiment_label']))\n",
    "\n",
    "    return text_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:40.002451700Z",
     "start_time": "2023-12-15T17:31:39.979449900Z"
    },
    "id": "6fpY0ZrK7rIV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, f_name='./twitter_sentiment', mode='train'):\n",
    "        self.mode = mode\n",
    "\n",
    "        text_list, label_list = get_texts(f_name, mode)\n",
    "        print('mode', mode, 'has', len(text_list), 'datas')\n",
    "        text_list = tokenizer(text_list,\n",
    "                             truncation=True, padding=True,\n",
    "                             return_tensors='pt')\n",
    "\n",
    "        self.text_list = text_list['input_ids']\n",
    "        self.mask_list = text_list['attention_mask']\n",
    "\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_list[idx]\n",
    "        mask = self.mask_list[idx]\n",
    "        if self.mode == 'test':\n",
    "            return text, mask\n",
    "        label = torch.tensor(self.label_list[idx])\n",
    "        return text, mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:41.499451500Z",
     "start_time": "2023-12-15T17:31:39.998449800Z"
    },
    "id": "nCmM4FSw7rIW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode train has 10248 datas\n",
      "mode val has 1317 datas\n",
      "mode test has 3075 datas\n"
     ]
    }
   ],
   "source": [
    "dataset_train = TwitterDataset(mode='train')\n",
    "dataset_val = TwitterDataset(mode='val')\n",
    "dataset_test = TwitterDataset(mode='test')\n",
    "\n",
    "batch_size = 24\n",
    "train_data = DataLoader(dataset_train, batch_size=batch_size,\n",
    "                       shuffle=True)\n",
    "val_data = DataLoader(dataset_val, batch_size=batch_size // 2,\n",
    "                       shuffle=False)\n",
    "test_data = DataLoader(dataset_test, batch_size=batch_size // 2,\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:41.560834700Z",
     "start_time": "2023-12-15T17:31:41.502449600Z"
    },
    "id": "bqkvofHc7rIY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['<s>', '@', 'united', 'ĠI', 'Ġhave', 'Ġnever', 'Ġbeen', 'Ġmislead', 'Ġby', 'Ġa', 'Ġcompany', 'Ġas', 'Ġmany', 'Ġtimes', 'Ġas', 'ĠI', 'Ġhave', 'Ġthis', 'Ġweek', 'Ġby', 'ĠUnited', 'ĠAirlines', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "token to s <s>@united I have never been mislead by a company as many times as I have this week by United Airlines!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(dataset_train[0][0])\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:43.074968900Z",
     "start_time": "2023-12-15T17:31:41.518449600Z"
    },
    "id": "DxZrfCqW7rIY"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# 定義標籤平滑化的KL損失函數 Paper: https://arxiv.org/pdf/2312.06522.pdf\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = -1\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n",
    "# 設定標籤平滑化水平\n",
    "\"\"\"\n",
    "LS2: smoothing=0.03（即3%平滑化）\n",
    "LS3: smoothing=0.075（即7.5%平滑化）\n",
    "LS4: smoothing=0.15（即15%平滑化）\n",
    "LS5: smoothing=0.3（即30%平滑化）\n",
    "\"\"\"\n",
    "criterion = LabelSmoothingLoss(classes=3, smoothing=0.15)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpwgE2Gd7rIZ"
   },
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:43.087969800Z",
     "start_time": "2023-12-15T17:31:43.068973400Z"
    },
    "id": "zlaiAZAD7rIa"
   },
   "outputs": [],
   "source": [
    "def accuracy(raw_preds, y):\n",
    "    preds = raw_preds.argmax(dim=1)\n",
    "    acc = (preds == y).sum()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:43.781088600Z",
     "start_time": "2023-12-15T17:31:43.086983500Z"
    },
    "id": "dmc_Gms97rIa"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Testing process                              #\n",
    "        #########################################################################\n",
    "        # 1. Clean the gradients of optimizer\n",
    "        # 2. Put correct variables into model\n",
    "        # 3. Get prediction\n",
    "        # 4. Evalutate by criterion and accuracy\n",
    "\n",
    "        # Clean the gradients of optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(text, attention_mask=mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.logits, label)\n",
    "\n",
    "        # Compute accuracy using the provided function\n",
    "        acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "def test(model, data, criterion, log_loss=False):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Training process                             #\n",
    "        #########################################################################\n",
    "        # 1. Put correct variables into model\n",
    "        # 2. Get prediction\n",
    "        # 3. Evalutate by criterion and accuracy\n",
    "\n",
    "        # No gradient calculation for evaluation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(text, attention_mask=mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.logits, label)\n",
    "\n",
    "            # Compute accuracy\n",
    "            acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if log_loss:\n",
    "            val_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "# class for monitoring train and test acc/loss\n",
    "class Meter:\n",
    "    def __init__(self):\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def update(self, train_loss, train_acc, val_loss, val_acc):\n",
    "        self.train_loss_list.append(train_loss)\n",
    "        self.train_acc_list.append(train_acc)\n",
    "        self.val_loss_list.append(val_loss)\n",
    "        self.val_acc_list.append(val_acc)\n",
    "\n",
    "    def plot(self):\n",
    "        x = range(len(self.train_loss_list))\n",
    "        plt.plot(x, self.train_loss_list)\n",
    "        plt.plot(x, self.val_loss_list, color='r')\n",
    "        plt.legend(['train_loss', 'val_loss'])\n",
    "        plt.show()\n",
    "        plt.plot(x, self.train_acc_list)\n",
    "        plt.plot(x, self.val_acc_list, color='r')\n",
    "        plt.legend(['train_acc', 'val_acc'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExZyrKd57rIb"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:46:38.549277200Z",
     "start_time": "2023-12-15T17:31:43.779147Z"
    },
    "id": "bVDe-fRe7rIc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:39<00:00,  2.68it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 32.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 0.03217569713616725 train_acc: 0.7971311475409836\n",
      "Epoch 1 val_loss:  0.05952208464853494 val_acc : 0.8488990129081245\n",
      "---------- e 1 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:40<00:00,  2.66it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 32.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss: 0.029001852900231842 train_acc: 0.8662177985948478\n",
      "Epoch 2 val_loss:  0.05993926742353852 val_acc : 0.8466211085801063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:40<00:00,  2.66it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 32.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss: 0.02770311037001435 train_acc: 0.8980288836846214\n",
      "Epoch 3 val_loss:  0.059782543911930275 val_acc : 0.8648443432042521\n",
      "---------- e 3 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:40<00:00,  2.66it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 33.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss: 0.026594569105407764 train_acc: 0.9164715066354411\n",
      "Epoch 4 val_loss:  0.06117344744384696 val_acc : 0.8549734244495064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:40<00:00,  2.66it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 32.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss: 0.02545781251534552 train_acc: 0.9399882903981265\n",
      "Epoch 5 val_loss:  0.062270790542176754 val_acc : 0.8481397114654518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#                          Hyper-parameters                             #\n",
    "#########################################################################\n",
    "max_epoch = 5\n",
    "log_interval = 1\n",
    "best_acc = 0\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################\n",
    "\n",
    "m = Meter()\n",
    "\n",
    "for epoch in range(1, max_epoch + 1):\n",
    "    train_loss, train_acc = train(model, train_data, optimizer, criterion)\n",
    "    val_loss, val_acc = test(model, val_data, criterion, log_loss=True)\n",
    "\n",
    "    if epoch % log_interval == 0:\n",
    "        print('Epoch {} train_loss: {} train_acc: {}'.format(\n",
    "            epoch, train_loss, train_acc\n",
    "        ))\n",
    "        print('Epoch {} val_loss:  {} val_acc : {}'.format(\n",
    "            epoch, val_loss, val_acc\n",
    "        ))\n",
    "\n",
    "    m.update(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "    # model checkpoint\n",
    "    if val_acc > best_acc:\n",
    "        best_model = model\n",
    "        best_acc = val_acc\n",
    "        print('-'*10, 'e', epoch, 'save best model', '-'*10)\n",
    "        torch.save(model.state_dict(), 'ckpts/e{}.pt'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:46:38.751579Z",
     "start_time": "2023-12-15T17:46:38.556479400Z"
    },
    "id": "SmtW58OR7rIc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot them out\n",
    "m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcJcHf7n7rId"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/f072e95f51bc48978225941dba218241).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:20:09.480889100Z",
     "start_time": "2023-12-15T17:20:08.292888900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = model\n",
    "best_model.load_state_dict(torch.load('ckpts/e3.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:20:16.819584100Z",
     "start_time": "2023-12-15T17:20:10.468888100Z"
    },
    "id": "Sf5UTlMZ7rId"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257/257 [00:06<00:00, 41.06it/s]\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "\n",
    "total_out = []\n",
    "for text, mask in tqdm(test_data, total=len(test_data)):\n",
    "    text = text.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    output = best_model(text, mask)\n",
    "    pred = output.logits\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    total_out.append(pred)\n",
    "\n",
    "total_out = torch.cat(total_out).cpu().numpy().tolist()\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(total_out):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:41:35.870336300Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Save best model\n",
    "torch.save(best_model.state_dict(), 'ckpts/best_roberta_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: In-Context learning (32 points)\n",
    "\n",
    "In this task, you will learn how to perform sentiment classification using prompts without the need for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:41:35.882334700Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyprind\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertTokenizer, BertForMaskedLM\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.591798400Z",
     "start_time": "2023-12-11T06:59:54.213731500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#         TODO: Design your own template(prefix) and verbalizer         #\n",
    "#########################################################################\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.prefix = 'This sentence is [MASK].'  # you can modify this line\n",
    "        self.verbalizer = {\n",
    "            'disappointing': 0,\n",
    "            'neutral': 1,\n",
    "            'perfect': 2,\n",
    "        }\n",
    "\n",
    "        self.max_seq_length = 512\n",
    "        self.batch_size = 32\n",
    "\n",
    "\n",
    "config = Config()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bert_type = 'bert-base-uncased'\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(bert_type, num_labels=3)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(bert_type)\n",
    "\n",
    "bert = model.from_pretrained(bert_type, config=bert_config).to(device)\n",
    "\n",
    "#######################################################################\n",
    "#                        End of your code                             #\n",
    "#######################################################################\n",
    "\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.601784800Z",
     "start_time": "2023-12-11T07:00:00.592783400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to obtaion verbalizer ids\n",
    "def obtain_verbalizer_ids(verbalizer, tokenizer):\n",
    "    verbalizer_ids = tokenizer.convert_tokens_to_ids(list(verbalizer.keys()))\n",
    "    index2ids = {i: verbalizer_ids[i] for i in range(len(verbalizer_ids))}\n",
    "    return verbalizer_ids, index2ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.605878500Z",
     "start_time": "2023-12-11T07:00:00.597783400Z"
    }
   },
   "outputs": [],
   "source": [
    "verbalizer_ids, index2ids = obtain_verbalizer_ids(config.verbalizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.615507200Z",
     "start_time": "2023-12-11T07:00:00.607878800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to concatenate prefix and text\n",
    "def concatenate_prefix(texts, config):\n",
    "    ##################################################\n",
    "    #   TODO: concatenate your own prefix and text   #                               \n",
    "    ##################################################\n",
    "    prefix_texts = [config.prefix + \" \" + text for text in texts]\n",
    "    ##################################################\n",
    "    #                 End of your code               #                               \n",
    "    ##################################################\n",
    "    return prefix_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.694028Z",
     "start_time": "2023-12-11T07:00:00.617508700Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(config):\n",
    "    # ['texts', 'labels']\n",
    "    df = pd.read_csv('./twitter_sentiment/train.csv')\n",
    "    original_texts = df['text'].tolist()\n",
    "    labels = df['sentiment_label'].tolist()\n",
    "\n",
    "    texts = concatenate_prefix(original_texts, config)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "texts, labels = load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.706026300Z",
     "start_time": "2023-12-11T07:00:00.696029900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['[CLS]', 'it', 'was', '[MASK]', 'sentence', '.', '@', 'united', 'i', 'have', 'never', 'been', 'mis', '##lea', '##d', 'by', 'a', 'company', 'as', 'many', 'times', 'as', 'i', 'have', 'this', 'week', 'by', 'united', 'airlines', '!', '[SEP]']\n",
      "token to s [CLS] it was [MASK] sentence . @ united i have never been mislead by a company as many times as i have this week by united airlines ! [SEP]\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(tokenizer.encode(texts[0], add_special_tokens=True))\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.718528600Z",
     "start_time": "2023-12-11T07:00:00.712026600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Batching of texts and labels for training or processing in batches\n",
    "def pack_batch(texts, labels, batch_size):\n",
    "    \"\"\"\n",
    "    :param texts: list\n",
    "    :param labels: list\n",
    "    :param batch_size: int\n",
    "    :return batch_X: list\n",
    "            [[text11, text12, ...], [text21, text22, ...], ...]\n",
    "    :return batch_y: list\n",
    "            [[label11, label12, ...], [label21, label22, ...], ...]\n",
    "    :return batch_count: int\n",
    "    \"\"\"\n",
    "    assert len(texts) == len(labels)\n",
    "\n",
    "    if len(texts) % batch_size != 0:\n",
    "        flag = False\n",
    "        batch_count = int(len(texts) / batch_size) + 1\n",
    "    else:\n",
    "        flag = True\n",
    "        batch_count = int(len(texts) / batch_size)\n",
    "\n",
    "    batch_X, batch_y = [], []\n",
    "\n",
    "    if flag:\n",
    "        for i in range(batch_count):\n",
    "            batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "            batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "    else:\n",
    "        for i in range(batch_count):\n",
    "            if i == batch_count - 1:\n",
    "                batch_X.append(texts[i * batch_size:])\n",
    "                batch_y.append(labels[i * batch_size:])\n",
    "            else:\n",
    "                batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "                batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "\n",
    "    return batch_X, batch_y, batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.754528500Z",
     "start_time": "2023-12-11T07:00:00.720528100Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_X, batch_y, batch_count = pack_batch(texts, labels, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:04:58.906945500Z",
     "start_time": "2023-12-11T07:00:00.739529800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[100 %] Time elapsed: 00:04:58 | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.456967 | precision: 0.606231 | recall: 0.456967 | f1: 0.463890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:04:58\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    pper = pyprind.ProgPercent(batch_count)\n",
    "    for i in range(batch_count):\n",
    "        inputs = batch_X[i]\n",
    "        labels = batch_y[i]\n",
    "\n",
    "        tokens = tokenizer.batch_encode_plus(inputs, add_special_tokens=True,\n",
    "                                             max_length=config.max_seq_length,\n",
    "                                             padding='max_length', truncation=True)\n",
    "        \n",
    "        ids = torch.tensor(tokens['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(tokens['attention_mask']).to(device)\n",
    "\n",
    "        # Shape: (batch_size, max_seq_length, vocab_size)\n",
    "        logits = bert(ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        mask_token_index = (ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "\n",
    "        # Find [MASK] logits\n",
    "        # shape: (batch_size, vocab_size)\n",
    "        masked_logits = logits[mask_token_index[0], mask_token_index[1], :]\n",
    "\n",
    "        # Extract the logits of the word in the verbalizer at the [MASK] position\n",
    "        # shape: (batch_size, verbalizer_size)\n",
    "        verbalizer_logits = masked_logits[:, verbalizer_ids]\n",
    "\n",
    "        # Construct a pseudo-distribution from the logits in these verbalizers\n",
    "        pseudo_distribution = softmax(verbalizer_logits)\n",
    "\n",
    "        #################################################################################\n",
    "        #   1. Find the index with the maximum probability in the pseudo-distribution   #\n",
    "        #   2. Convert the index to the corresponding word ID                           #\n",
    "        #   3. Convert the ID to a token                                                #\n",
    "        #   4. Find the label corresponding to the token                                #\n",
    "        #################################################################################\n",
    "        # 1. Find the index with the maximum probability in the pseudo-distribution\n",
    "        pred_indices = torch.argmax(pseudo_distribution, dim=1)\n",
    "\n",
    "        # 2. Convert the index to the corresponding word ID\n",
    "        pred_ids = [verbalizer_ids[index] for index in pred_indices]\n",
    "\n",
    "        # 3. Convert the ID to a token\n",
    "        pred_tokens = tokenizer.convert_ids_to_tokens(pred_ids)\n",
    "\n",
    "        # 4. Find the label corresponding to the token\n",
    "        pred_labels = [config.verbalizer[token] if token in config.verbalizer else -1 for token in pred_tokens]\n",
    "        pred_labels = np.array(pred_labels)\n",
    "        #################################################################################\n",
    "        #                             End of your code                                  #\n",
    "        #################################################################################\n",
    "\n",
    "        predict_all = np.append(predict_all, pred_labels)\n",
    "        labels_all = np.append(labels_all, labels)\n",
    "\n",
    "        pper.update()\n",
    "\n",
    "    acc = accuracy_score(labels_all, predict_all)\n",
    "    p = precision_score(labels_all, predict_all, average=\"weighted\")\n",
    "    r = recall_score(labels_all, predict_all, average=\"weighted\")\n",
    "    f1 = f1_score(labels_all, predict_all, average=\"weighted\")\n",
    "\n",
    "    print('accuracy: %f | precision: %f | recall: %f | f1: %f' % (acc, p, r, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:04:59.175549700Z",
     "start_time": "2023-12-11T07:04:58.906945500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10243</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10244</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10245</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10246</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10247</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10248 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1\n",
       "0      0  2\n",
       "1      0  2\n",
       "2      2  2\n",
       "3      1  0\n",
       "4      0  2\n",
       "...   .. ..\n",
       "10243  0  2\n",
       "10244  0  1\n",
       "10245  0  0\n",
       "10246  2  2\n",
       "10247  1  1\n",
       "\n",
       "[10248 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([labels_all, predict_all]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: LM-BFF (45 points)\n",
    "\n",
    "https://arxiv.org/pdf/2012.15723.pdf\n",
    "\n",
    "Unlike the previous task, LM-BFF can generate templates and verbalizers automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請先到共用雲端硬碟將檔案 `SST-2.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/14MDYFasXU94dUE9DjgfcZE61iTRI2007/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install openprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T03:31:13.543193800Z",
     "start_time": "2023-12-09T03:30:21.808076200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openprompt\n",
      "  Downloading openprompt-1.0.1-py3-none-any.whl (146 kB)\n",
      "     ---------------------------------------- 0.0/146.4 kB ? eta -:--:--\n",
      "     ---------------- ---------------------- 61.4/146.4 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 146.4/146.4 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting transformers>=4.10.0 (from openprompt)\n",
      "  Obtaining dependency information for transformers>=4.10.0 from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 123.5/123.5 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting sentencepiece==0.1.96 (from openprompt)\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 0.1/1.1 MB 2.2 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.2/1.1 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.3/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.4/1.1 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.5/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 0.6/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.6/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 0.7/1.1 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 0.8/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 0.9/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.0/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.1/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.1/1.1 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting tqdm>=4.62.2 (from openprompt)\n",
      "  Obtaining dependency information for tqdm>=4.62.2 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting tensorboardX (from openprompt)\n",
      "  Obtaining dependency information for tensorboardX from https://files.pythonhosted.org/packages/44/71/f3e7c9b2ab67e28c572ab4e9d5fa3499e0d252650f96d8a3a03e26677f53/tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nltk (from openprompt)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting yacs (from openprompt)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting dill (from openprompt)\n",
      "  Obtaining dependency information for dill from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting datasets (from openprompt)\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting rouge==1.0.0 (from openprompt)\n",
      "  Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting pyarrow (from openprompt)\n",
      "  Obtaining dependency information for pyarrow from https://files.pythonhosted.org/packages/28/82/9adfafaf0de581a39a1c86002cafd1a55a1255c9e0d362dc3e970aab0656/pyarrow-14.0.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading pyarrow-14.0.1-cp39-cp39-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openprompt) (1.8.0)\n",
      "Requirement already satisfied: six in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rouge==1.0.0->openprompt) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\eddie\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.62.2->openprompt) (0.4.4)\n",
      "Collecting filelock (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/fc/85/0d1038f068900896a8590d6d0da198b90d31f731a39166a432aa2b92249b/regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (2.25.1)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/9f/90/a6821e7757d2db194c16cbca78c80e206f30f6cc62c7f15fb27428f8c6dd/tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/4e/96/f4ee4434d8b6452fe7d5d44df2e72d1c6b2add1c3a5fb5c81aae83cb90c6/safetensors-0.4.1-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.1-cp39-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->openprompt)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets->openprompt) (1.4.1)\n",
      "Collecting xxhash (from datasets->openprompt)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/33/db/e07aa80e39a7ee82e9ca6f5a0fa9bf0b4465934272116a1b578eaee7f4b3/xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->openprompt)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/c6/c9/820b5ab056f4ada76fbe05bd481a948f287957d6cbfd59e2dd2618b408c1/multiprocess-0.70.15-py39-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets->openprompt)\n",
      "  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets->openprompt) (3.7.4.post0)\n",
      "Requirement already satisfied: click in c:\\users\\eddie\\appdata\\roaming\\python\\python39\\site-packages (from nltk->openprompt) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->openprompt) (1.1.0)\n",
      "Collecting protobuf>=3.20 (from tensorboardX->openprompt)\n",
      "  Obtaining dependency information for protobuf>=3.20 from https://files.pythonhosted.org/packages/b6/4b/f4f3334784576822d7817a664b757030ebb35b981978baf9c2eb3c5f33a8/protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (21.2.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (3.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (5.2.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (1.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers>=4.10.0->openprompt) (3.0.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets->openprompt) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets->openprompt) (2021.3)\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 41.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/7.9 MB 3.2 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.2/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.4/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.4/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.6/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.7/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.8/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.9/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.0/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.1/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.2/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.3/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.4/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.5/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.6/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.7/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.8/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.9/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.2/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.3/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.4/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.5/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.6/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.7/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.8/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.9/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.2/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.3/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.4/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.6/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.7/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.9/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.0/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.1/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.2/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.3/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.5/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.6/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.9/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.0/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.1/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.2/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.3/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.5/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.6/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.7/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.9/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.0/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.1/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.3/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.4/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.5/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.6/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.7/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.8/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.9/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.0/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.3/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.4/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.5/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.6/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.7/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.8/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "   ---------------------------------------- 0.0/521.2 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 92.2/521.2 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 174.1/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 276.5/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 368.6/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 471.0/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 521.2/521.2 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 61.4/115.3 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 115.3/115.3 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow-14.0.1-cp39-cp39-win_amd64.whl (24.6 MB)\n",
      "   ---------------------------------------- 0.0/24.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/24.6 MB 3.2 MB/s eta 0:00:08\n",
      "   ---------------------------------------- 0.2/24.6 MB 2.6 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 0.3/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.4/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.5/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.6/24.6 MB 2.2 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.7/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.8/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.8/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.9/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.0/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.1/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.2/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.3/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.4/24.6 MB 1.9 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.4/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.7/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.8/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.9/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.0/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.1/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.2/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.3/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.4/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.5/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.6/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.7/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.8/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.9/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 3.0/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.2/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.3/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.4/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.5/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.6/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.7/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.8/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.9/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.0/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.2/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.3/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.4/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.5/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.6/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.8/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.9/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.0/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.1/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.2/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.3/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.4/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.5/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.6/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.8/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.9/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.0/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.1/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.2/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.3/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.4/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.5/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.6/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.7/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 6.8/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 6.9/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.0/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.1/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.2/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.3/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.5/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.6/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.7/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.8/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.9/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.0/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.1/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.2/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.3/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.5/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 8.6/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.7/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.8/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.3/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.4/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.5/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.6/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.7/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.8/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 9.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.0/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.3/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.4/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 10.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.6/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.7/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.8/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.9/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 11.0/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.1/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.2/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.4/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.6/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.7/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 11.8/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 11.9/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.0/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.1/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.2/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.7/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.8/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.0/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.1/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.3/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.7/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.8/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.1/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.3/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 14.8/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 14.9/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.0/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.1/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.2/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.3/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.4/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.5/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.8/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.9/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.0/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.1/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.2/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.3/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.4/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.5/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 16.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 16.8/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 16.9/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.0/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.1/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.3/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.4/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.5/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.6/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.7/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.8/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 17.9/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.0/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.1/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.3/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.4/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.5/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.6/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.7/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.7/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.8/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 19.0/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.1/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.2/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.3/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.4/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.5/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.7/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.8/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.0/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.1/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.2/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.3/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.4/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.5/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.7/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.9/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.0/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.1/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.2/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.3/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.4/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.5/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.9/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.0/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.1/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.2/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.3/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.4/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.5/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.7/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.8/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.2/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.3/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.4/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.5/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.6/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.7/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.8/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.2/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.5/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.6/24.6 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.7 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 92.2/101.7 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 101.7/101.7 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 0.0/311.7 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 112.6/311.7 kB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 194.6/311.7 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  307.2/311.7 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 311.7/311.7 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 112.6/413.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 194.6/413.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 307.2/413.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 368.6/413.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 413.4/413.4 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 92.2/269.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 174.1/269.6 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/269.6 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 269.6/269.6 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp39-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.8 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 112.6/277.8 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 194.6/277.8 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 277.8/277.8 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.2 MB 1.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 1.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.4/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.3 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 92.2/133.3 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 133.3/133.3 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl (29 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 71.7/166.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 166.4/166.4 kB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, yacs, xxhash, tqdm, safetensors, rouge, regex, pyarrow-hotfix, pyarrow, protobuf, fsspec, filelock, dill, tensorboardX, nltk, multiprocess, huggingface-hub, tokenizers, transformers, datasets, openprompt\n",
      "Successfully installed datasets-2.15.0 dill-0.3.7 filelock-3.13.1 fsspec-2023.10.0 huggingface-hub-0.19.4 multiprocess-0.70.15 nltk-3.8.1 openprompt-1.0.1 protobuf-4.25.1 pyarrow-14.0.1 pyarrow-hotfix-0.6 regex-2023.10.3 rouge-1.0.0 safetensors-0.4.1 sentencepiece-0.1.96 tensorboardX-2.6.2.2 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.35.2 xxhash-3.4.1 yacs-0.1.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install openprompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import openprompt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.181341Z",
     "start_time": "2023-12-10T03:35:52.654085900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.194342100Z",
     "start_time": "2023-12-10T03:36:10.182341900Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda = True\n",
    "auto_t = True # Whether to perform automatic template generation\n",
    "auto_v = True # Whether to perform automatic verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.226340200Z",
     "start_time": "2023-12-10T03:36:10.195343500Z"
    }
   },
   "outputs": [],
   "source": [
    "from openprompt.data_utils.text_classification_dataset import SST2Processor\n",
    "dataset = {'train': SST2Processor().get_train_examples(\"SST-2/\"),\n",
    "           'validation': SST2Processor().get_dev_examples(\"SST-2/\"),\n",
    "           'test': SST2Processor().get_test_examples(\"SST-2/\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.607530300Z",
     "start_time": "2023-12-10T03:36:10.228340900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: {\n",
      "  \"guid\": \"train-0\",\n",
      "  \"label\": 0,\n",
      "  \"meta\": {\n",
      "    \"labelword\": \"terrible\"\n",
      "  },\n",
      "  \"text_a\": \"nothing happens , and it happens to flat characters .\",\n",
      "  \"text_b\": \"\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print('load model...')\n",
    "from openprompt.plms import load_plm\n",
    "# load mlm model for main tasks\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")\n",
    "\n",
    "# load generation model for template generation\n",
    "template_generate_model, template_generate_tokenizer, template_generate_model_config, template_tokenizer_wrapper = load_plm('t5', 't5-large')\n",
    "\n",
    "from openprompt.prompts import ManualVerbalizer, ManualTemplate\n",
    "verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "#   TODO: You need to switch LMBFFTemplateGenerationTemplate or ManualTemplate to                                 #\n",
    "#         compare auto generate template and manual generate template                                             #\n",
    "###################################################################################################################\n",
    "from openprompt.prompts.prompt_generator import LMBFFTemplateGenerationTemplate\n",
    "########################################\n",
    "#   LMBFFTemplateGenerationTemplate    #\n",
    "########################################\n",
    "import random\n",
    "\n",
    "# number of demonstrations\n",
    "num_demonstrations = 1  # try different number\n",
    "\n",
    "demonstrations = []\n",
    "\n",
    "for _ in range(num_demonstrations):\n",
    "    # random choice training set example with label 0 \n",
    "    random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n",
    "\n",
    "    # random choice training set example with label 1\n",
    "    random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n",
    "    \n",
    "    demonstration = f'{random_example_1.text_a} It was terrible. {random_example_2.text_a} It was great.'\n",
    "    demonstrations.append(demonstration)\n",
    "\n",
    "# You can modify the demonstrations and try different combinations\n",
    "template_text = '{\"placeholder\": \"text_a\"} {\"mask\"} {\"meta\": \"labelword\"} {\"mask\"}.' + ' '.join(demonstrations)\n",
    "template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "\n",
    "#############################################\n",
    "#   End of LMBFFTemplateGenerationTemplate  #\n",
    "#############################################\n",
    "\n",
    "########################################\n",
    "#          ManualTemplate              #\n",
    "########################################\n",
    "\n",
    "#template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} It was {\"mask\"}.')\n",
    "\n",
    "#############################################\n",
    "#          End of ManualTemplate            # \n",
    "#############################################\n",
    "\n",
    "###################################################################################################################\n",
    "#                                           End of your code                                                      #\n",
    "###################################################################################################################\n",
    "\n",
    "\n",
    "# view wrapped example\n",
    "wrapped_example = template.wrap_one_example(dataset['train'][0])\n",
    "print(\"dataset:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.673523500Z",
     "start_time": "2023-12-10T03:36:46.625523700Z"
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "\n",
    "# Returns the best evaluation score achieved during training\n",
    "def fit(model, train_dataloader, val_dataloader, loss_func, optimizer, epochs=5):\n",
    "    best_score = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_dataloader, loss_func, optimizer)\n",
    "        score = evaluate(model, val_dataloader)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "        print(f\"Epoch {epoch+1}: Train loss={train_loss}, Eval score={score}\")\n",
    "    return best_score\n",
    "\n",
    "# Trains the model on the training data and computes the training loss\n",
    "def train_epoch(model, train_dataloader, loss_func, optimizer):\n",
    "    model.train()\n",
    "    loss_all = []\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits #\n",
    "        # 2. Get labels                                     #\n",
    "        # 3. Evalutate using loss_func                      #\n",
    "        # 4. Append loss to loss_all                        #\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits\n",
    "        logits = model(batch=inputs)\n",
    "\n",
    "        # 2. Get labels\n",
    "        labels = inputs['label']\n",
    "\n",
    "        # 3. Evalutate using loss_func\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Append loss to loss_all\n",
    "        loss_all.append(loss.item())\n",
    "        #####################################################\n",
    "        #                 End of your code                  #\n",
    "        #####################################################\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return np.mean(loss_all)\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    with torch.no_grad():\n",
    "        for step, inputs in enumerate(val_dataloader):\n",
    "            if cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits #\n",
    "            # 2. Get labels                                     #\n",
    "            # 3. Extend labels to list                          #\n",
    "            # 4. Get predictions and extend preds to list       #\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits\n",
    "            logits = model(batch=inputs)\n",
    "\n",
    "            # 2. Get labels\n",
    "            labels = inputs['label']\n",
    "\n",
    "            # 3. Extend labels to list\n",
    "            alllabels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # 4. Get predictions and extend preds to list\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            allpreds.extend(preds.cpu().numpy())\n",
    "            #####################################################\n",
    "            #                 End of your code                  #\n",
    "            #####################################################\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generated template from TemplateGenerator and find the best template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:13:36.594464200Z",
     "start_time": "2023-12-10T03:36:46.673523500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 1454.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:37<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"placeholder\": \"text_a\"} It was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} it was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001b[A\n",
      "tokenizing: 32it [00:00, 202.53it/s]A\n",
      "\n",
      "tokenizing: 32it [00:00, 727.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.562330920593297, Eval score=0.5\n",
      "Epoch 2: Train loss=1.029085214715451, Eval score=0.5\n",
      "Epoch 3: Train loss=0.671642615867313, Eval score=0.71875\n",
      "Epoch 4: Train loss=0.2566610455396585, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [15:59<1:03:57, 959.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.2469192902863142, Eval score=0.78125\n",
      "Current template: {\"placeholder\": \"text_a\"} it was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' it was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 379.37it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.511358927668084, Eval score=0.5\n",
      "Epoch 2: Train loss=0.7635227439459413, Eval score=0.5\n",
      "Epoch 3: Train loss=0.3154368309772053, Eval score=0.53125\n",
      "Epoch 4: Train loss=0.8977778584977472, Eval score=0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [36:43<56:21, 1127.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.7606429383413342, Eval score=0.5\n",
      "Current template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 571.42it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1180.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.765889931773188, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.5693292848736746, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.041312409681268036, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.2322409824218994, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [56:36<38:33, 1156.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.22881872234574985, Eval score=0.84375\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 1066.65it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1523.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.7746381501195, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9883591458201408, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8319057337939739, Eval score=0.625\n",
      "Epoch 4: Train loss=0.508084288907412, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [1:16:21<19:27, 1167.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.39746711112820776, Eval score=0.53125\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 864.68it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1454.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.6042319842349002, Eval score=0.5\n",
      "Epoch 2: Train loss=1.069442194304429, Eval score=0.5\n",
      "Epoch 3: Train loss=0.47684725234284997, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.2768472472046142, Eval score=0.59375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:36:08<00:00, 1153.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.07557142606344769, Eval score=0.625\n",
      "Final best template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class ManualTemplateWithoutParse(ManualTemplate):\n",
    "    \"\"\"The generated template from TemplateGenerator is a list of dict of parsed template_text. So no further parsing is needed.\"\"\"\n",
    "    def on_text_set(self):\n",
    "        pass\n",
    "\n",
    "# Template generation\n",
    "if auto_t:\n",
    "    print('performing auto_t...')\n",
    "\n",
    "    if cuda:\n",
    "        template_generate_model = template_generate_model.cuda()\n",
    "    template_generator = T5TemplateGenerator(template_generate_model, template_generate_tokenizer, template_tokenizer_wrapper, verbalizer, beam_width=5) # Beam_width is set to 5 here for efficiency; to improve performance, try a larger number.\n",
    "\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=template_generate_tokenizer, tokenizer_wrapper_class=template_tokenizer_wrapper, batch_size=len(dataset['train']), decoder_max_length=128, max_seq_length=128, shuffle=False, teacher_forcing=False) # Register all data at once\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        template_generator._register_buffer(data)\n",
    "\n",
    "    template_generate_model.eval()\n",
    "    print('generating...')\n",
    "    template_texts = template_generator._get_templates()\n",
    "\n",
    "    original_template = template.text\n",
    "    template_texts = [template_generator.convert_template(template_text, original_template) for template_text in template_texts]\n",
    "    # template_generator._show_template()\n",
    "    template_generator.release_memory()\n",
    "    # Generate a number of candidate template text\n",
    "    print(template_texts)\n",
    "    \n",
    "    # Iterate over each candidate and select the best one\n",
    "    best_metrics = 0.0\n",
    "    best_template_text = None\n",
    "    for template_text in tqdm(template_texts):\n",
    "        verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "        template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "        print(f\"Current template: {template_text}\\n\\nWrapped example: {template.wrap_one_example(dataset['train'][0])}\")\n",
    "\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "        \n",
    "        #######################################################\n",
    "        # TODO: Use score to Find your best template_text     #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_template_text = template_text\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # Use the best template\n",
    "    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "    template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=best_template_text)\n",
    "    print(\"Final best template:\", best_template_text)\n",
    "    print()\n",
    "    print(\"Wrapped example:\", template.wrap_one_example(dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verbalizer template from VerbalizerGenerator and find the best verbalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T11:58:04.838076700Z",
     "start_time": "2023-12-10T05:13:36.638466100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_v...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 1391.30it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current label_words: ['terrible', 'beautiful']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001b[A\n",
      "tokenizing: 3it [00:00, 21.48it/s]\u001b[A\n",
      "tokenizing: 6it [00:01,  5.15it/s]\u001b[A\n",
      "tokenizing: 8it [00:01,  6.27it/s]\u001b[A\n",
      "tokenizing: 32it [00:01, 23.45it/s]\u001b[A\n",
      "\n",
      "tokenizing: 32it [00:00, 969.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.1593105591260544, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.5407680265489034, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.17979280870349612, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.006995583800744498, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [21:05<6:40:40, 1265.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.002051841642924046, Eval score=0.84375\n",
      "current label_words: ['horrible', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 695.63it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1388.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.5843039118335565, Eval score=0.84375\n",
      "Epoch 2: Train loss=0.0798521326687478, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.0026221817748819376, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.0010607897513068565, Eval score=0.78125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [42:02<6:18:07, 1260.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0004625524882726495, Eval score=0.78125\n",
      "current label_words: ['horrible', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 762.07it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1333.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.0936600251085586, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.849827597849071, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.414547988853883, Eval score=0.75\n",
      "Epoch 4: Train loss=0.0500250239797424, Eval score=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [1:02:46<5:55:04, 1253.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.004471211226245941, Eval score=0.8125\n",
      "current label_words: ['sad', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 415.60it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 941.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4185917818103917, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.5065413688316767, Eval score=0.875\n",
      "Epoch 3: Train loss=0.012464412650388113, Eval score=0.875\n",
      "Epoch 4: Train loss=0.002708091426967485, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [1:23:35<5:33:44, 1251.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0004699288858773798, Eval score=0.875\n",
      "current label_words: ['bad', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 551.73it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=3.1230944280390602, Eval score=0.5\n",
      "Epoch 2: Train loss=1.270681380527094, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8551086119841784, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.6224154182709754, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [1:45:07<5:16:30, 1266.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.22766433987999335, Eval score=0.8125\n",
      "current label_words: ['horrible', 'remarkable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 426.62it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1031.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.2276666148868003, Eval score=0.6875\n",
      "Epoch 2: Train loss=0.4889321715090773, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2634941844644345, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.25736280560994373, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [2:07:36<5:01:58, 1294.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.3903749104914027, Eval score=0.625\n",
      "current label_words: ['terrible', 'brilliant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 493.33it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.128300400949797, Eval score=0.5\n",
      "Epoch 2: Train loss=1.0674331626432831, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.6011688623111695, Eval score=0.875\n",
      "Epoch 4: Train loss=0.6810656589186692, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [2:28:49<4:38:53, 1287.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.1570308672144165, Eval score=0.84375\n",
      "current label_words: ['terrible', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 451.65it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1033.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.1662085960851982, Eval score=0.84375\n",
      "Epoch 2: Train loss=0.046194135922632995, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.20158991879031873, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.009880203132524912, Eval score=0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [2:51:16<4:21:16, 1306.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.27703922392970526, Eval score=0.90625\n",
      "current label_words: ['disappointing', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 415.57it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1032.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.3897865504303581, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.6985758165101288, Eval score=0.5625\n",
      "Epoch 3: Train loss=0.20094251398768392, Eval score=0.875\n",
      "Epoch 4: Train loss=0.021359096241212683, Eval score=0.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [3:11:17<3:53:28, 1273.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0014102687238164435, Eval score=0.90625\n",
      "current label_words: ['strange', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 410.23it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4916955009493904, Eval score=0.5\n",
      "Epoch 2: Train loss=0.5601507005485473, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.050391235166898696, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.04447055474645367, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [3:31:10<3:28:06, 1248.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.5673459974156145, Eval score=0.78125\n",
      "current label_words: ['terrible', 'good']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 363.21it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.9351653824437118, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9128216816243366, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.16854792425147025, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.004711857527581742, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [3:50:34<3:03:23, 1222.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0015127657302400621, Eval score=0.84375\n",
      "current label_words: ['bad', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 524.59it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.8022562539517715, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9302898038731655, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.43625156552297994, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.04196147369020764, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [4:10:06<2:40:57, 1207.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.012102704274127518, Eval score=0.65625\n",
      "current label_words: ['short', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 412.44it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1203.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.8797737168388267, Eval score=0.75\n",
      "Epoch 2: Train loss=0.3757321042244257, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2311989847357836, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.43366863449273296, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [4:30:04<2:20:30, 1204.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.620734619528541, Eval score=0.59375\n",
      "current label_words: ['disappointing', 'delightful']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 426.69it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1143.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.324861448905718, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.5381804386270232, Eval score=0.5625\n",
      "Epoch 3: Train loss=0.36967586419450527, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.12384688404563349, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [4:49:18<1:58:55, 1189.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.18691727038913086, Eval score=0.6875\n",
      "current label_words: ['bad', 'brilliant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 490.61it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1103.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.5005056243027184, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.7745201710495166, Eval score=0.875\n",
      "Epoch 3: Train loss=0.13810731278863386, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.004411970109288177, Eval score=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [5:08:29<1:38:08, 1177.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.00313117326692236, Eval score=0.78125\n",
      "current label_words: ['bad', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 492.79it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.182302282977588, Eval score=0.5\n",
      "Epoch 2: Train loss=0.5332906207768247, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.08191546555644891, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.35411459776105403, Eval score=0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [5:26:58<1:17:08, 1157.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.13951104862388775, Eval score=0.46875\n",
      "current label_words: ['horrible', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 466.99it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1219.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4121702450024713, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.32750329683221935, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.01347528245059948, Eval score=0.90625\n",
      "Epoch 4: Train loss=0.3011642301885331, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [5:45:44<57:23, 1147.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.022153147599055956, Eval score=0.875\n",
      "current label_words: ['fine', 'remarkable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 492.08it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1185.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.7259275259780793, Eval score=0.5\n",
      "Epoch 2: Train loss=0.7986743192886934, Eval score=0.65625\n",
      "Epoch 3: Train loss=0.6269949703128077, Eval score=0.5\n",
      "Epoch 4: Train loss=0.16056611831118062, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [6:04:43<38:10, 1145.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.003870869544044808, Eval score=0.78125\n",
      "current label_words: ['disappointing', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 454.61it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1183.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4480454477061357, Eval score=0.5\n",
      "Epoch 2: Train loss=0.8791331336833537, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.4247932309117459, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.2258107879970339, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [6:23:21<18:56, 1137.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0734178872121447, Eval score=0.875\n",
      "current label_words: ['terrible', 'fine']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 489.28it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1164.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.6715138442009447, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.6137157797584223, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.38552796499482156, Eval score=0.875\n",
      "Epoch 4: Train loss=0.34600197029577373, Eval score=0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [6:41:58<00:00, 1205.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.6120385159649686, Eval score=0.78125\n",
      "final best label words: ['terrible', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verbalizer generation\n",
    "from openprompt.prompts.prompt_generator import RobertaVerbalizerGenerator\n",
    "if auto_v:\n",
    "    print('performing auto_v...')\n",
    "    # Load generation model for verbalizer generation\n",
    "    if cuda:\n",
    "        plm = plm.cuda()\n",
    "    verbalizer_generator = RobertaVerbalizerGenerator(model=plm, tokenizer=tokenizer, candidate_num=20, label_word_num_per_class=20)\n",
    "    # To improve performance, try larger numbers\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, batch_size=32)\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        verbalizer_generator.register_buffer(data)\n",
    "    label_words_list = verbalizer_generator.generate()\n",
    "    verbalizer_generator.release_memory()\n",
    "\n",
    "    # Iterate over each candidate and select the best one\n",
    "    current_verbalizer = copy.deepcopy(verbalizer)\n",
    "    best_metrics = 0.0\n",
    "    best_label_words = None\n",
    "    for label_words in tqdm(label_words_list):\n",
    "        print(f\"current label_words: {label_words}\")\n",
    "        current_verbalizer.label_words = label_words\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, current_verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "\n",
    "        #######################################################\n",
    "        # TODO: Use score to find your best_label_word        #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_label_words = label_words\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # use the best verbalizer\n",
    "    print(\"final best label words:\", best_label_words)\n",
    "    verbalizer = ManualVerbalizer(tokenizer, num_classes=2, label_words=best_label_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:18:43.541949400Z",
     "start_time": "2023-12-10T11:58:04.849042700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 383.01it/s]\n",
      "tokenizing: 32it [00:00, 1142.78it/s]\n",
      "tokenizing: 872it [00:00, 1095.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.2980121186483302, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.38737686289732665, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.08258779709103692, Eval score=0.875\n",
      "Epoch 4: Train loss=0.3668047572554656, Eval score=0.84375\n",
      "Epoch 5: Train loss=0.00465579945631589, Eval score=0.84375\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "test_dataloader = PromptDataLoader(dataset['test'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "\n",
    "model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# It's always good practice to set no decay to bias and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/5b8876ed26fd495b8353ad7ce94b6f65)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:46:57.884699300Z",
     "start_time": "2023-12-10T12:18:43.474013600Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "allpreds = []\n",
    "for step, inputs in tqdm(enumerate(test_dataloader)):\n",
    "    if cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = model(inputs)\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(allpreds):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report (15 points)\n",
    "\n",
    "- Task 1: Compare **two** different models you employed and provide a brief discussion of your implementation.\n",
    "\n",
    "- Task 2: You need to try at least **three** different templates and verbalizers to compare how your prompts work with the model. Report your performance in zero-shot, one-shot, and few-shot scenarios, with examples drawn from the training set.\n",
    "\n",
    "- Task 3: Try at least three different manually crafted templates to compare them with auto-generated templates. Evaluate the performance with different numbers of demonstrations and plot the graph from Figure 3 in the paper (https://arxiv.org/pdf/2012.15723.pdf). Also, report your best template and verbalizer. . \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
