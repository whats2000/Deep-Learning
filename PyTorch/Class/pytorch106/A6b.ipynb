{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6Z1Snuk7rIK"
   },
   "source": [
    "# MIS 583 Assignment 6: Text Sentiment Classification with Prompt Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSwr9MgZogRZ"
   },
   "source": [
    "Before we start, please put your name and SID in following format: <br>\n",
    ": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷, M114020035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DzsjuDhlz_k"
   },
   "source": [
    "**Your Answer:**   \n",
    "Hi I'm 鄔仁迪, B104020009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d-Zzebq7rIM"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kc9gd_Wk7rIN"
   },
   "source": [
    "**Sentiment Classification** is an automated process of identifying opinions in text and labeling them as positive or negative based on the emotions customers express within them.\n",
    "\n",
    "In Task 1, you need to fine-tune a pre-trained language model (e.g., BERT) to predict the sentiment of given tweets.\n",
    "\n",
    "In Task 2, we employ prompts to enable the model to perform sentiment analysis through in-context learning, eliminating the need for additional training.\n",
    "\n",
    "In Task 3, you will use the method called LM-BFF to utilize the model in generating the optimal template and verbalizer autonomously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgFVtACk5ZYy"
   },
   "source": [
    "# Notice\n",
    "**You are not allow to use the model like GPT family or pre-trained weight using SST-2 and twitter dataset!!!!!!!!!!!!!!!!!**\n",
    "\n",
    "You can use BERT and RoBERTa encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giUId1Naqacs",
    "tags": []
   },
   "source": [
    "##  Versions of used packages\n",
    "\n",
    "We will check PyTorch version to make sure everything work properly.  \n",
    "We use `python==3.7.14`, `torch==1.12.1+cu113` and `torchvision==0.13.1+cu113`.  \n",
    "This is the default version in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:34.469380700Z",
     "start_time": "2023-12-15T17:31:29.590207100Z"
    },
    "id": "Vuw-gNvjqcYe",
    "outputId": "e4a163ba-f0a9-4fac-8fc8-21d20c5afedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]\n",
      "torch 2.1.0+cu118\n",
      "torchvision 0.16.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "print('python', sys.version.split('\\n')[0])\n",
    "print('torch', torch.__version__)\n",
    "print('torchvision', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JMymKQI5ZY0"
   },
   "source": [
    "# Task 1: Text Sentiment Classification (40 points)\n",
    "\n",
    "In this task, you need to fine-tune a pre-trained language model (e.g., BERT or RoBERTa encoder) to predict the sentiment of given tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a4s_a5D7rIR"
   },
   "source": [
    "## Loading Model and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPUkTbnL7rIR"
   },
   "source": [
    "First, let's talk about the model. The Hugging Face team has created an amazing framework called \"transformers\" for NLP tasks. It includes many state-of-the-art machine learning models for PyTorch, TensorFlow, and JAX.\n",
    "\n",
    "To start with this package, follow [this link to installation and a basic tutorial](https://pytorch.org/hub/huggingface_pytorch-transformers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:34.531387200Z",
     "start_time": "2023-12-15T17:31:34.473380300Z"
    },
    "id": "rK0ouXa09pDU",
    "outputId": "e7e8e9c1-316f-4791-826e-0054336d0eef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!echo happy installation\\n!pip -V\\n!pip install grpcio\\n!pip install google-auth\\n!pip install protobuf==3.9.2\\n!pip install pyprind\\n!pip install tqdm boto3 requests regex sentencepiece sacremoses'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you might need some additional installations there\n",
    "\"\"\"!echo happy installation\n",
    "!pip -V\n",
    "!pip install grpcio\n",
    "!pip install google-auth\n",
    "!pip install protobuf==3.9.2\n",
    "!pip install pyprind\n",
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:39.975451200Z",
     "start_time": "2023-12-15T17:31:34.487379500Z"
    },
    "id": "dmGCAevi7rIS",
    "outputId": "39935bf9-2a81-4796-cdcd-82a75f2dc353"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "#########################################################################\n",
    "#            Loading tokenizer and model from transformer               #\n",
    "#########################################################################\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, AutoConfig\n",
    "\n",
    "bert_type = 'roberta-base'\n",
    "\n",
    "# ---------- 1. load from torch.hub ----------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# create a Bert-extended task (classification)\n",
    "model = RobertaForSequenceClassification.from_pretrained(bert_type)\n",
    "\n",
    "# finetune from the output from bert to your task\n",
    "# Replace the out_proj layer in the classifier with a new Linear layer for 3 classes\n",
    "model.classifier.out_proj = nn.Linear(in_features=768, out_features=3, bias=True)\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiMThsYeDa2O"
   },
   "source": [
    "## How to Get Data\n",
    "\n",
    "Please open the file `twitter_sentiment.zip`, creat shortcut to your Google Drive.\n",
    "\n",
    "1. open [LINK of Google Drive](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
    "3. Select the location where you want to place the shortcut.\n",
    "4. Click Add shortcut.\n",
    "\n",
    "After above procedures, we have a shortcut of zip file of dataset.  \n",
    "We can access this in colab after granting the permission of Google Drive.\n",
    "\n",
    "---\n",
    "\n",
    "請先到共用雲端硬碟將檔案 `twitter_sentiment.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:40.000454300Z",
     "start_time": "2023-12-15T17:31:39.933450400Z"
    },
    "id": "lZnFgi5i_2oA"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqO8DiB6VRQZ"
   },
   "source": [
    "## Unzip Data\n",
    "\n",
    "解壓縮 `twitter_sentiment.zip` 後可以發現裡面有三個csv檔。\n",
    "\n",
    "- `train.csv`, `test.csv` and `val.csv`\n",
    "\n",
    "Training set 有 **10248** 筆資料.  \n",
    "Validation set 有 **1317** 筆資料.  \n",
    "Testing set 有 **3075** 筆資料.  \n",
    "\n",
    "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:40.001450500Z",
     "start_time": "2023-12-15T17:31:39.950449600Z"
    },
    "id": "OSlTMdxf8Zd7"
   },
   "outputs": [],
   "source": [
    "# !unzip -qq ./drive/MyDrive/twitter_sentiment.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:40.001450500Z",
     "start_time": "2023-12-15T17:31:39.967450100Z"
    },
    "id": "wf5GXTme7rIT"
   },
   "outputs": [],
   "source": [
    "# Utility function to extract text and label from csv file\n",
    "def get_texts(f_name='./twitter_sentiment', mode='train'):\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "\n",
    "    f_path = os.path.join(f_name, '{}.csv'.format(mode))\n",
    "    with open(f_path, encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for line in reader:\n",
    "            text_list.append(line['text'])\n",
    "            if mode != 'test':\n",
    "                label_list.append(int(line['sentiment_label']))\n",
    "\n",
    "    return text_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:40.002451700Z",
     "start_time": "2023-12-15T17:31:39.979449900Z"
    },
    "id": "6fpY0ZrK7rIV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, f_name='./twitter_sentiment', mode='train'):\n",
    "        self.mode = mode\n",
    "\n",
    "        text_list, label_list = get_texts(f_name, mode)\n",
    "        print('mode', mode, 'has', len(text_list), 'datas')\n",
    "        text_list = tokenizer(text_list,\n",
    "                             truncation=True, padding=True,\n",
    "                             return_tensors='pt')\n",
    "\n",
    "        self.text_list = text_list['input_ids']\n",
    "        self.mask_list = text_list['attention_mask']\n",
    "\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_list[idx]\n",
    "        mask = self.mask_list[idx]\n",
    "        if self.mode == 'test':\n",
    "            return text, mask\n",
    "        label = torch.tensor(self.label_list[idx])\n",
    "        return text, mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:41.499451500Z",
     "start_time": "2023-12-15T17:31:39.998449800Z"
    },
    "id": "nCmM4FSw7rIW",
    "outputId": "2d939ca7-a361-4330-861c-1700683a30ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode train has 10248 datas\n",
      "mode val has 1317 datas\n",
      "mode test has 3075 datas\n"
     ]
    }
   ],
   "source": [
    "dataset_train = TwitterDataset(mode='train')\n",
    "dataset_val = TwitterDataset(mode='val')\n",
    "dataset_test = TwitterDataset(mode='test')\n",
    "\n",
    "batch_size = 24\n",
    "train_data = DataLoader(dataset_train, batch_size=batch_size,\n",
    "                       shuffle=True)\n",
    "val_data = DataLoader(dataset_val, batch_size=batch_size // 2,\n",
    "                       shuffle=False)\n",
    "test_data = DataLoader(dataset_test, batch_size=batch_size // 2,\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:41.560834700Z",
     "start_time": "2023-12-15T17:31:41.502449600Z"
    },
    "id": "bqkvofHc7rIY",
    "outputId": "e96e8d6f-79d2-4134-b70e-071d21df20b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['<s>', '@', 'united', 'ĠI', 'Ġhave', 'Ġnever', 'Ġbeen', 'Ġmislead', 'Ġby', 'Ġa', 'Ġcompany', 'Ġas', 'Ġmany', 'Ġtimes', 'Ġas', 'ĠI', 'Ġhave', 'Ġthis', 'Ġweek', 'Ġby', 'ĠUnited', 'ĠAirlines', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "token to s <s>@united I have never been mislead by a company as many times as I have this week by United Airlines!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(dataset_train[0][0])\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:43.074968900Z",
     "start_time": "2023-12-15T17:31:41.518449600Z"
    },
    "id": "DxZrfCqW7rIY"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# 定義標籤平滑化的KL損失函數 Paper: https://arxiv.org/pdf/2312.06522.pdf\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = -1\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n",
    "# 設定標籤平滑化水平\n",
    "\"\"\"\n",
    "LS2: smoothing=0.03（即3%平滑化）\n",
    "LS3: smoothing=0.075（即7.5%平滑化）\n",
    "LS4: smoothing=0.15（即15%平滑化）\n",
    "LS5: smoothing=0.3（即30%平滑化）\n",
    "\"\"\"\n",
    "criterion = LabelSmoothingLoss(classes=3, smoothing=0.3)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpwgE2Gd7rIZ"
   },
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:43.087969800Z",
     "start_time": "2023-12-15T17:31:43.068973400Z"
    },
    "id": "zlaiAZAD7rIa"
   },
   "outputs": [],
   "source": [
    "def accuracy(raw_preds, y):\n",
    "    preds = raw_preds.argmax(dim=1)\n",
    "    acc = (preds == y).sum()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:31:43.781088600Z",
     "start_time": "2023-12-15T17:31:43.086983500Z"
    },
    "id": "dmc_Gms97rIa"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Testing process                              #\n",
    "        #########################################################################\n",
    "        # 1. Clean the gradients of optimizer\n",
    "        # 2. Put correct variables into model\n",
    "        # 3. Get prediction\n",
    "        # 4. Evalutate by criterion and accuracy\n",
    "\n",
    "        # Clean the gradients of optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(text, attention_mask=mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.logits, label)\n",
    "\n",
    "        # Compute accuracy using the provided function\n",
    "        acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "def test(model, data, criterion, log_loss=False):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Training process                             #\n",
    "        #########################################################################\n",
    "        # 1. Put correct variables into model\n",
    "        # 2. Get prediction\n",
    "        # 3. Evalutate by criterion and accuracy\n",
    "\n",
    "        # No gradient calculation for evaluation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(text, attention_mask=mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.logits, label)\n",
    "\n",
    "            # Compute accuracy\n",
    "            acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if log_loss:\n",
    "            val_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "# class for monitoring train and test acc/loss\n",
    "class Meter:\n",
    "    def __init__(self):\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def update(self, train_loss, train_acc, val_loss, val_acc):\n",
    "        self.train_loss_list.append(train_loss)\n",
    "        self.train_acc_list.append(train_acc)\n",
    "        self.val_loss_list.append(val_loss)\n",
    "        self.val_acc_list.append(val_acc)\n",
    "\n",
    "    def plot(self):\n",
    "        x = range(len(self.train_loss_list))\n",
    "        plt.plot(x, self.train_loss_list)\n",
    "        plt.plot(x, self.val_loss_list, color='r')\n",
    "        plt.legend(['train_loss', 'val_loss'])\n",
    "        plt.show()\n",
    "        plt.plot(x, self.train_acc_list)\n",
    "        plt.plot(x, self.val_acc_list, color='r')\n",
    "        plt.legend(['train_acc', 'val_acc'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExZyrKd57rIb"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:46:38.549277200Z",
     "start_time": "2023-12-15T17:31:43.779147Z"
    },
    "id": "bVDe-fRe7rIc",
    "outputId": "446a3224-9ee6-4b57-c700-ba5d2e29223e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/427 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:35<00:00,  2.74it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 32.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 0.03920696243050506 train_acc: 0.8010343481654957\n",
      "Epoch 1 val_loss:  0.07610615619494324 val_acc : 0.85041761579347\n",
      "---------- e 1 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:38<00:00,  2.70it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 33.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss: 0.03762321863367072 train_acc: 0.8673887587822015\n",
      "Epoch 2 val_loss:  0.07628057128838182 val_acc : 0.8519362186788155\n",
      "---------- e 2 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:39<00:00,  2.68it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 33.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss: 0.037006662011286134 train_acc: 0.8953942232630757\n",
      "Epoch 3 val_loss:  0.0764251428747503 val_acc : 0.8519362186788155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:40<00:00,  2.67it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 33.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss: 0.03647378207439561 train_acc: 0.9153981264637002\n",
      "Epoch 4 val_loss:  0.07631823287013814 val_acc : 0.8587699316628702\n",
      "---------- e 4 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [02:40<00:00,  2.66it/s]\n",
      "100%|██████████| 110/110 [00:03<00:00, 32.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss: 0.03598570158293617 train_acc: 0.9365729898516784\n",
      "Epoch 5 val_loss:  0.07669588382622466 val_acc : 0.8602885345482156\n",
      "---------- e 5 save best model ----------\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#                          Hyper-parameters                             #\n",
    "#########################################################################\n",
    "max_epoch = 5\n",
    "log_interval = 1\n",
    "best_acc = 0\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################\n",
    "\n",
    "m = Meter()\n",
    "\n",
    "for epoch in range(1, max_epoch + 1):\n",
    "    train_loss, train_acc = train(model, train_data, optimizer, criterion)\n",
    "    val_loss, val_acc = test(model, val_data, criterion, log_loss=True)\n",
    "\n",
    "    if epoch % log_interval == 0:\n",
    "        print('Epoch {} train_loss: {} train_acc: {}'.format(\n",
    "            epoch, train_loss, train_acc\n",
    "        ))\n",
    "        print('Epoch {} val_loss:  {} val_acc : {}'.format(\n",
    "            epoch, val_loss, val_acc\n",
    "        ))\n",
    "\n",
    "    m.update(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "    # model checkpoint\n",
    "    if val_acc > best_acc:\n",
    "        best_model = model\n",
    "        best_acc = val_acc\n",
    "        print('-'*10, 'e', epoch, 'save best model', '-'*10)\n",
    "        torch.save(model.state_dict(), 'ckpts/e{}.pt'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:46:38.751579Z",
     "start_time": "2023-12-15T17:46:38.556479400Z"
    },
    "id": "SmtW58OR7rIc",
    "outputId": "e605056c-a00c-4504-d68c-d26338c4dfe5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot them out\n",
    "m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcJcHf7n7rId"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/f072e95f51bc48978225941dba218241).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:20:09.480889100Z",
     "start_time": "2023-12-15T17:20:08.292888900Z"
    },
    "id": "Afsx9Cy15ZY4",
    "outputId": "2c34c373-f018-4766-96ec-52a4feec1beb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = model\n",
    "best_model.load_state_dict(torch.load('ckpts/e2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:20:16.819584100Z",
     "start_time": "2023-12-15T17:20:10.468888100Z"
    },
    "id": "Sf5UTlMZ7rId",
    "outputId": "d22483d9-aac3-4a3d-ca55-0c3c163e79fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257/257 [00:06<00:00, 40.21it/s]\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "\n",
    "total_out = []\n",
    "for text, mask in tqdm(test_data, total=len(test_data)):\n",
    "    text = text.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    output = best_model(text, mask)\n",
    "    pred = output.logits\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    total_out.append(pred)\n",
    "\n",
    "total_out = torch.cat(total_out).cpu().numpy().tolist()\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(total_out):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:41:35.870336300Z"
    },
    "is_executing": true,
    "id": "pXUH4ykZ5ZY5"
   },
   "outputs": [],
   "source": [
    "# Save best model\n",
    "torch.save(best_model.state_dict(), 'ckpts/best_roberta_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95V4JcAg5ZY5"
   },
   "source": [
    "# Task 2: In-Context learning (32 points)\n",
    "\n",
    "In this task, you will learn how to perform sentiment classification using prompts without the need for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T14:41:35.882334700Z"
    },
    "is_executing": true,
    "id": "cjWGKnkm5ZY5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyprind\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertTokenizer, BertForMaskedLM\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO6xBLpt5ZY5"
   },
   "source": [
    "# Loading model and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.591798400Z",
     "start_time": "2023-12-11T06:59:54.213731500Z"
    },
    "id": "8n8s-suY5ZY5",
    "outputId": "5204fa7a-00d7-4104-e8e7-017f5d0c746b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#         TODO: Design your own template(prefix) and verbalizer         #\n",
    "#########################################################################\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Zero-shot learning template\n",
    "        # self.prefix = \"This sentence is [MASK].\"\n",
    "\n",
    "        # One-shot learning template\n",
    "        # self.prefix = (\n",
    "        #     \"This sentence is bad. @united be worse?oh you can't! delayed with no reason on the way to Lon. [SEP] \"\n",
    "        #     \"This sentence is [MASK].\"\n",
    "        # )\n",
    "\n",
    "        # Few-shot learning template\n",
    "        self.prefix = (\n",
    "            \"This sentence is bad. @united be worse?oh you can't! delayed with no reason on the way to Lon. [SEP] \"\n",
    "            \"This sentence is okay. @AmericanAir what's the best number to use? [SEP] \"\n",
    "            \"This sentenc is good. @JetBlue I was so excited when I saw that you fly there! #ionlyflyblue. [SEP] \"\n",
    "            \"This sentence is [MASK].\"\n",
    "        )\n",
    "\n",
    "        # Define a more comprehensive verbalizer\n",
    "        self.verbalizer = {\n",
    "            'bad': 0,\n",
    "            'okay': 1,\n",
    "            'good': 2,\n",
    "        }\n",
    "\n",
    "        self.max_seq_length = 512\n",
    "        self.batch_size = 32\n",
    "\n",
    "\n",
    "config = Config()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bert_type = 'bert-base-uncased'\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(bert_type, num_labels=3)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(bert_type)\n",
    "\n",
    "bert = model.from_pretrained(bert_type, config=bert_config).to(device)\n",
    "\n",
    "#######################################################################\n",
    "#                        End of your code                             #\n",
    "#######################################################################\n",
    "\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.601784800Z",
     "start_time": "2023-12-11T07:00:00.592783400Z"
    },
    "id": "s0fSeui65ZY5"
   },
   "outputs": [],
   "source": [
    "# Utility function to obtaion verbalizer ids\n",
    "def obtain_verbalizer_ids(verbalizer, tokenizer):\n",
    "    verbalizer_ids = tokenizer.convert_tokens_to_ids(list(verbalizer.keys()))\n",
    "    index2ids = {i: verbalizer_ids[i] for i in range(len(verbalizer_ids))}\n",
    "    return verbalizer_ids, index2ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.605878500Z",
     "start_time": "2023-12-11T07:00:00.597783400Z"
    },
    "id": "0FYVERAI5ZY5"
   },
   "outputs": [],
   "source": [
    "verbalizer_ids, index2ids = obtain_verbalizer_ids(config.verbalizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.615507200Z",
     "start_time": "2023-12-11T07:00:00.607878800Z"
    },
    "id": "I4wo-dmM5ZY5"
   },
   "outputs": [],
   "source": [
    "# Utility function to concatenate prefix and text\n",
    "def concatenate_prefix(texts, config):\n",
    "    ##################################################\n",
    "    #   TODO: concatenate your own prefix and text   #\n",
    "    ##################################################\n",
    "    prefix_texts = [config.prefix + \" \" + text for text in texts]\n",
    "    ##################################################\n",
    "    #                 End of your code               #\n",
    "    ##################################################\n",
    "    return prefix_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.694028Z",
     "start_time": "2023-12-11T07:00:00.617508700Z"
    },
    "id": "Ag1ZJtg-5ZY9"
   },
   "outputs": [],
   "source": [
    "def load_data(config):\n",
    "    # ['texts', 'labels']\n",
    "    df = pd.read_csv('./twitter_sentiment/train.csv')\n",
    "    original_texts = df['text'].tolist()\n",
    "    labels = df['sentiment_label'].tolist()\n",
    "\n",
    "    texts = concatenate_prefix(original_texts, config)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "texts, labels = load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.706026300Z",
     "start_time": "2023-12-11T07:00:00.696029900Z"
    },
    "id": "qAUZRuqH5ZY9",
    "outputId": "06ed6463-a095-4f65-b7a0-ad7b56f414a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['[CLS]', 'this', 'sentence', 'is', 'bad', '.', '@', 'united', 'be', 'worse', '?', 'oh', 'you', 'can', \"'\", 't', '!', 'delayed', 'with', 'no', 'reason', 'on', 'the', 'way', 'to', 'lo', '##n', '.', '[SEP]', 'this', 'sentence', 'is', 'okay', '.', '@', 'americana', '##ir', 'what', \"'\", 's', 'the', 'best', 'number', 'to', 'use', '?', '[SEP]', 'this', 'sent', '##en', '##c', 'is', 'good', '.', '@', 'jet', '##bl', '##ue', 'i', 'was', 'so', 'excited', 'when', 'i', 'saw', 'that', 'you', 'fly', 'there', '!', '#', 'ion', '##ly', '##fly', '##bl', '##ue', '.', '[SEP]', 'this', 'sentence', 'is', '[MASK]', '.', '@', 'united', 'i', 'have', 'never', 'been', 'mis', '##lea', '##d', 'by', 'a', 'company', 'as', 'many', 'times', 'as', 'i', 'have', 'this', 'week', 'by', 'united', 'airlines', '!', '[SEP]']\n",
      "token to s [CLS] this sentence is bad . @ united be worse ? oh you can ' t ! delayed with no reason on the way to lon . [SEP] this sentence is okay . @ americanair what ' s the best number to use ? [SEP] this sentenc is good . @ jetblue i was so excited when i saw that you fly there ! # ionlyflyblue . [SEP] this sentence is [MASK] . @ united i have never been mislead by a company as many times as i have this week by united airlines ! [SEP]\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(tokenizer.encode(texts[0], add_special_tokens=True))\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.718528600Z",
     "start_time": "2023-12-11T07:00:00.712026600Z"
    },
    "id": "b6wcvnI05ZY9"
   },
   "outputs": [],
   "source": [
    "# Batching of texts and labels for training or processing in batches\n",
    "def pack_batch(texts, labels, batch_size):\n",
    "    \"\"\"\n",
    "    :param texts: list\n",
    "    :param labels: list\n",
    "    :param batch_size: int\n",
    "    :return batch_X: list\n",
    "            [[text11, text12, ...], [text21, text22, ...], ...]\n",
    "    :return batch_y: list\n",
    "            [[label11, label12, ...], [label21, label22, ...], ...]\n",
    "    :return batch_count: int\n",
    "    \"\"\"\n",
    "    assert len(texts) == len(labels)\n",
    "\n",
    "    if len(texts) % batch_size != 0:\n",
    "        flag = False\n",
    "        batch_count = int(len(texts) / batch_size) + 1\n",
    "    else:\n",
    "        flag = True\n",
    "        batch_count = int(len(texts) / batch_size)\n",
    "\n",
    "    batch_X, batch_y = [], []\n",
    "\n",
    "    if flag:\n",
    "        for i in range(batch_count):\n",
    "            batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "            batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "    else:\n",
    "        for i in range(batch_count):\n",
    "            if i == batch_count - 1:\n",
    "                batch_X.append(texts[i * batch_size:])\n",
    "                batch_y.append(labels[i * batch_size:])\n",
    "            else:\n",
    "                batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "                batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "\n",
    "    return batch_X, batch_y, batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:00:00.754528500Z",
     "start_time": "2023-12-11T07:00:00.720528100Z"
    },
    "id": "vek8Tla25ZY-"
   },
   "outputs": [],
   "source": [
    "batch_X, batch_y, batch_count = pack_batch(texts, labels, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:04:58.906945500Z",
     "start_time": "2023-12-11T07:00:00.739529800Z"
    },
    "id": "68qgiLoZ5ZY-",
    "outputId": "110116f8-6451-4d8a-cc8c-023960b493a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[100 %] Time elapsed: 00:05:03 | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.580991 | precision: 0.628245 | recall: 0.580991 | f1: 0.556272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:05:03\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    pper = pyprind.ProgPercent(batch_count)\n",
    "    for i in range(batch_count):\n",
    "        inputs = batch_X[i]\n",
    "        labels = batch_y[i]\n",
    "\n",
    "        tokens = tokenizer.batch_encode_plus(inputs, add_special_tokens=True,\n",
    "                                             max_length=config.max_seq_length,\n",
    "                                             padding='max_length', truncation=True)\n",
    "\n",
    "        ids = torch.tensor(tokens['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(tokens['attention_mask']).to(device)\n",
    "\n",
    "        # Shape: (batch_size, max_seq_length, vocab_size)\n",
    "        logits = bert(ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        mask_token_index = (ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "\n",
    "        # Find [MASK] logits\n",
    "        # shape: (batch_size, vocab_size)\n",
    "        masked_logits = logits[mask_token_index[0], mask_token_index[1], :]\n",
    "\n",
    "        # Extract the logits of the word in the verbalizer at the [MASK] position\n",
    "        # shape: (batch_size, verbalizer_size)\n",
    "        verbalizer_logits = masked_logits[:, verbalizer_ids]\n",
    "\n",
    "        # Construct a pseudo-distribution from the logits in these verbalizers\n",
    "        pseudo_distribution = softmax(verbalizer_logits)\n",
    "\n",
    "        #################################################################################\n",
    "        #   1. Find the index with the maximum probability in the pseudo-distribution   #\n",
    "        #   2. Convert the index to the corresponding word ID                           #\n",
    "        #   3. Convert the ID to a token                                                #\n",
    "        #   4. Find the label corresponding to the token                                #\n",
    "        #################################################################################\n",
    "        # 1. Find the index with the maximum probability in the pseudo-distribution\n",
    "        pred_indices = torch.argmax(pseudo_distribution, dim=1)\n",
    "\n",
    "        # 2. Convert the index to the corresponding word ID\n",
    "        pred_ids = [verbalizer_ids[index] for index in pred_indices]\n",
    "\n",
    "        # 3. Convert the ID to a token\n",
    "        pred_tokens = tokenizer.convert_ids_to_tokens(pred_ids)\n",
    "\n",
    "        # 4. Find the label corresponding to the token\n",
    "        pred_labels = [config.verbalizer[token] if token in config.verbalizer else -1 for token in pred_tokens]\n",
    "        pred_labels = np.array(pred_labels)\n",
    "        #################################################################################\n",
    "        #                             End of your code                                  #\n",
    "        #################################################################################\n",
    "\n",
    "        predict_all = np.append(predict_all, pred_labels)\n",
    "        labels_all = np.append(labels_all, labels)\n",
    "\n",
    "        pper.update()\n",
    "\n",
    "    acc = accuracy_score(labels_all, predict_all)\n",
    "    p = precision_score(labels_all, predict_all, average=\"weighted\")\n",
    "    r = recall_score(labels_all, predict_all, average=\"weighted\")\n",
    "    f1 = f1_score(labels_all, predict_all, average=\"weighted\")\n",
    "\n",
    "    print('accuracy: %f | precision: %f | recall: %f | f1: %f' % (acc, p, r, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T07:04:59.175549700Z",
     "start_time": "2023-12-11T07:04:58.906945500Z"
    },
    "id": "Rx9tcXRK5ZY-",
    "outputId": "cb8bbe7e-228c-4880-a473-b1ddd2d579ca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10243</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10244</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10245</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10246</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10247</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10248 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1\n",
       "0      0  2\n",
       "1      0  2\n",
       "2      2  2\n",
       "3      1  0\n",
       "4      0  2\n",
       "...   .. ..\n",
       "10243  0  2\n",
       "10244  0  1\n",
       "10245  0  0\n",
       "10246  2  2\n",
       "10247  1  1\n",
       "\n",
       "[10248 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([labels_all, predict_all]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HK1z31TX5ZY-"
   },
   "source": [
    "# Task 3: LM-BFF (45 points)\n",
    "\n",
    "https://arxiv.org/pdf/2012.15723.pdf\n",
    "\n",
    "Unlike the previous task, LM-BFF can generate templates and verbalizers automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weEpJ3lZ5ZY-"
   },
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N43toJL65ZY-"
   },
   "source": [
    "請先到共用雲端硬碟將檔案 `SST-2.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/14MDYFasXU94dUE9DjgfcZE61iTRI2007/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXhp3iIa5ZY-"
   },
   "source": [
    "# Install openprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T03:31:13.543193800Z",
     "start_time": "2023-12-09T03:30:21.808076200Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uSRxT8x_5ZY-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1702909113432,
     "user_tz": -480,
     "elapsed": 19736,
     "user": {
      "displayName": "whats2000 _",
      "userId": "13524042620637171415"
     }
    },
    "outputId": "78351b9e-58d0-4f2f-fcfc-a3977d836e44"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: openprompt in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
      "Requirement already satisfied: transformers>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from openprompt) (4.35.2)\n",
      "Requirement already satisfied: sentencepiece==0.1.96 in /usr/local/lib/python3.10/dist-packages (from openprompt) (0.1.96)\n",
      "Requirement already satisfied: tqdm>=4.62.2 in /usr/local/lib/python3.10/dist-packages (from openprompt) (4.66.1)\n",
      "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (from openprompt) (2.6.2.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from openprompt) (3.8.1)\n",
      "Requirement already satisfied: yacs in /usr/local/lib/python3.10/dist-packages (from openprompt) (0.1.8)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from openprompt) (0.3.7)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from openprompt) (2.15.0)\n",
      "Requirement already satisfied: rouge==1.0.0 in /usr/local/lib/python3.10/dist-packages (from openprompt) (1.0.0)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from openprompt) (10.0.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from openprompt) (1.11.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge==1.0.0->openprompt) (1.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (0.4.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (0.6)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->openprompt) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->openprompt) (1.3.2)\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX->openprompt) (3.20.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.10.0->openprompt) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->openprompt) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->openprompt) (2023.3.post1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.96)\n"
     ]
    }
   ],
   "source": [
    "!pip install openprompt\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hftYtky5ZY-"
   },
   "source": [
    "# Import openprompt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.181341Z",
     "start_time": "2023-12-10T03:35:52.654085900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZTrhYgoS5ZY_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1702909132704,
     "user_tz": -480,
     "elapsed": 17717,
     "user": {
      "displayName": "whats2000 _",
      "userId": "13524042620637171415"
     }
    },
    "outputId": "62d983e3-500f-48eb-a700-828d6a1680b6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jAZhg_3i5ZY_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1702909180153,
     "user_tz": -480,
     "elapsed": 39453,
     "user": {
      "displayName": "whats2000 _",
      "userId": "13524042620637171415"
     }
    },
    "outputId": "8785515d-4f9b-4d1a-9757-ba0db7d7f68b"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.194342100Z",
     "start_time": "2023-12-10T03:36:10.182341900Z"
    },
    "id": "y7lrRk0u5ZY_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1702909180153,
     "user_tz": -480,
     "elapsed": 4,
     "user": {
      "displayName": "whats2000 _",
      "userId": "13524042620637171415"
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cuda = True\n",
    "auto_t = False # Whether to perform automatic template generation\n",
    "auto_v = False # Whether to perform automatic verbalizer generation\n",
    "\n",
    "# Define the directory and file path\n",
    "ckpt_dir = 'ckpts'\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AubIjFbX5ZY_"
   },
   "source": [
    "# Load dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.226340200Z",
     "start_time": "2023-12-10T03:36:10.195343500Z"
    },
    "id": "NCjLRB205ZY_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1702909181315,
     "user_tz": -480,
     "elapsed": 1165,
     "user": {
      "displayName": "whats2000 _",
      "userId": "13524042620637171415"
     }
    }
   },
   "outputs": [],
   "source": [
    "from openprompt.data_utils.text_classification_dataset import SST2Processor\n",
    "dataset = {'train': SST2Processor().get_train_examples(\"/content/drive/MyDrive/Colab Notebooks/SST-2\"),\n",
    "           'validation': SST2Processor().get_dev_examples(\"/content/drive/MyDrive/Colab Notebooks/SST-2\"),\n",
    "           'test': SST2Processor().get_test_examples(\"/content/drive/MyDrive/Colab Notebooks/SST-2\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.607530300Z",
     "start_time": "2023-12-10T03:36:10.228340900Z"
    },
    "id": "T-Sl1mhx5ZY_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1702910748568,
     "user_tz": -480,
     "elapsed": 36784,
     "user": {
      "displayName": "whats2000 _",
      "userId": "13524042620637171415"
     }
    },
    "outputId": "a2e59cda-33e1-4a41-bc1a-1bd3c8880f05"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataset: {\n",
      "  \"guid\": \"train-0\",\n",
      "  \"label\": 0,\n",
      "  \"meta\": {},\n",
      "  \"text_a\": \"nothing happens , and it happens to flat characters .\",\n",
      "  \"text_b\": \"\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print('load model...')\n",
    "from openprompt.plms import load_plm\n",
    "# load mlm model for main tasks\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")\n",
    "\n",
    "# load generation model for template generation\n",
    "template_generate_model, template_generate_tokenizer, template_generate_model_config, template_tokenizer_wrapper = load_plm('t5', 't5-large')\n",
    "\n",
    "from openprompt.prompts import ManualVerbalizer, ManualTemplate\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "#   TODO: You need to switch LMBFFTemplateGenerationTemplate or ManualTemplate to                                 #\n",
    "#         compare auto generate template and manual generate template                                             #\n",
    "###################################################################################################################\n",
    "from openprompt.prompts.prompt_generator import LMBFFTemplateGenerationTemplate\n",
    "########################################\n",
    "#   LMBFFTemplateGenerationTemplate    #\n",
    "########################################\n",
    "import random\n",
    "\n",
    "if auto_t:\n",
    "    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "\n",
    "    # number of demonstrations\n",
    "    num_demonstrations = 4  # try different number\n",
    "\n",
    "    demonstrations = []\n",
    "\n",
    "    for _ in range(num_demonstrations):\n",
    "        # random choice training set example with label 0\n",
    "        random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n",
    "\n",
    "        # random choice training set example with label 1\n",
    "        random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n",
    "\n",
    "        demonstration = f'{random_example_1.text_a} It was terrible. {random_example_2.text_a} It was great.'\n",
    "        demonstrations.append(demonstration)\n",
    "\n",
    "    # You can modify the demonstrations and try different combinations\n",
    "    template_text = '{\"placeholder\": \"text_a\"} {\"mask\"} {\"meta\": \"labelword\"} {\"mask\"}.' + ' '.join(demonstrations)\n",
    "    template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "#############################################\n",
    "#   End of LMBFFTemplateGenerationTemplate  #\n",
    "#############################################\n",
    "\n",
    "########################################\n",
    "#          ManualTemplate              #\n",
    "########################################\n",
    "else:\n",
    "    # number of demonstrations\n",
    "    num_demonstrations = 4  # try different number\n",
    "\n",
    "    demonstrations = []\n",
    "\n",
    "    for _ in range(num_demonstrations):\n",
    "        # random choice training set example with label 0\n",
    "        random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n",
    "\n",
    "        # random choice training set example with label 1\n",
    "        random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n",
    "\n",
    "        demonstration = f'{random_example_1.text_a} It was terrible. {random_example_2.text_a} It was great.'\n",
    "        demonstrations.append(demonstration)\n",
    "    # ManualTemplate A\n",
    "    template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} It was {\"mask\"}.' + ' '.join(demonstrations))\n",
    "    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "\n",
    "    # ManualTemplate B\n",
    "    # template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} This sentence is {\"mask\"}.' + ' '.join(demonstrations))\n",
    "    # verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['bad','good'])\n",
    "\n",
    "    # ManualTemplate C\n",
    "    # template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} Sentence for analyze: This sentence is {\"mask\"}.' + ' '.join(demonstrations))\n",
    "    # verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['negative','positive'])\n",
    "#############################################\n",
    "#          End of ManualTemplate            #\n",
    "#############################################\n",
    "\n",
    "###################################################################################################################\n",
    "#                                           End of your code                                                      #\n",
    "###################################################################################################################\n",
    "\n",
    "\n",
    "# view wrapped example\n",
    "wrapped_example = template.wrap_one_example(dataset['train'][0])\n",
    "print(\"dataset:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OujPDWs45ZY_"
   },
   "source": [
    "# Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.673523500Z",
     "start_time": "2023-12-10T03:36:46.625523700Z"
    },
    "id": "gFUrnx-95ZY_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1702910753782,
     "user_tz": -480,
     "elapsed": 337,
     "user": {
      "displayName": "whats2000 _",
      "userId": "13524042620637171415"
     }
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "\n",
    "# Returns the best evaluation score achieved during training\n",
    "def fit(model, train_dataloader, val_dataloader, loss_func, optimizer, epochs=5):\n",
    "    best_score = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_dataloader, loss_func, optimizer)\n",
    "        score = evaluate(model, val_dataloader)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            torch.save(model.state_dict(), 'ckpts/best_model_by_template.pt')\n",
    "        print(f\"Epoch {epoch+1}: Train loss={train_loss}, Eval score={score}\")\n",
    "    return best_score\n",
    "\n",
    "# Trains the model on the training data and computes the training loss\n",
    "def train_epoch(model, train_dataloader, loss_func, optimizer):\n",
    "    model.train()\n",
    "    loss_all = []\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits #\n",
    "        # 2. Get labels                                     #\n",
    "        # 3. Evalutate using loss_func                      #\n",
    "        # 4. Append loss to loss_all                        #\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits\n",
    "        logits = model(batch=inputs)\n",
    "\n",
    "        # 2. Get labels\n",
    "        labels = inputs['label']\n",
    "\n",
    "        # 3. Evalutate using loss_func\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Append loss to loss_all\n",
    "        loss_all.append(loss.item())\n",
    "        #####################################################\n",
    "        #                 End of your code                  #\n",
    "        #####################################################\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return np.mean(loss_all)\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    with torch.no_grad():\n",
    "        for step, inputs in enumerate(val_dataloader):\n",
    "            if cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits #\n",
    "            # 2. Get labels                                     #\n",
    "            # 3. Extend labels to list                          #\n",
    "            # 4. Get predictions and extend preds to list       #\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits\n",
    "            logits = model(batch=inputs)\n",
    "\n",
    "            # 2. Get labels\n",
    "            labels = inputs['label']\n",
    "\n",
    "            # 3. Extend labels to list\n",
    "            alllabels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # 4. Get predictions and extend preds to list\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            allpreds.extend(preds.cpu().numpy())\n",
    "            #####################################################\n",
    "            #                 End of your code                  #\n",
    "            #####################################################\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0s5eNZr75ZZA"
   },
   "source": [
    "# Template generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlQ3rktW5ZZA"
   },
   "source": [
    "generated template from TemplateGenerator and find the best template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uLiIP3y85ZZA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1702910756790,
     "user_tz": -480,
     "elapsed": 2,
     "user": {
      "displayName": "whats2000 _",
      "userId": "13524042620637171415"
     }
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class ManualTemplateWithoutParse(ManualTemplate):\n",
    "    \"\"\"The generated template from TemplateGenerator is a list of dict of parsed template_text. So no further parsing is needed.\"\"\"\n",
    "    def on_text_set(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "performing auto_t...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "tokenizing: 32it [00:00, 504.77it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "generating...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 18/18 [03:19<00:00, 11.10s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['{\"placeholder\": \"text_a\"} It was {\"mask\"} ..nothing happens , and it happens to flat characters . It was terrible. a film that will enthrall the whole family . It was great. the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it \\'s also too stupid to realize that they \\'ve already seen this exact same movie a hundred times It was terrible. the concept is a hoot . It was great. just because a walk to remember is shrewd enough to activate girlish tear ducts does n\\'t mean it \\'s good enough for our girls . It was terrible. neil burger here succeeded in ... making the mystery of four decades back the springboard for a more immediate mystery in the present . It was great. often lingers just as long on the irrelevant as on the engaging , which gradually turns what time is it there ? It was terrible. the film jolts the laughs from the audience -- as if by cattle prod . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . nothing happens.nothing happens , and it happens to flat characters . It was terrible. a film that will enthrall the whole family . It was great. the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it \\'s also too stupid to realize that they \\'ve already seen this exact same movie a hundred times It was terrible. the concept is a hoot . It was great. just because a walk to remember is shrewd enough to activate girlish tear ducts does n\\'t mean it \\'s good enough for our girls . It was terrible. neil burger here succeeded in ... making the mystery of four decades back the springboard for a more immediate mystery in the present . It was great. often lingers just as long on the irrelevant as on the engaging , which gradually turns what time is it there ? It was terrible. the film jolts the laughs from the audience -- as if by cattle prod . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . everything happens.nothing happens , and it happens to flat characters . It was terrible. a film that will enthrall the whole family . It was great. the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it \\'s also too stupid to realize that they \\'ve already seen this exact same movie a hundred times It was terrible. the concept is a hoot . It was great. just because a walk to remember is shrewd enough to activate girlish tear ducts does n\\'t mean it \\'s good enough for our girls . It was terrible. neil burger here succeeded in ... making the mystery of four decades back the springboard for a more immediate mystery in the present . It was great. often lingers just as long on the irrelevant as on the engaging , which gradually turns what time is it there ? It was terrible. the film jolts the laughs from the audience -- as if by cattle prod . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . the plot is flat.nothing happens , and it happens to flat characters . It was terrible. a film that will enthrall the whole family . It was great. the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it \\'s also too stupid to realize that they \\'ve already seen this exact same movie a hundred times It was terrible. the concept is a hoot . It was great. just because a walk to remember is shrewd enough to activate girlish tear ducts does n\\'t mean it \\'s good enough for our girls . It was terrible. neil burger here succeeded in ... making the mystery of four decades back the springboard for a more immediate mystery in the present . It was great. often lingers just as long on the irrelevant as on the engaging , which gradually turns what time is it there ? It was terrible. the film jolts the laughs from the audience -- as if by cattle prod . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . the film is flat.nothing happens , and it happens to flat characters . It was terrible. a film that will enthrall the whole family . It was great. the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it \\'s also too stupid to realize that they \\'ve already seen this exact same movie a hundred times It was terrible. the concept is a hoot . It was great. just because a walk to remember is shrewd enough to activate girlish tear ducts does n\\'t mean it \\'s good enough for our girls . It was terrible. neil burger here succeeded in ... making the mystery of four decades back the springboard for a more immediate mystery in the present . It was great. often lingers just as long on the irrelevant as on the engaging , which gradually turns what time is it there ? It was terrible. the film jolts the laughs from the audience -- as if by cattle prod . It was great.']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} ..nothing happens , and it happens to flat characters . It was terrible. a film that will enthrall the whole family . It was great. the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the concept is a hoot . It was great. just because a walk to remember is shrewd enough to activate girlish tear ducts does n't mean it 's good enough for our girls . It was terrible. neil burger here succeeded in ... making the mystery of four decades back the springboard for a more immediate mystery in the present . It was great. often lingers just as long on the irrelevant as on the engaging , which gradually turns what time is it there ? It was terrible. the film jolts the laughs from the audience -- as if by cattle prod . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': \" ..nothing happens , and it happens to flat characters . It was terrible. a film that will enthrall the whole family . It was great. the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the concept is a hoot . It was great. just because a walk to remember is shrewd enough to activate girlish tear ducts does n't mean it 's good enough for our girls . It was terrible. neil burger here succeeded in ... making the mystery of four decades back the springboard for a more immediate mystery in the present . It was great. often lingers just as long on the irrelevant as on the engaging , which gradually turns what time is it there ? It was terrible. the film jolts the laughs from the audience -- as if by cattle prod . It was great.\", 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 413.50it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 391.50it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.4946343977135204, Eval score=0.5\n",
      "Epoch 2: Train loss=0.8851797990500927, Eval score=0.5\n",
      "Epoch 3: Train loss=0.5173136370140128, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.17564449286464878, Eval score=0.8125\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 20%|██        | 1/5 [02:27<09:48, 147.17s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.3112325806973786, Eval score=0.75\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . nothing happens.nothing happens , and it happens to flat characters . It was terrible. a film that will enthrall the whole family . It was great. the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the concept is a hoot . It was great. just because a walk to remember is shrewd enough to activate girlish tear ducts does n't mean it 's good enough for our girls . It was terrible. neil burger here succeeded in ... making the mystery of four decades back the springboard for a more immediate mystery in the present . It was great. often lingers just as long on the irrelevant as on the engaging , which gradually turns what time is it there ? It was terrible. the film jolts the laughs from the audience -- as if by cattle prod . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': \" . nothing happens.nothing happens , and it happens to flat characters . It was terrible. a film that will enthrall the whole family . It was great. the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the concept is a hoot . It was great. just because a walk to remember is shrewd enough to activate girlish tear ducts does n't mean it 's good enough for our girls . It was terrible. neil burger here succeeded in ... making the mystery of four decades back the springboard for a more immediate mystery in the present . It was great. often lingers just as long on the irrelevant as on the engaging , which gradually turns what time is it there ? It was terrible. the film jolts the laughs from the audience -- as if by cattle prod . It was great.\", 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 387.11it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 506.55it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.0736806745044305, Eval score=0.875\n",
      "Epoch 2: Train loss=0.41847860334155484, Eval score=0.5\n",
      "Epoch 3: Train loss=0.3479322086996035, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.349908948714301, Eval score=0.8125\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 40%|████      | 2/5 [04:55<07:23, 147.73s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.02802245668908654, Eval score=0.78125\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . everything happens.nothing happens , and it happens to flat characters . It was terrible. a film that will enthrall the whole family . It was great. the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the concept is a hoot . It was great. just because a walk to remember is shrewd enough to activate girlish tear ducts does n't mean it 's good enough for our girls . It was terrible. neil burger here succeeded in ... making the mystery of four decades back the springboard for a more immediate mystery in the present . It was great. often lingers just as long on the irrelevant as on the engaging , which gradually turns what time is it there ? It was terrible. the film jolts the laughs from the audience -- as if by cattle prod . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': \" . everything happens.nothing happens , and it happens to flat characters . It was terrible. a film that will enthrall the whole family . It was great. the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the concept is a hoot . It was great. just because a walk to remember is shrewd enough to activate girlish tear ducts does n't mean it 's good enough for our girls . It was terrible. neil burger here succeeded in ... making the mystery of four decades back the springboard for a more immediate mystery in the present . It was great. often lingers just as long on the irrelevant as on the engaging , which gradually turns what time is it there ? It was terrible. the film jolts the laughs from the audience -- as if by cattle prod . It was great.\", 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001B[A\n",
      "tokenizing: 32it [00:00, 274.05it/s]\n",
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001B[A\n",
      "tokenizing: 32it [00:00, 268.82it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=2.075998729204912, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9005076612811536, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8909644670784473, Eval score=0.625\n",
      "Epoch 4: Train loss=0.524562876613345, Eval score=0.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 60%|██████    | 3/5 [09:07<06:31, 195.55s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.2298063248778135, Eval score=0.84375\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . the plot is flat.nothing happens , and it happens to flat characters . It was terrible. a film that will enthrall the whole family . It was great. the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the concept is a hoot . It was great. just because a walk to remember is shrewd enough to activate girlish tear ducts does n't mean it 's good enough for our girls . It was terrible. neil burger here succeeded in ... making the mystery of four decades back the springboard for a more immediate mystery in the present . It was great. often lingers just as long on the irrelevant as on the engaging , which gradually turns what time is it there ? It was terrible. the film jolts the laughs from the audience -- as if by cattle prod . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': \" . the plot is flat.nothing happens , and it happens to flat characters . It was terrible. a film that will enthrall the whole family . It was great. the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the concept is a hoot . It was great. just because a walk to remember is shrewd enough to activate girlish tear ducts does n't mean it 's good enough for our girls . It was terrible. neil burger here succeeded in ... making the mystery of four decades back the springboard for a more immediate mystery in the present . It was great. often lingers just as long on the irrelevant as on the engaging , which gradually turns what time is it there ? It was terrible. the film jolts the laughs from the audience -- as if by cattle prod . It was great.\", 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 496.38it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 530.66it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.6587017360143221, Eval score=0.5\n",
      "Epoch 2: Train loss=0.4318069693108555, Eval score=0.875\n",
      "Epoch 3: Train loss=0.33179798610217404, Eval score=0.875\n",
      "Epoch 4: Train loss=0.48561923225679493, Eval score=0.8125\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 80%|████████  | 4/5 [12:24<03:16, 196.08s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.23021699525043005, Eval score=0.84375\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . the film is flat.nothing happens , and it happens to flat characters . It was terrible. a film that will enthrall the whole family . It was great. the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the concept is a hoot . It was great. just because a walk to remember is shrewd enough to activate girlish tear ducts does n't mean it 's good enough for our girls . It was terrible. neil burger here succeeded in ... making the mystery of four decades back the springboard for a more immediate mystery in the present . It was great. often lingers just as long on the irrelevant as on the engaging , which gradually turns what time is it there ? It was terrible. the film jolts the laughs from the audience -- as if by cattle prod . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': \" . the film is flat.nothing happens , and it happens to flat characters . It was terrible. a film that will enthrall the whole family . It was great. the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the concept is a hoot . It was great. just because a walk to remember is shrewd enough to activate girlish tear ducts does n't mean it 's good enough for our girls . It was terrible. neil burger here succeeded in ... making the mystery of four decades back the springboard for a more immediate mystery in the present . It was great. often lingers just as long on the irrelevant as on the engaging , which gradually turns what time is it there ? It was terrible. the film jolts the laughs from the audience -- as if by cattle prod . It was great.\", 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001B[A\n",
      "tokenizing: 32it [00:00, 257.63it/s]\n",
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001B[A\n",
      "tokenizing: 32it [00:00, 254.87it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.6403787810286303, Eval score=0.5\n",
      "Epoch 2: Train loss=0.2839757042238489, Eval score=0.75\n",
      "Epoch 3: Train loss=0.4986373214728701, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.169764008603579, Eval score=0.84375\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [18:24<00:00, 220.81s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.0007209689467799762, Eval score=0.875\n",
      "Final best template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . nothing happens.nothing happens , and it happens to flat characters . It was terrible. a film that will enthrall the whole family . It was great. the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the concept is a hoot . It was great. just because a walk to remember is shrewd enough to activate girlish tear ducts does n't mean it 's good enough for our girls . It was terrible. neil burger here succeeded in ... making the mystery of four decades back the springboard for a more immediate mystery in the present . It was great. often lingers just as long on the irrelevant as on the engaging , which gradually turns what time is it there ? It was terrible. the film jolts the laughs from the audience -- as if by cattle prod . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': \" . nothing happens.nothing happens , and it happens to flat characters . It was terrible. a film that will enthrall the whole family . It was great. the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times It was terrible. the concept is a hoot . It was great. just because a walk to remember is shrewd enough to activate girlish tear ducts does n't mean it 's good enough for our girls . It was terrible. neil burger here succeeded in ... making the mystery of four decades back the springboard for a more immediate mystery in the present . It was great. often lingers just as long on the irrelevant as on the engaging , which gradually turns what time is it there ? It was terrible. the film jolts the laughs from the audience -- as if by cattle prod . It was great.\", 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Template generation\n",
    "if auto_t:\n",
    "    print('performing auto_t...')\n",
    "\n",
    "    if cuda:\n",
    "        template_generate_model = template_generate_model.cuda()\n",
    "    template_generator = T5TemplateGenerator(template_generate_model, template_generate_tokenizer, template_tokenizer_wrapper, verbalizer, beam_width=5) # Beam_width is set to 5 here for efficiency; to improve performance, try a larger number.\n",
    "\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=template_generate_tokenizer, tokenizer_wrapper_class=template_tokenizer_wrapper, batch_size=len(dataset['train']), decoder_max_length=512, max_seq_length=512, shuffle=False, teacher_forcing=False) # Register all data at once\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        template_generator._register_buffer(data)\n",
    "\n",
    "    template_generate_model.eval()\n",
    "    print('generating...')\n",
    "    template_texts = template_generator._get_templates()\n",
    "\n",
    "    original_template = template.text\n",
    "    template_texts = [template_generator.convert_template(template_text, original_template) for template_text in template_texts]\n",
    "    # template_generator._show_template()\n",
    "    template_generator.release_memory()\n",
    "    # Generate a number of candidate template text\n",
    "    print(template_texts)\n",
    "\n",
    "    # Iterate over each candidate and select the best one\n",
    "    best_metrics = 0.0\n",
    "    best_template_text = None\n",
    "    for template_text in tqdm(template_texts):\n",
    "        verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "        template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "        print(f\"Current template: {template_text}\\n\\nWrapped example: {template.wrap_one_example(dataset['train'][0])}\")\n",
    "\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "\n",
    "        #######################################################\n",
    "        # TODO: Use score to Find your best template_text     #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_template_text = template_text\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # Use the best template\n",
    "    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "    template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=best_template_text)\n",
    "    print(\"Final best template:\", best_template_text)\n",
    "    print()\n",
    "    print(\"Wrapped example:\", template.wrap_one_example(dataset[\"train\"][0]))"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:13:36.594464200Z",
     "start_time": "2023-12-10T03:36:46.673523500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ShOASdkl5ZZA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1702904023891,
     "user_tz": -480,
     "elapsed": 1307479,
     "user": {
      "displayName": "whats2000 _",
      "userId": "13524042620637171415"
     }
    },
    "outputId": "f54e8300-22ac-42ea-842f-13c81b1f414e"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYOp5jZ85ZZA"
   },
   "source": [
    "# Verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTKe5SOX5ZZA"
   },
   "source": [
    "verbalizer template from VerbalizerGenerator and find the best verbalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T11:58:04.838076700Z",
     "start_time": "2023-12-10T05:13:36.638466100Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-sIlZohO5ZZA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1702908291649,
     "user_tz": -480,
     "elapsed": 4266492,
     "user": {
      "displayName": "whats2000 _",
      "userId": "13524042620637171415"
     }
    },
    "outputId": "059e940b-33e9-4406-c71f-2b2625f3786a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "performing auto_v...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "tokenizing: 32it [00:00, 479.82it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "current label_words: ['dreadful', 'good']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 540.00it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 503.54it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.4418779945890492, Eval score=0.5\n",
      "Epoch 2: Train loss=1.5483898496750044, Eval score=0.5\n",
      "Epoch 3: Train loss=1.0393187629524618, Eval score=0.5\n",
      "Epoch 4: Train loss=0.8782225630711764, Eval score=0.84375\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r  5%|▌         | 1/20 [03:19<1:03:19, 199.99s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.33760338339197915, Eval score=0.5\n",
      "current label_words: ['awful', 'terrific']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 440.09it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 532.12it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=2.4773558018491713, Eval score=0.6875\n",
      "Epoch 2: Train loss=0.7145644505508244, Eval score=0.84375\n",
      "Epoch 3: Train loss=0.2616088666648011, Eval score=0.5\n",
      "Epoch 4: Train loss=1.3939823869150132, Eval score=0.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 10%|█         | 2/20 [06:41<1:00:15, 200.85s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.6918381443247199, Eval score=0.625\n",
      "current label_words: ['horrible', 'terrific']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 503.50it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 521.80it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.9709273227927042, Eval score=0.78125\n",
      "Epoch 2: Train loss=0.7343919142149389, Eval score=0.59375\n",
      "Epoch 3: Train loss=0.6275723083526827, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.4167134926683502, Eval score=0.65625\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 15%|█▌        | 3/20 [09:12<50:31, 178.31s/it]  "
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.41795693839753767, Eval score=0.625\n",
      "current label_words: ['awful', 'excellent']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 449.90it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 547.95it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.8858477063958075, Eval score=0.5\n",
      "Epoch 2: Train loss=0.8423814749112353, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8239326662151143, Eval score=0.5\n",
      "Epoch 4: Train loss=0.9015705908532254, Eval score=0.71875\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 20%|██        | 4/20 [12:40<50:35, 189.74s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.5304532944428502, Eval score=0.71875\n",
      "current label_words: ['devastating', 'perfect']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 524.84it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 528.78it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.3089162486949135, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.3165487935411875, Eval score=0.875\n",
      "Epoch 3: Train loss=0.8028901626912557, Eval score=0.90625\n",
      "Epoch 4: Train loss=0.24093646104483923, Eval score=0.90625\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 25%|██▌       | 5/20 [16:36<51:35, 206.38s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.018293241856554232, Eval score=0.875\n",
      "current label_words: ['dreadful', 'terrific']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 491.29it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 497.49it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=2.200600218430452, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.7992692454718053, Eval score=0.5\n",
      "Epoch 3: Train loss=0.5792897804640234, Eval score=0.90625\n",
      "Epoch 4: Train loss=0.446066882883315, Eval score=0.625\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 30%|███       | 6/20 [19:59<47:53, 205.26s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.27334663753572386, Eval score=0.875\n",
      "current label_words: ['disastrous', 'perfect']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 524.92it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 527.40it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=2.148460784352208, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9193787954282016, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8262611523387022, Eval score=0.5\n",
      "Epoch 4: Train loss=0.8136870309244841, Eval score=0.71875\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 35%|███▌      | 7/20 [23:30<44:55, 207.37s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.8246723371557891, Eval score=0.53125\n",
      "current label_words: ['excellent', 'terrific']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 498.49it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 519.94it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.2113548013439868, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.752667726483196, Eval score=0.53125\n",
      "Epoch 3: Train loss=0.6829169006086886, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.7641150722047314, Eval score=0.5625\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 40%|████      | 8/20 [27:46<44:31, 222.65s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.07451593255973421, Eval score=0.78125\n",
      "current label_words: ['bad', 'terrific']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 521.95it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 500.55it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.9928430195722129, Eval score=0.75\n",
      "Epoch 2: Train loss=0.5422443723655306, Eval score=0.84375\n",
      "Epoch 3: Train loss=0.02404342335099585, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.02572072479063081, Eval score=0.59375\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 45%|████▌     | 9/20 [31:09<39:42, 216.63s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.3989652201513536, Eval score=0.75\n",
      "current label_words: ['horrible', 'awesome']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 442.32it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 428.68it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.562164344226158, Eval score=0.9375\n",
      "Epoch 2: Train loss=0.1786581893093171, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.36053120036135056, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.02054297854192555, Eval score=0.8125\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 50%|█████     | 10/20 [33:41<32:46, 196.60s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.003348520088366058, Eval score=0.875\n",
      "current label_words: ['disastrous', 'awesome']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 476.97it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 395.57it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.7442095401602273, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.2238836304677534, Eval score=0.84375\n",
      "Epoch 3: Train loss=0.08799588326048968, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.002351849735695133, Eval score=0.8125\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 55%|█████▌    | 11/20 [37:01<29:37, 197.54s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.0006634553132509602, Eval score=0.8125\n",
      "current label_words: ['fine', 'perfect']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 482.16it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 522.59it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.5559320746649448, Eval score=0.65625\n",
      "Epoch 2: Train loss=0.3240035316930516, Eval score=0.875\n",
      "Epoch 3: Train loss=0.289992676048314, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.008957077240665967, Eval score=0.875\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 60%|██████    | 12/20 [41:20<28:50, 216.29s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.0004182804607921753, Eval score=0.90625\n",
      "current label_words: ['disastrous', 'marvelous']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 527.45it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 523.97it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.343435816908368, Eval score=0.875\n",
      "Epoch 2: Train loss=0.1722834688189181, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.006401543108372598, Eval score=0.875\n",
      "Epoch 4: Train loss=8.53570444696583e-05, Eval score=0.875\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 65%|██████▌   | 13/20 [44:48<24:57, 213.88s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.0001112944593828491, Eval score=0.875\n",
      "current label_words: ['disastrous', 'brilliant']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 512.93it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 489.62it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.6308704997505856, Eval score=0.5\n",
      "Epoch 2: Train loss=0.6668694718973711, Eval score=0.875\n",
      "Epoch 3: Train loss=0.32314306863599995, Eval score=0.875\n",
      "Epoch 4: Train loss=0.36641363489979994, Eval score=0.9375\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 70%|███████   | 14/20 [48:59<22:30, 225.14s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.3544367317663273, Eval score=0.875\n",
      "current label_words: ['incredible', 'perfect']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001B[A\n",
      "tokenizing: 32it [00:00, 291.95it/s]\n",
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001B[A\n",
      "tokenizing: 32it [00:00, 276.53it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=2.3306051243582715, Eval score=0.5\n",
      "Epoch 2: Train loss=0.8191916858777404, Eval score=0.5\n",
      "Epoch 3: Train loss=0.9323138245381415, Eval score=0.5\n",
      "Epoch 4: Train loss=0.8888364394661039, Eval score=0.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 75%|███████▌  | 15/20 [52:15<18:01, 216.30s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.6731900351587683, Eval score=0.8125\n",
      "current label_words: ['horrible', 'perfect']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 479.82it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 523.24it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.51905191669357, Eval score=0.6875\n",
      "Epoch 2: Train loss=0.605319028487429, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.015245699937054269, Eval score=0.75\n",
      "Epoch 4: Train loss=0.00025274148040566047, Eval score=0.75\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 80%|████████  | 16/20 [55:39<14:10, 212.54s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.00019586670020643737, Eval score=0.75\n",
      "current label_words: ['disastrous', 'incredible']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001B[A\n",
      "tokenizing: 32it [00:00, 246.02it/s]\n",
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001B[A\n",
      "tokenizing: 32it [00:00, 270.66it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=2.628712450990861, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.7688917011255398, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.44526085929828696, Eval score=0.75\n",
      "Epoch 4: Train loss=0.18349123687585234, Eval score=0.71875\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 85%|████████▌ | 17/20 [59:51<11:13, 224.35s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.40411152679826046, Eval score=0.875\n",
      "current label_words: ['devastating', 'awesome']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 493.12it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 505.85it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=2.6560526308775767, Eval score=0.46875\n",
      "Epoch 2: Train loss=0.7848476003855467, Eval score=0.5\n",
      "Epoch 3: Train loss=1.018752518342808, Eval score=0.5\n",
      "Epoch 4: Train loss=1.0488730832585134, Eval score=0.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 90%|█████████ | 18/20 [1:03:26<07:23, 221.58s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.8011876256205142, Eval score=0.5\n",
      "current label_words: ['devastating', 'incredible']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 479.08it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 503.07it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.9769604799512306, Eval score=0.5\n",
      "Epoch 2: Train loss=0.21768535726005211, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2537488829925678, Eval score=0.71875\n",
      "Epoch 4: Train loss=0.22309844995470485, Eval score=0.84375\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 95%|█████████▌| 19/20 [1:07:40<03:51, 231.26s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.07318617857310983, Eval score=0.78125\n",
      "current label_words: ['dreadful', 'excellent']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 496.74it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 510.98it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.9419679209986143, Eval score=0.5\n",
      "Epoch 2: Train loss=0.8552874866873026, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.5584743795952818, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.31302716940263053, Eval score=0.5625\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [1:11:00<00:00, 213.04s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train loss=0.871506059938838, Eval score=0.875\n",
      "final best label words: ['horrible', 'awesome']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verbalizer generation\n",
    "from openprompt.prompts.prompt_generator import RobertaVerbalizerGenerator\n",
    "if auto_v:\n",
    "    print('performing auto_v...')\n",
    "    # Load generation model for verbalizer generation\n",
    "    if cuda:\n",
    "        plm = plm.cuda()\n",
    "    verbalizer_generator = RobertaVerbalizerGenerator(model=plm, tokenizer=tokenizer, candidate_num=20, label_word_num_per_class=20)\n",
    "    # To improve performance, try larger numbers\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, batch_size=32)\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        verbalizer_generator.register_buffer(data)\n",
    "    label_words_list = verbalizer_generator.generate()\n",
    "    verbalizer_generator.release_memory()\n",
    "\n",
    "    # Iterate over each candidate and select the best one\n",
    "    current_verbalizer = copy.deepcopy(verbalizer)\n",
    "    best_metrics = 0.0\n",
    "    best_label_words = None\n",
    "    for label_words in tqdm(label_words_list):\n",
    "        print(f\"current label_words: {label_words}\")\n",
    "        current_verbalizer.label_words = label_words\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, current_verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "\n",
    "        #######################################################\n",
    "        # TODO: Use score to find your best_label_word        #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_label_words = label_words\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # use the best verbalizer\n",
    "    print(\"final best label words:\", best_label_words)\n",
    "    verbalizer = ManualVerbalizer(tokenizer, num_classes=2, label_words=best_label_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Auto Generate Template"
   ],
   "metadata": {
    "collapsed": false,
    "id": "x6oYYVCR5ZZA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:18:43.541949400Z",
     "start_time": "2023-12-10T11:58:04.849042700Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPFQRXXk5ZZA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1702908487579,
     "user_tz": -480,
     "elapsed": 92448,
     "user": {
      "displayName": "whats2000 _",
      "userId": "13524042620637171415"
     }
    },
    "outputId": "213693cf-eb2d-45e0-dec6-8e839648e8f7"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 261.85it/s]\n",
      "tokenizing: 32it [00:00, 277.42it/s]\n",
      "tokenizing: 872it [00:02, 320.32it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=0.5648131870239013, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.015312466964253701, Eval score=0.875\n",
      "Epoch 3: Train loss=0.00020708530388446889, Eval score=0.875\n",
      "Epoch 4: Train loss=7.901829550860384e-05, Eval score=0.875\n",
      "Epoch 5: Train loss=9.812058551617042e-05, Eval score=0.875\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "test_dataloader = PromptDataLoader(dataset['test'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "\n",
    "model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# It's always good practice to set no decay to bias and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "\n",
    "model.load_state_dict(torch.load('ckpts/best_model_by_template.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Result: 0.92295"
   ],
   "metadata": {
    "id": "WumgzU4iTdmF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Manual Template A: `It was [Mask].`"
   ],
   "metadata": {
    "collapsed": false,
    "id": "7vwEJq2y5ZZB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "tokenizing: 32it [00:00, 429.58it/s]\n",
      "tokenizing: 32it [00:00, 466.02it/s]\n",
      "tokenizing: 872it [00:01, 457.90it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=0.9716309494060624, Eval score=0.875\n",
      "Epoch 2: Train loss=0.006464897541775372, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.0014783659996737697, Eval score=0.90625\n",
      "Epoch 4: Train loss=0.0006579204292833651, Eval score=0.90625\n",
      "Epoch 5: Train loss=0.0005292369572771349, Eval score=0.90625\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer,\n",
    "                                    tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer,\n",
    "                                    tokenizer_wrapper_class=WrapperClass)\n",
    "test_dataloader = PromptDataLoader(dataset['test'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# It's always good practice to set no decay to bias and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "\n",
    "model.load_state_dict(torch.load('ckpts/best_model_by_template.pt'))"
   ],
   "metadata": {
    "id": "vXMHLeMd5ZZB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1702911089873,
     "user_tz": -480,
     "elapsed": 327955,
     "user": {
      "displayName": "whats2000 _",
      "userId": "13524042620637171415"
     }
    },
    "outputId": "47c2c644-470f-43c8-c624-1d2004e31554"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Manual Template B: `This sentence is [Mask].`"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Dc87m3RN5ZZB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "tokenizing: 32it [00:00, 368.78it/s]\n",
      "tokenizing: 32it [00:00, 434.34it/s]\n",
      "tokenizing: 872it [00:02, 428.48it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=1.1735597462393343, Eval score=0.5\n",
      "Epoch 2: Train loss=0.6550492576789111, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.3076258135720309, Eval score=0.9375\n",
      "Epoch 4: Train loss=0.07046551704297599, Eval score=0.90625\n",
      "Epoch 5: Train loss=0.006190342209265509, Eval score=0.875\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer,\n",
    "                                    tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer,\n",
    "                                    tokenizer_wrapper_class=WrapperClass)\n",
    "test_dataloader = PromptDataLoader(dataset['test'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# It's always good practice to set no decay to bias and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "\n",
    "model.load_state_dict(torch.load('ckpts/best_model_by_template.pt'))"
   ],
   "metadata": {
    "id": "z57Q92ji5ZZB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1702910408942,
     "user_tz": -480,
     "elapsed": 363203,
     "user": {
      "displayName": "whats2000 _",
      "userId": "13524042620637171415"
     }
    },
    "outputId": "ce22ef6f-f55b-4f57-b2be-d2012d2007e1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Manual Template C: `Sentence for analyze: This sentence is [Mask].`"
   ],
   "metadata": {
    "collapsed": false,
    "id": "f0AYoaZ65ZZB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "tokenizing: 32it [00:00, 184.65it/s]\n",
      "tokenizing: 32it [00:00, 35.63it/s]\n",
      "tokenizing: 872it [00:04, 186.94it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train loss=0.8918939381328528, Eval score=0.78125\n",
      "Epoch 2: Train loss=0.42001259433163796, Eval score=0.875\n",
      "Epoch 3: Train loss=0.012179564895632211, Eval score=0.875\n",
      "Epoch 4: Train loss=0.008113019405186606, Eval score=0.875\n",
      "Epoch 5: Train loss=0.0024462400031097786, Eval score=0.875\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer,\n",
    "                                    tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer,\n",
    "                                    tokenizer_wrapper_class=WrapperClass)\n",
    "test_dataloader = PromptDataLoader(dataset['test'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# It's always good practice to set no decay to bias and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "\n",
    "model.load_state_dict(torch.load('ckpts/best_model_by_template.pt'))"
   ],
   "metadata": {
    "id": "iTbQTPKv5ZZB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1702909398673,
     "user_tz": -480,
     "elapsed": 143836,
     "user": {
      "displayName": "whats2000 _",
      "userId": "13524042620637171415"
     }
    },
    "outputId": "0d41f0ac-ac9d-47e8-ade3-3f8c3ffb87e7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Result: 0.90819"
   ],
   "metadata": {
    "id": "aekX9ZwYYmD4"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDM3nuOd5ZZB"
   },
   "source": [
    "# Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/5b8876ed26fd495b8353ad7ce94b6f65)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:46:57.884699300Z",
     "start_time": "2023-12-10T12:18:43.474013600Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQoA8SVN5ZZB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1702911198430,
     "user_tz": -480,
     "elapsed": 108561,
     "user": {
      "displayName": "whats2000 _",
      "userId": "13524042620637171415"
     }
    },
    "outputId": "f6d35ed2-ae11-4345-af84-78249f8f3203"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "872it [01:48,  8.03it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "allpreds = []\n",
    "for step, inputs in tqdm(enumerate(test_dataloader)):\n",
    "    if cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = model(inputs)\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(allpreds):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMCrj5GX5ZZB"
   },
   "source": [
    "# Report (15 points)\n",
    "\n",
    "- Task 1: Compare **two** different models you employed and provide a brief discussion of your implementation.\n",
    "\n",
    "- Task 2: You need to try at least **three** different templates and verbalizers to compare how your prompts work with the model. Report your performance in zero-shot, one-shot, and few-shot scenarios, with examples drawn from the training set.\n",
    "\n",
    "- Task 3: Try at least three different manually crafted templates to compare them with auto-generated templates. Evaluate the performance with different numbers of demonstrations and plot the graph from Figure 3 in the paper (https://arxiv.org/pdf/2012.15723.pdf). Also, report your best template and verbalizer. .\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
