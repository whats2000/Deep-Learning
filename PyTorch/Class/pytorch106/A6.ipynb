{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6Z1Snuk7rIK"
   },
   "source": [
    "# MIS 583 Assignment 6: Text Sentiment Classification with Prompt Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSwr9MgZogRZ"
   },
   "source": [
    "Before we start, please put your name and SID in following format: <br>\n",
    ": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷, M114020035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DzsjuDhlz_k"
   },
   "source": [
    "**Your Answer:**   \n",
    "Hi I'm 鄔仁迪, B104020009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d-Zzebq7rIM"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kc9gd_Wk7rIN"
   },
   "source": [
    "**Sentiment Classification** is an automated process of identifying opinions in text and labeling them as positive or negative based on the emotions customers express within them.\n",
    "\n",
    "In Task 1, you need to fine-tune a pre-trained language model (e.g., BERT) to predict the sentiment of given tweets.\n",
    "\n",
    "In Task 2, we employ prompts to enable the model to perform sentiment analysis through in-context learning, eliminating the need for additional training.\n",
    "\n",
    "In Task 3, you will use the method called LM-BFF to utilize the model in generating the optimal template and verbalizer autonomously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice \n",
    "**You are not allow to use the model like GPT family or pre-trained weight using SST-2 and twitter dataset!!!!!!!!!!!!!!!!!**\n",
    "\n",
    "You can use BERT and RoBERTa encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giUId1Naqacs",
    "tags": []
   },
   "source": [
    "##  Versions of used packages\n",
    "\n",
    "We will check PyTorch version to make sure everything work properly.  \n",
    "We use `python==3.7.14`, `torch==1.12.1+cu113` and `torchvision==0.13.1+cu113`.  \n",
    "This is the default version in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Vuw-gNvjqcYe",
    "ExecuteTime": {
     "end_time": "2023-12-14T00:44:30.400529800Z",
     "start_time": "2023-12-14T00:44:22.179796900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]\n",
      "torch 2.1.0+cu118\n",
      "torchvision 0.16.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "print('python', sys.version.split('\\n')[0])\n",
    "print('torch', torch.__version__)\n",
    "print('torchvision', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Text Sentiment Classification (40 points)\n",
    "\n",
    "In this task, you need to fine-tune a pre-trained language model (e.g., BERT or RoBERTa encoder) to predict the sentiment of given tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a4s_a5D7rIR"
   },
   "source": [
    "## Loading Model and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPUkTbnL7rIR"
   },
   "source": [
    "First, let's talk about the model. The Hugging Face team has created an amazing framework called \"transformers\" for NLP tasks. It includes many state-of-the-art machine learning models for PyTorch, TensorFlow, and JAX.\n",
    "\n",
    "To start with this package, follow [this link to installation and a basic tutorial](https://pytorch.org/hub/huggingface_pytorch-transformers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rK0ouXa09pDU",
    "ExecuteTime": {
     "end_time": "2023-12-14T00:44:30.404183400Z",
     "start_time": "2023-12-14T00:44:30.400529800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'!echo happy installation\\n!pip -V\\n!pip install grpcio\\n!pip install google-auth\\n!pip install protobuf==3.9.2\\n!pip install pyprind\\n!pip install tqdm boto3 requests regex sentencepiece sacremoses'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you might need some additional installations there\n",
    "\"\"\"!echo happy installation\n",
    "!pip -V\n",
    "!pip install grpcio\n",
    "!pip install google-auth\n",
    "!pip install protobuf==3.9.2\n",
    "!pip install pyprind\n",
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dmGCAevi7rIS",
    "ExecuteTime": {
     "end_time": "2023-12-14T00:44:38.518294400Z",
     "start_time": "2023-12-14T00:44:30.405182700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "#########################################################################\n",
    "#            Loading tokenizer and model from transformer               #\n",
    "#########################################################################\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, AutoConfig\n",
    "\n",
    "bert_type = 'roberta-base'\n",
    "\n",
    "# ---------- 1. load from torch.hub ----------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# create a Bert-extended task (classification)\n",
    "model = RobertaForSequenceClassification.from_pretrained(bert_type)\n",
    "\n",
    "# finetune from the output from bert to your task\n",
    "# Replace the out_proj layer in the classifier with a new Linear layer for 3 classes\n",
    "model.classifier.out_proj = nn.Linear(in_features=768, out_features=3, bias=True)\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiMThsYeDa2O"
   },
   "source": [
    "## How to Get Data\n",
    "\n",
    "Please open the file `twitter_sentiment.zip`, creat shortcut to your Google Drive.\n",
    "\n",
    "1. open [LINK of Google Drive](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
    "3. Select the location where you want to place the shortcut.\n",
    "4. Click Add shortcut.\n",
    "\n",
    "After above procedures, we have a shortcut of zip file of dataset.  \n",
    "We can access this in colab after granting the permission of Google Drive.\n",
    "\n",
    "---\n",
    "\n",
    "請先到共用雲端硬碟將檔案 `twitter_sentiment.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lZnFgi5i_2oA",
    "ExecuteTime": {
     "end_time": "2023-12-14T00:44:38.519294200Z",
     "start_time": "2023-12-14T00:44:38.507704400Z"
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqO8DiB6VRQZ"
   },
   "source": [
    "## Unzip Data\n",
    "\n",
    "解壓縮 `twitter_sentiment.zip` 後可以發現裡面有三個csv檔。\n",
    "\n",
    "- `train.csv`, `test.csv` and `val.csv`\n",
    "\n",
    "Training set 有 **10248** 筆資料.  \n",
    "Validation set 有 **1317** 筆資料.  \n",
    "Testing set 有 **3075** 筆資料.  \n",
    "\n",
    "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OSlTMdxf8Zd7",
    "ExecuteTime": {
     "end_time": "2023-12-14T00:44:40.191002200Z",
     "start_time": "2023-12-14T00:44:38.512294300Z"
    }
   },
   "outputs": [],
   "source": [
    "# !unzip -qq ./drive/MyDrive/twitter_sentiment.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wf5GXTme7rIT",
    "ExecuteTime": {
     "end_time": "2023-12-14T00:44:40.191002200Z",
     "start_time": "2023-12-14T00:44:40.132531200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to extract text and label from csv file\n",
    "def get_texts(f_name='./twitter_sentiment', mode='train'):\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "\n",
    "    f_path = os.path.join(f_name, '{}.csv'.format(mode))\n",
    "    with open(f_path, encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for line in reader:\n",
    "            text_list.append(line['text'])\n",
    "            if mode != 'test':\n",
    "                label_list.append(int(line['sentiment_label']))\n",
    "\n",
    "    return text_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6fpY0ZrK7rIV",
    "ExecuteTime": {
     "end_time": "2023-12-14T00:44:40.282059200Z",
     "start_time": "2023-12-14T00:44:40.159697800Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, f_name='./twitter_sentiment', mode='train'):\n",
    "        self.mode = mode\n",
    "\n",
    "        text_list, label_list = get_texts(f_name, mode)\n",
    "        print('mode', mode, 'has', len(text_list), 'datas')\n",
    "        text_list = tokenizer(text_list,\n",
    "                             truncation=True, padding=True,\n",
    "                             return_tensors='pt')\n",
    "\n",
    "        self.text_list = text_list['input_ids']\n",
    "        self.mask_list = text_list['attention_mask']\n",
    "\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_list[idx]\n",
    "        mask = self.mask_list[idx]\n",
    "        if self.mode == 'test':\n",
    "            return text, mask\n",
    "        label = torch.tensor(self.label_list[idx])\n",
    "        return text, mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nCmM4FSw7rIW",
    "ExecuteTime": {
     "end_time": "2023-12-14T00:44:41.817431400Z",
     "start_time": "2023-12-14T00:44:40.283057800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode train has 10248 datas\n",
      "mode val has 1317 datas\n",
      "mode test has 3075 datas\n"
     ]
    }
   ],
   "source": [
    "dataset_train = TwitterDataset(mode='train')\n",
    "dataset_val = TwitterDataset(mode='val')\n",
    "dataset_test = TwitterDataset(mode='test')\n",
    "\n",
    "batch_size = 32\n",
    "train_data = DataLoader(dataset_train, batch_size=batch_size,\n",
    "                       shuffle=True)\n",
    "val_data = DataLoader(dataset_val, batch_size=batch_size // 2,\n",
    "                       shuffle=False)\n",
    "test_data = DataLoader(dataset_test, batch_size=batch_size // 2,\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bqkvofHc7rIY",
    "ExecuteTime": {
     "end_time": "2023-12-14T00:44:41.824560500Z",
     "start_time": "2023-12-14T00:44:41.816431200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['<s>', '@', 'united', 'ĠI', 'Ġhave', 'Ġnever', 'Ġbeen', 'Ġmislead', 'Ġby', 'Ġa', 'Ġcompany', 'Ġas', 'Ġmany', 'Ġtimes', 'Ġas', 'ĠI', 'Ġhave', 'Ġthis', 'Ġweek', 'Ġby', 'ĠUnited', 'ĠAirlines', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "token to s <s>@united I have never been mislead by a company as many times as I have this week by United Airlines!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(dataset_train[0][0])\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DxZrfCqW7rIY",
    "ExecuteTime": {
     "end_time": "2023-12-14T00:44:42.274929100Z",
     "start_time": "2023-12-14T00:44:41.824560500Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# 定義標籤平滑化的KL損失函數 Paper: https://arxiv.org/pdf/2312.06522.pdf\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = -1\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n",
    "# 設定標籤平滑化水平\n",
    "\"\"\"\n",
    "LS2: smoothing=0.03（即3%平滑化）\n",
    "LS3: smoothing=0.075（即7.5%平滑化）\n",
    "LS4: smoothing=0.15（即15%平滑化）\n",
    "LS5: smoothing=0.3（即30%平滑化）\n",
    "\"\"\"\n",
    "criterion = LabelSmoothingLoss(classes=3, smoothing=0.075)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpwgE2Gd7rIZ"
   },
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zlaiAZAD7rIa",
    "ExecuteTime": {
     "end_time": "2023-12-14T00:44:44.623118700Z",
     "start_time": "2023-12-14T00:44:44.616582100Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(raw_preds, y):\n",
    "    preds = raw_preds.argmax(dim=1)\n",
    "    acc = (preds == y).sum()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dmc_Gms97rIa",
    "ExecuteTime": {
     "end_time": "2023-12-14T00:44:46.999207700Z",
     "start_time": "2023-12-14T00:44:45.461161300Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Testing process                              #\n",
    "        #########################################################################\n",
    "        # 1. Clean the gradients of optimizer\n",
    "        # 2. Put correct variables into model\n",
    "        # 3. Get prediction\n",
    "        # 4. Evalutate by criterion and accuracy\n",
    "\n",
    "        # Clean the gradients of optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(text, attention_mask=mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.logits, label)\n",
    "\n",
    "        # Compute accuracy using the provided function\n",
    "        acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "def test(model, data, criterion, log_loss=False):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Training process                             #\n",
    "        #########################################################################\n",
    "        # 1. Put correct variables into model\n",
    "        # 2. Get prediction\n",
    "        # 3. Evalutate by criterion and accuracy\n",
    "\n",
    "        # No gradient calculation for evaluation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(text, attention_mask=mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.logits, label)\n",
    "\n",
    "            # Compute accuracy\n",
    "            acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if log_loss:\n",
    "            val_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "# class for monitoring train and test acc/loss\n",
    "class Meter:\n",
    "    def __init__(self):\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def update(self, train_loss, train_acc, val_loss, val_acc):\n",
    "        self.train_loss_list.append(train_loss)\n",
    "        self.train_acc_list.append(train_acc)\n",
    "        self.val_loss_list.append(val_loss)\n",
    "        self.val_acc_list.append(val_acc)\n",
    "\n",
    "    def plot(self):\n",
    "        x = range(len(self.train_loss_list))\n",
    "        plt.plot(x, self.train_loss_list)\n",
    "        plt.plot(x, self.val_loss_list, color='r')\n",
    "        plt.legend(['train_loss', 'val_loss'])\n",
    "        plt.show()\n",
    "        plt.plot(x, self.train_acc_list)\n",
    "        plt.plot(x, self.val_acc_list, color='r')\n",
    "        plt.legend(['train_acc', 'val_acc'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExZyrKd57rIb"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bVDe-fRe7rIc",
    "ExecuteTime": {
     "end_time": "2023-12-14T01:01:23.758794200Z",
     "start_time": "2023-12-14T00:44:48.255763200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:34<00:00,  2.07it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 28.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 0.02120756387861793 train_acc: 0.7795667447306791\n",
      "Epoch 1 val_loss:  0.03716525091487708 val_acc : 0.8435839028094153\n",
      "---------- e 1 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:32<00:00,  2.10it/s]\n",
      "100%|██████████| 83/83 [00:05<00:00, 15.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss: 0.017385279051639623 train_acc: 0.8625097580015613\n",
      "Epoch 2 val_loss:  0.035051837620087074 val_acc : 0.8625664388762339\n",
      "---------- e 2 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [04:00<00:00,  1.34it/s]\n",
      "100%|██████████| 83/83 [00:05<00:00, 16.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss: 0.01607438712835219 train_acc: 0.8895394223263076\n",
      "Epoch 3 val_loss:  0.03603086866659021 val_acc : 0.8572513287775246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [04:00<00:00,  1.34it/s]\n",
      "100%|██████████| 83/83 [00:05<00:00, 16.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss: 0.014730338443428347 train_acc: 0.9169594067135051\n",
      "Epoch 4 val_loss:  0.03742016651635833 val_acc : 0.856492027334852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:52<00:00,  1.86it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 28.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss: 0.013831915344045462 train_acc: 0.9325722092115535\n",
      "Epoch 5 val_loss:  0.03808154979255404 val_acc : 0.8542141230068337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#                          Hyper-parameters                             #\n",
    "#########################################################################\n",
    "max_epoch = 5\n",
    "log_interval = 1\n",
    "best_acc = 0\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################\n",
    "\n",
    "m = Meter()\n",
    "\n",
    "for epoch in range(1, max_epoch + 1):\n",
    "    train_loss, train_acc = train(model, train_data, optimizer, criterion)\n",
    "    val_loss, val_acc = test(model, val_data, criterion, log_loss=True)\n",
    "\n",
    "    if epoch % log_interval == 0:\n",
    "        print('Epoch {} train_loss: {} train_acc: {}'.format(\n",
    "            epoch, train_loss, train_acc\n",
    "        ))\n",
    "        print('Epoch {} val_loss:  {} val_acc : {}'.format(\n",
    "            epoch, val_loss, val_acc\n",
    "        ))\n",
    "\n",
    "    m.update(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "    # model checkpoint\n",
    "    if val_acc > best_acc:\n",
    "        best_model = model\n",
    "        best_acc = val_acc\n",
    "        print('-'*10, 'e', epoch, 'save best model', '-'*10)\n",
    "        torch.save(model.state_dict(), 'ckpts/e{}.pt'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SmtW58OR7rIc",
    "ExecuteTime": {
     "end_time": "2023-12-14T01:01:49.411948200Z",
     "start_time": "2023-12-14T01:01:49.133515900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot them out\n",
    "m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcJcHf7n7rId"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/f072e95f51bc48978225941dba218241).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = model\n",
    "best_model.load_state_dict(torch.load('ckpts/e2.pt'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T01:02:15.616865Z",
     "start_time": "2023-12-14T01:02:14.521744Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Sf5UTlMZ7rId",
    "ExecuteTime": {
     "end_time": "2023-12-14T01:02:26.981120200Z",
     "start_time": "2023-12-14T01:02:20.611701100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:06<00:00, 30.95it/s]\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "\n",
    "total_out = []\n",
    "for text, mask in tqdm(test_data, total=len(test_data)):\n",
    "    text = text.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    output = best_model(text, mask)\n",
    "    pred = output.logits\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    total_out.append(pred)\n",
    "\n",
    "total_out = torch.cat(total_out).cpu().numpy().tolist()\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(total_out):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Save best model\n",
    "torch.save(best_model.state_dict(), 'ckpts/best_model.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:03:51.107060Z",
     "start_time": "2023-12-13T18:03:49.503594900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: In-Context learning (32 points)\n",
    "\n",
    "In this task, you will learn how to perform sentiment classification using prompts without the need for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T17:20:33.671371500Z",
     "start_time": "2023-12-13T17:20:25.621846200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyprind\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertTokenizer, BertForMaskedLM\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T17:20:40.662163100Z",
     "start_time": "2023-12-13T17:20:33.674374400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#         TODO: Design your own template(prefix) and verbalizer         #\n",
    "#########################################################################\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # you can modify this line\n",
    "        self.prefix = 'This sentence is [MASK].'  # you can modify this line\n",
    "\n",
    "        self.verbalizer = {\n",
    "            'negative': 0,\n",
    "            'neutral': 1,\n",
    "            'positive': 2,\n",
    "        }\n",
    "\n",
    "        self.max_seq_length = 512\n",
    "        self.batch_size = 32\n",
    "\n",
    "\n",
    "config = Config()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bert_type = 'bert-base-uncased'\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(bert_type, num_labels=3)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(bert_type)\n",
    "\n",
    "bert = model.from_pretrained(bert_type, config=bert_config).to(device)\n",
    "\n",
    "#######################################################################\n",
    "#                        End of your code                             #\n",
    "#######################################################################\n",
    "\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T17:20:40.665168900Z",
     "start_time": "2023-12-13T17:20:40.663171100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to obtaion verbalizer ids\n",
    "def obtain_verbalizer_ids(verbalizer, tokenizer):\n",
    "    verbalizer_ids = tokenizer.convert_tokens_to_ids(list(verbalizer.keys()))\n",
    "    index2ids = {i: verbalizer_ids[i] for i in range(len(verbalizer_ids))}\n",
    "    return verbalizer_ids, index2ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T17:20:40.675722Z",
     "start_time": "2023-12-13T17:20:40.666168800Z"
    }
   },
   "outputs": [],
   "source": [
    "verbalizer_ids, index2ids = obtain_verbalizer_ids(config.verbalizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T17:20:40.682600200Z",
     "start_time": "2023-12-13T17:20:40.676730800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to concatenate prefix and text\n",
    "def concatenate_prefix(texts, config):\n",
    "    ##################################################\n",
    "    #   TODO: concatenate your own prefix and text   #                               \n",
    "    ##################################################\n",
    "    prefix_texts = [config.prefix + \" \" + text for text in texts]\n",
    "    ##################################################\n",
    "    #                 End of your code               #                               \n",
    "    ##################################################\n",
    "    return prefix_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T17:20:40.716410800Z",
     "start_time": "2023-12-13T17:20:40.683585400Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(config):\n",
    "    # ['texts', 'labels']\n",
    "    df = pd.read_csv('./twitter_sentiment/train.csv')\n",
    "    original_texts = df['text'].tolist()\n",
    "    labels = df['sentiment_label'].tolist()\n",
    "\n",
    "    texts = concatenate_prefix(original_texts, config)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "texts, labels = load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['[CLS]', 'this', 'sentence', 'is', '[MASK]', '.', '@', 'united', 'i', 'have', 'never', 'been', 'mis', '##lea', '##d', 'by', 'a', 'company', 'as', 'many', 'times', 'as', 'i', 'have', 'this', 'week', 'by', 'united', 'airlines', '!', '[SEP]']\n",
      "token to s [CLS] this sentence is [MASK] . @ united i have never been mislead by a company as many times as i have this week by united airlines ! [SEP]\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(tokenizer.encode(texts[0], add_special_tokens=True))\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T17:20:40.723614Z",
     "start_time": "2023-12-13T17:20:40.717407300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T17:20:40.731481300Z",
     "start_time": "2023-12-13T17:20:40.724616700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Batching of texts and labels for training or processing in batches\n",
    "def pack_batch(texts, labels, batch_size):\n",
    "    \"\"\"\n",
    "    :param texts: list\n",
    "    :param labels: list\n",
    "    :param batch_size: int\n",
    "    :return batch_X: list\n",
    "            [[text11, text12, ...], [text21, text22, ...], ...]\n",
    "    :return batch_y: list\n",
    "            [[label11, label12, ...], [label21, label22, ...], ...]\n",
    "    :return batch_count: int\n",
    "    \"\"\"\n",
    "    assert len(texts) == len(labels)\n",
    "\n",
    "    if len(texts) % batch_size != 0:\n",
    "        flag = False\n",
    "        batch_count = int(len(texts) / batch_size) + 1\n",
    "    else:\n",
    "        flag = True\n",
    "        batch_count = int(len(texts) / batch_size)\n",
    "\n",
    "    batch_X, batch_y = [], []\n",
    "\n",
    "    if flag:\n",
    "        for i in range(batch_count):\n",
    "            batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "            batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "    else:\n",
    "        for i in range(batch_count):\n",
    "            if i == batch_count - 1:\n",
    "                batch_X.append(texts[i * batch_size:])\n",
    "                batch_y.append(labels[i * batch_size:])\n",
    "            else:\n",
    "                batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "                batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "\n",
    "    return batch_X, batch_y, batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T17:20:40.740601900Z",
     "start_time": "2023-12-13T17:20:40.732483Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_X, batch_y, batch_count = pack_batch(texts, labels, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T17:25:48.427689300Z",
     "start_time": "2023-12-13T17:20:40.744602Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[100 %] Time elapsed: 00:05:07 | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.669399 | precision: 0.533643 | recall: 0.669399 | f1: 0.593294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:05:07\n",
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    pper = pyprind.ProgPercent(batch_count)\n",
    "    for i in range(batch_count):\n",
    "        inputs = batch_X[i]\n",
    "        labels = batch_y[i]\n",
    "\n",
    "        tokens = tokenizer.batch_encode_plus(inputs, add_special_tokens=True,\n",
    "                                             max_length=config.max_seq_length,\n",
    "                                             padding='max_length', truncation=True)\n",
    "        \n",
    "        ids = torch.tensor(tokens['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(tokens['attention_mask']).to(device)\n",
    "\n",
    "        # Shape: (batch_size, max_seq_length, vocab_size)\n",
    "        logits = bert(ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        mask_token_index = (ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "\n",
    "        # Find [MASK] logits\n",
    "        # shape: (batch_size, vocab_size)\n",
    "        masked_logits = logits[mask_token_index[0], mask_token_index[1], :]\n",
    "\n",
    "        # Extract the logits of the word in the verbalizer at the [MASK] position\n",
    "        # shape: (batch_size, verbalizer_size)\n",
    "        verbalizer_logits = masked_logits[:, verbalizer_ids]\n",
    "\n",
    "        # Construct a pseudo-distribution from the logits in these verbalizers\n",
    "        pseudo_distribution = softmax(verbalizer_logits)\n",
    "\n",
    "        #################################################################################\n",
    "        #   1. Find the index with the maximum probability in the pseudo-distribution   #\n",
    "        #   2. Convert the index to the corresponding word ID                           #\n",
    "        #   3. Convert the ID to a token                                                #\n",
    "        #   4. Find the label corresponding to the token                                #\n",
    "        #################################################################################\n",
    "        # 1. Find the index with the maximum probability in the pseudo-distribution\n",
    "        pred_indices = torch.argmax(pseudo_distribution, dim=1)\n",
    "\n",
    "        # 2. Convert the index to the corresponding word ID\n",
    "        pred_ids = [verbalizer_ids[index] for index in pred_indices]\n",
    "\n",
    "        # 3. Convert the ID to a token\n",
    "        pred_tokens = tokenizer.convert_ids_to_tokens(pred_ids)\n",
    "\n",
    "        # 4. Find the label corresponding to the token\n",
    "        pred_labels = [config.verbalizer[token] if token in config.verbalizer else -1 for token in pred_tokens]\n",
    "        pred_labels = np.array(pred_labels)\n",
    "        #################################################################################\n",
    "        #                             End of your code                                  #\n",
    "        #################################################################################\n",
    "\n",
    "        predict_all = np.append(predict_all, pred_labels)\n",
    "        labels_all = np.append(labels_all, labels)\n",
    "\n",
    "        pper.update()\n",
    "\n",
    "    acc = accuracy_score(labels_all, predict_all)\n",
    "    p = precision_score(labels_all, predict_all, average=\"weighted\")\n",
    "    r = recall_score(labels_all, predict_all, average=\"weighted\")\n",
    "    f1 = f1_score(labels_all, predict_all, average=\"weighted\")\n",
    "\n",
    "    print('accuracy: %f | precision: %f | recall: %f | f1: %f' % (acc, p, r, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "       0  1\n0      0  0\n1      0  0\n2      2  2\n3      1  0\n4      0  0\n...   .. ..\n10243  0  0\n10244  0  0\n10245  0  0\n10246  2  2\n10247  1  0\n\n[10248 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10243</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10244</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10245</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10246</th>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10247</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10248 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([labels_all, predict_all]).T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T17:25:48.681832600Z",
     "start_time": "2023-12-13T17:25:48.428696300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: LM-BFF (45 points)\n",
    "\n",
    "https://arxiv.org/pdf/2012.15723.pdf\n",
    "\n",
    "Unlike the previous task, LM-BFF can generate templates and verbalizers automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請先到共用雲端硬碟將檔案 `SST-2.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/14MDYFasXU94dUE9DjgfcZE61iTRI2007/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install openprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T03:31:13.543193800Z",
     "start_time": "2023-12-09T03:30:21.808076200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openprompt\n",
      "  Downloading openprompt-1.0.1-py3-none-any.whl (146 kB)\n",
      "     ---------------------------------------- 0.0/146.4 kB ? eta -:--:--\n",
      "     ---------------- ---------------------- 61.4/146.4 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 146.4/146.4 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting transformers>=4.10.0 (from openprompt)\n",
      "  Obtaining dependency information for transformers>=4.10.0 from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 123.5/123.5 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting sentencepiece==0.1.96 (from openprompt)\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 0.1/1.1 MB 2.2 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.2/1.1 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.3/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.4/1.1 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.5/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 0.6/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.6/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 0.7/1.1 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 0.8/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 0.9/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.0/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.1/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.1/1.1 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting tqdm>=4.62.2 (from openprompt)\n",
      "  Obtaining dependency information for tqdm>=4.62.2 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting tensorboardX (from openprompt)\n",
      "  Obtaining dependency information for tensorboardX from https://files.pythonhosted.org/packages/44/71/f3e7c9b2ab67e28c572ab4e9d5fa3499e0d252650f96d8a3a03e26677f53/tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nltk (from openprompt)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting yacs (from openprompt)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting dill (from openprompt)\n",
      "  Obtaining dependency information for dill from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting datasets (from openprompt)\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting rouge==1.0.0 (from openprompt)\n",
      "  Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting pyarrow (from openprompt)\n",
      "  Obtaining dependency information for pyarrow from https://files.pythonhosted.org/packages/28/82/9adfafaf0de581a39a1c86002cafd1a55a1255c9e0d362dc3e970aab0656/pyarrow-14.0.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading pyarrow-14.0.1-cp39-cp39-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openprompt) (1.8.0)\n",
      "Requirement already satisfied: six in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rouge==1.0.0->openprompt) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\eddie\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.62.2->openprompt) (0.4.4)\n",
      "Collecting filelock (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/fc/85/0d1038f068900896a8590d6d0da198b90d31f731a39166a432aa2b92249b/regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (2.25.1)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/9f/90/a6821e7757d2db194c16cbca78c80e206f30f6cc62c7f15fb27428f8c6dd/tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/4e/96/f4ee4434d8b6452fe7d5d44df2e72d1c6b2add1c3a5fb5c81aae83cb90c6/safetensors-0.4.1-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.1-cp39-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->openprompt)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets->openprompt) (1.4.1)\n",
      "Collecting xxhash (from datasets->openprompt)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/33/db/e07aa80e39a7ee82e9ca6f5a0fa9bf0b4465934272116a1b578eaee7f4b3/xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->openprompt)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/c6/c9/820b5ab056f4ada76fbe05bd481a948f287957d6cbfd59e2dd2618b408c1/multiprocess-0.70.15-py39-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets->openprompt)\n",
      "  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets->openprompt) (3.7.4.post0)\n",
      "Requirement already satisfied: click in c:\\users\\eddie\\appdata\\roaming\\python\\python39\\site-packages (from nltk->openprompt) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->openprompt) (1.1.0)\n",
      "Collecting protobuf>=3.20 (from tensorboardX->openprompt)\n",
      "  Obtaining dependency information for protobuf>=3.20 from https://files.pythonhosted.org/packages/b6/4b/f4f3334784576822d7817a664b757030ebb35b981978baf9c2eb3c5f33a8/protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (21.2.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (3.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (5.2.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (1.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers>=4.10.0->openprompt) (3.0.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets->openprompt) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets->openprompt) (2021.3)\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 41.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/7.9 MB 3.2 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.2/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.4/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.4/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.6/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.7/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.8/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.9/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.0/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.1/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.2/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.3/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.4/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.5/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.6/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.7/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.8/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.9/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.2/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.3/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.4/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.5/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.6/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.7/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.8/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.9/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.2/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.3/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.4/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.6/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.7/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.9/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.0/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.1/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.2/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.3/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.5/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.6/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.9/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.0/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.1/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.2/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.3/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.5/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.6/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.7/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.9/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.0/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.1/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.3/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.4/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.5/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.6/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.7/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.8/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.9/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.0/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.3/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.4/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.5/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.6/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.7/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.8/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "   ---------------------------------------- 0.0/521.2 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 92.2/521.2 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 174.1/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 276.5/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 368.6/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 471.0/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 521.2/521.2 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 61.4/115.3 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 115.3/115.3 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow-14.0.1-cp39-cp39-win_amd64.whl (24.6 MB)\n",
      "   ---------------------------------------- 0.0/24.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/24.6 MB 3.2 MB/s eta 0:00:08\n",
      "   ---------------------------------------- 0.2/24.6 MB 2.6 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 0.3/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.4/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.5/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.6/24.6 MB 2.2 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.7/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.8/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.8/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.9/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.0/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.1/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.2/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.3/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.4/24.6 MB 1.9 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.4/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.7/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.8/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.9/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.0/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.1/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.2/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.3/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.4/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.5/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.6/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.7/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.8/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.9/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 3.0/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.2/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.3/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.4/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.5/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.6/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.7/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.8/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.9/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.0/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.2/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.3/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.4/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.5/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.6/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.8/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.9/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.0/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.1/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.2/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.3/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.4/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.5/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.6/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.8/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.9/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.0/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.1/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.2/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.3/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.4/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.5/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.6/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.7/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 6.8/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 6.9/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.0/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.1/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.2/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.3/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.5/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.6/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.7/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.8/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.9/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.0/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.1/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.2/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.3/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.5/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 8.6/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.7/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.8/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.3/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.4/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.5/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.6/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.7/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.8/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 9.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.0/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.3/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.4/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 10.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.6/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.7/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.8/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.9/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 11.0/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.1/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.2/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.4/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.6/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.7/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 11.8/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 11.9/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.0/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.1/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.2/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.7/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.8/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.0/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.1/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.3/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.7/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.8/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.1/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.3/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 14.8/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 14.9/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.0/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.1/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.2/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.3/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.4/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.5/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.8/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.9/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.0/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.1/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.2/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.3/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.4/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.5/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 16.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 16.8/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 16.9/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.0/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.1/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.3/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.4/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.5/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.6/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.7/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.8/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 17.9/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.0/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.1/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.3/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.4/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.5/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.6/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.7/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.7/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.8/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 19.0/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.1/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.2/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.3/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.4/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.5/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.7/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.8/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.0/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.1/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.2/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.3/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.4/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.5/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.7/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.9/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.0/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.1/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.2/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.3/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.4/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.5/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.9/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.0/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.1/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.2/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.3/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.4/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.5/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.7/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.8/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.2/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.3/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.4/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.5/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.6/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.7/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.8/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.2/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.5/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.6/24.6 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.7 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 92.2/101.7 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 101.7/101.7 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 0.0/311.7 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 112.6/311.7 kB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 194.6/311.7 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  307.2/311.7 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 311.7/311.7 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 112.6/413.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 194.6/413.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 307.2/413.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 368.6/413.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 413.4/413.4 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 92.2/269.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 174.1/269.6 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/269.6 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 269.6/269.6 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp39-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.8 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 112.6/277.8 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 194.6/277.8 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 277.8/277.8 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.2 MB 1.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 1.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.4/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.3 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 92.2/133.3 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 133.3/133.3 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl (29 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 71.7/166.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 166.4/166.4 kB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, yacs, xxhash, tqdm, safetensors, rouge, regex, pyarrow-hotfix, pyarrow, protobuf, fsspec, filelock, dill, tensorboardX, nltk, multiprocess, huggingface-hub, tokenizers, transformers, datasets, openprompt\n",
      "Successfully installed datasets-2.15.0 dill-0.3.7 filelock-3.13.1 fsspec-2023.10.0 huggingface-hub-0.19.4 multiprocess-0.70.15 nltk-3.8.1 openprompt-1.0.1 protobuf-4.25.1 pyarrow-14.0.1 pyarrow-hotfix-0.6 regex-2023.10.3 rouge-1.0.0 safetensors-0.4.1 sentencepiece-0.1.96 tensorboardX-2.6.2.2 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.35.2 xxhash-3.4.1 yacs-0.1.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install openprompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import openprompt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.181341Z",
     "start_time": "2023-12-10T03:35:52.654085900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.194342100Z",
     "start_time": "2023-12-10T03:36:10.182341900Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda = True\n",
    "auto_t = True # Whether to perform automatic template generation\n",
    "auto_v = True # Whether to perform automatic verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.226340200Z",
     "start_time": "2023-12-10T03:36:10.195343500Z"
    }
   },
   "outputs": [],
   "source": [
    "from openprompt.data_utils.text_classification_dataset import SST2Processor\n",
    "dataset = {'train': SST2Processor().get_train_examples(\"SST-2/\"),\n",
    "           'validation': SST2Processor().get_dev_examples(\"SST-2/\"),\n",
    "           'test': SST2Processor().get_test_examples(\"SST-2/\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.607530300Z",
     "start_time": "2023-12-10T03:36:10.228340900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: {\n",
      "  \"guid\": \"train-0\",\n",
      "  \"label\": 0,\n",
      "  \"meta\": {\n",
      "    \"labelword\": \"terrible\"\n",
      "  },\n",
      "  \"text_a\": \"nothing happens , and it happens to flat characters .\",\n",
      "  \"text_b\": \"\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print('load model...')\n",
    "from openprompt.plms import load_plm\n",
    "# load mlm model for main tasks\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")\n",
    "\n",
    "# load generation model for template generation\n",
    "template_generate_model, template_generate_tokenizer, template_generate_model_config, template_tokenizer_wrapper = load_plm('t5', 't5-large')\n",
    "\n",
    "from openprompt.prompts import ManualVerbalizer, ManualTemplate\n",
    "verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "#   TODO: You need to switch LMBFFTemplateGenerationTemplate or ManualTemplate to                                 #\n",
    "#         compare auto generate template and manual generate template                                             #\n",
    "###################################################################################################################\n",
    "from openprompt.prompts.prompt_generator import LMBFFTemplateGenerationTemplate\n",
    "########################################\n",
    "#   LMBFFTemplateGenerationTemplate    #\n",
    "########################################\n",
    "import random\n",
    "\n",
    "# number of demonstrations\n",
    "num_demonstrations = 1  # try different number\n",
    "\n",
    "demonstrations = []\n",
    "\n",
    "for _ in range(num_demonstrations):\n",
    "    # random choice training set example with label 0 \n",
    "    random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n",
    "\n",
    "    # random choice training set example with label 1\n",
    "    random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n",
    "    \n",
    "    demonstration = f'{random_example_1.text_a} It was terrible. {random_example_2.text_a} It was great.'\n",
    "    demonstrations.append(demonstration)\n",
    "\n",
    "# You can modify the demonstrations and try different combinations\n",
    "template_text = '{\"placeholder\": \"text_a\"} {\"mask\"} {\"meta\": \"labelword\"} {\"mask\"}.' + ' '.join(demonstrations)\n",
    "template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "\n",
    "#############################################\n",
    "#   End of LMBFFTemplateGenerationTemplate  #\n",
    "#############################################\n",
    "\n",
    "########################################\n",
    "#          ManualTemplate              #\n",
    "########################################\n",
    "\n",
    "#template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} It was {\"mask\"}.')\n",
    "\n",
    "#############################################\n",
    "#          End of ManualTemplate            # \n",
    "#############################################\n",
    "\n",
    "###################################################################################################################\n",
    "#                                           End of your code                                                      #\n",
    "###################################################################################################################\n",
    "\n",
    "\n",
    "# view wrapped example\n",
    "wrapped_example = template.wrap_one_example(dataset['train'][0])\n",
    "print(\"dataset:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.673523500Z",
     "start_time": "2023-12-10T03:36:46.625523700Z"
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "\n",
    "# Returns the best evaluation score achieved during training\n",
    "def fit(model, train_dataloader, val_dataloader, loss_func, optimizer, epochs=5):\n",
    "    best_score = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_dataloader, loss_func, optimizer)\n",
    "        score = evaluate(model, val_dataloader)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "        print(f\"Epoch {epoch+1}: Train loss={train_loss}, Eval score={score}\")\n",
    "    return best_score\n",
    "\n",
    "# Trains the model on the training data and computes the training loss\n",
    "def train_epoch(model, train_dataloader, loss_func, optimizer):\n",
    "    model.train()\n",
    "    loss_all = []\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits #\n",
    "        # 2. Get labels                                     #\n",
    "        # 3. Evalutate using loss_func                      #\n",
    "        # 4. Append loss to loss_all                        #\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits\n",
    "        logits = model(batch=inputs)\n",
    "\n",
    "        # 2. Get labels\n",
    "        labels = inputs['label']\n",
    "\n",
    "        # 3. Evalutate using loss_func\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Append loss to loss_all\n",
    "        loss_all.append(loss.item())\n",
    "        #####################################################\n",
    "        #                 End of your code                  #\n",
    "        #####################################################\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return np.mean(loss_all)\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    with torch.no_grad():\n",
    "        for step, inputs in enumerate(val_dataloader):\n",
    "            if cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits #\n",
    "            # 2. Get labels                                     #\n",
    "            # 3. Extend labels to list                          #\n",
    "            # 4. Get predictions and extend preds to list       #\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits\n",
    "            logits = model(batch=inputs)\n",
    "\n",
    "            # 2. Get labels\n",
    "            labels = inputs['label']\n",
    "\n",
    "            # 3. Extend labels to list\n",
    "            alllabels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # 4. Get predictions and extend preds to list\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            allpreds.extend(preds.cpu().numpy())\n",
    "            #####################################################\n",
    "            #                 End of your code                  #\n",
    "            #####################################################\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generated template from TemplateGenerator and find the best template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:13:36.594464200Z",
     "start_time": "2023-12-10T03:36:46.673523500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 1454.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:37<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"placeholder\": \"text_a\"} It was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} it was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001B[A\n",
      "tokenizing: 32it [00:00, 202.53it/s]A\n",
      "\n",
      "tokenizing: 32it [00:00, 727.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.562330920593297, Eval score=0.5\n",
      "Epoch 2: Train loss=1.029085214715451, Eval score=0.5\n",
      "Epoch 3: Train loss=0.671642615867313, Eval score=0.71875\n",
      "Epoch 4: Train loss=0.2566610455396585, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [15:59<1:03:57, 959.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.2469192902863142, Eval score=0.78125\n",
      "Current template: {\"placeholder\": \"text_a\"} it was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' it was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 379.37it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.511358927668084, Eval score=0.5\n",
      "Epoch 2: Train loss=0.7635227439459413, Eval score=0.5\n",
      "Epoch 3: Train loss=0.3154368309772053, Eval score=0.53125\n",
      "Epoch 4: Train loss=0.8977778584977472, Eval score=0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [36:43<56:21, 1127.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.7606429383413342, Eval score=0.5\n",
      "Current template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 571.42it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1180.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.765889931773188, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.5693292848736746, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.041312409681268036, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.2322409824218994, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [56:36<38:33, 1156.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.22881872234574985, Eval score=0.84375\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 1066.65it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1523.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.7746381501195, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9883591458201408, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8319057337939739, Eval score=0.625\n",
      "Epoch 4: Train loss=0.508084288907412, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [1:16:21<19:27, 1167.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.39746711112820776, Eval score=0.53125\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 864.68it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1454.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.6042319842349002, Eval score=0.5\n",
      "Epoch 2: Train loss=1.069442194304429, Eval score=0.5\n",
      "Epoch 3: Train loss=0.47684725234284997, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.2768472472046142, Eval score=0.59375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:36:08<00:00, 1153.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.07557142606344769, Eval score=0.625\n",
      "Final best template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class ManualTemplateWithoutParse(ManualTemplate):\n",
    "    \"\"\"The generated template from TemplateGenerator is a list of dict of parsed template_text. So no further parsing is needed.\"\"\"\n",
    "    def on_text_set(self):\n",
    "        pass\n",
    "\n",
    "# Template generation\n",
    "if auto_t:\n",
    "    print('performing auto_t...')\n",
    "\n",
    "    if cuda:\n",
    "        template_generate_model = template_generate_model.cuda()\n",
    "    template_generator = T5TemplateGenerator(template_generate_model, template_generate_tokenizer, template_tokenizer_wrapper, verbalizer, beam_width=5) # Beam_width is set to 5 here for efficiency; to improve performance, try a larger number.\n",
    "\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=template_generate_tokenizer, tokenizer_wrapper_class=template_tokenizer_wrapper, batch_size=len(dataset['train']), decoder_max_length=128, max_seq_length=128, shuffle=False, teacher_forcing=False) # Register all data at once\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        template_generator._register_buffer(data)\n",
    "\n",
    "    template_generate_model.eval()\n",
    "    print('generating...')\n",
    "    template_texts = template_generator._get_templates()\n",
    "\n",
    "    original_template = template.text\n",
    "    template_texts = [template_generator.convert_template(template_text, original_template) for template_text in template_texts]\n",
    "    # template_generator._show_template()\n",
    "    template_generator.release_memory()\n",
    "    # Generate a number of candidate template text\n",
    "    print(template_texts)\n",
    "    \n",
    "    # Iterate over each candidate and select the best one\n",
    "    best_metrics = 0.0\n",
    "    best_template_text = None\n",
    "    for template_text in tqdm(template_texts):\n",
    "        verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "        template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "        print(f\"Current template: {template_text}\\n\\nWrapped example: {template.wrap_one_example(dataset['train'][0])}\")\n",
    "\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "        \n",
    "        #######################################################\n",
    "        # TODO: Use score to Find your best template_text     #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_template_text = template_text\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # Use the best template\n",
    "    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "    template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=best_template_text)\n",
    "    print(\"Final best template:\", best_template_text)\n",
    "    print()\n",
    "    print(\"Wrapped example:\", template.wrap_one_example(dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verbalizer template from VerbalizerGenerator and find the best verbalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T11:58:04.838076700Z",
     "start_time": "2023-12-10T05:13:36.638466100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_v...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 1391.30it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current label_words: ['terrible', 'beautiful']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001B[A\n",
      "tokenizing: 3it [00:00, 21.48it/s]\u001B[A\n",
      "tokenizing: 6it [00:01,  5.15it/s]\u001B[A\n",
      "tokenizing: 8it [00:01,  6.27it/s]\u001B[A\n",
      "tokenizing: 32it [00:01, 23.45it/s]\u001B[A\n",
      "\n",
      "tokenizing: 32it [00:00, 969.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.1593105591260544, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.5407680265489034, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.17979280870349612, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.006995583800744498, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [21:05<6:40:40, 1265.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.002051841642924046, Eval score=0.84375\n",
      "current label_words: ['horrible', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 695.63it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1388.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.5843039118335565, Eval score=0.84375\n",
      "Epoch 2: Train loss=0.0798521326687478, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.0026221817748819376, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.0010607897513068565, Eval score=0.78125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [42:02<6:18:07, 1260.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0004625524882726495, Eval score=0.78125\n",
      "current label_words: ['horrible', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 762.07it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1333.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.0936600251085586, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.849827597849071, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.414547988853883, Eval score=0.75\n",
      "Epoch 4: Train loss=0.0500250239797424, Eval score=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [1:02:46<5:55:04, 1253.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.004471211226245941, Eval score=0.8125\n",
      "current label_words: ['sad', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 415.60it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 941.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4185917818103917, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.5065413688316767, Eval score=0.875\n",
      "Epoch 3: Train loss=0.012464412650388113, Eval score=0.875\n",
      "Epoch 4: Train loss=0.002708091426967485, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [1:23:35<5:33:44, 1251.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0004699288858773798, Eval score=0.875\n",
      "current label_words: ['bad', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 551.73it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=3.1230944280390602, Eval score=0.5\n",
      "Epoch 2: Train loss=1.270681380527094, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8551086119841784, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.6224154182709754, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [1:45:07<5:16:30, 1266.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.22766433987999335, Eval score=0.8125\n",
      "current label_words: ['horrible', 'remarkable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 426.62it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1031.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.2276666148868003, Eval score=0.6875\n",
      "Epoch 2: Train loss=0.4889321715090773, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2634941844644345, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.25736280560994373, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [2:07:36<5:01:58, 1294.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.3903749104914027, Eval score=0.625\n",
      "current label_words: ['terrible', 'brilliant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 493.33it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.128300400949797, Eval score=0.5\n",
      "Epoch 2: Train loss=1.0674331626432831, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.6011688623111695, Eval score=0.875\n",
      "Epoch 4: Train loss=0.6810656589186692, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [2:28:49<4:38:53, 1287.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.1570308672144165, Eval score=0.84375\n",
      "current label_words: ['terrible', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 451.65it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1033.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.1662085960851982, Eval score=0.84375\n",
      "Epoch 2: Train loss=0.046194135922632995, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.20158991879031873, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.009880203132524912, Eval score=0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [2:51:16<4:21:16, 1306.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.27703922392970526, Eval score=0.90625\n",
      "current label_words: ['disappointing', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 415.57it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1032.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.3897865504303581, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.6985758165101288, Eval score=0.5625\n",
      "Epoch 3: Train loss=0.20094251398768392, Eval score=0.875\n",
      "Epoch 4: Train loss=0.021359096241212683, Eval score=0.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [3:11:17<3:53:28, 1273.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0014102687238164435, Eval score=0.90625\n",
      "current label_words: ['strange', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 410.23it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4916955009493904, Eval score=0.5\n",
      "Epoch 2: Train loss=0.5601507005485473, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.050391235166898696, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.04447055474645367, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [3:31:10<3:28:06, 1248.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.5673459974156145, Eval score=0.78125\n",
      "current label_words: ['terrible', 'good']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 363.21it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.9351653824437118, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9128216816243366, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.16854792425147025, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.004711857527581742, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [3:50:34<3:03:23, 1222.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0015127657302400621, Eval score=0.84375\n",
      "current label_words: ['bad', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 524.59it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.8022562539517715, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9302898038731655, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.43625156552297994, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.04196147369020764, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [4:10:06<2:40:57, 1207.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.012102704274127518, Eval score=0.65625\n",
      "current label_words: ['short', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 412.44it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1203.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.8797737168388267, Eval score=0.75\n",
      "Epoch 2: Train loss=0.3757321042244257, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2311989847357836, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.43366863449273296, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [4:30:04<2:20:30, 1204.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.620734619528541, Eval score=0.59375\n",
      "current label_words: ['disappointing', 'delightful']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 426.69it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1143.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.324861448905718, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.5381804386270232, Eval score=0.5625\n",
      "Epoch 3: Train loss=0.36967586419450527, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.12384688404563349, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [4:49:18<1:58:55, 1189.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.18691727038913086, Eval score=0.6875\n",
      "current label_words: ['bad', 'brilliant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 490.61it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1103.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.5005056243027184, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.7745201710495166, Eval score=0.875\n",
      "Epoch 3: Train loss=0.13810731278863386, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.004411970109288177, Eval score=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [5:08:29<1:38:08, 1177.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.00313117326692236, Eval score=0.78125\n",
      "current label_words: ['bad', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 492.79it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.182302282977588, Eval score=0.5\n",
      "Epoch 2: Train loss=0.5332906207768247, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.08191546555644891, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.35411459776105403, Eval score=0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [5:26:58<1:17:08, 1157.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.13951104862388775, Eval score=0.46875\n",
      "current label_words: ['horrible', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 466.99it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1219.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4121702450024713, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.32750329683221935, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.01347528245059948, Eval score=0.90625\n",
      "Epoch 4: Train loss=0.3011642301885331, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [5:45:44<57:23, 1147.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.022153147599055956, Eval score=0.875\n",
      "current label_words: ['fine', 'remarkable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 492.08it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1185.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.7259275259780793, Eval score=0.5\n",
      "Epoch 2: Train loss=0.7986743192886934, Eval score=0.65625\n",
      "Epoch 3: Train loss=0.6269949703128077, Eval score=0.5\n",
      "Epoch 4: Train loss=0.16056611831118062, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [6:04:43<38:10, 1145.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.003870869544044808, Eval score=0.78125\n",
      "current label_words: ['disappointing', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 454.61it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1183.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4480454477061357, Eval score=0.5\n",
      "Epoch 2: Train loss=0.8791331336833537, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.4247932309117459, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.2258107879970339, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [6:23:21<18:56, 1137.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0734178872121447, Eval score=0.875\n",
      "current label_words: ['terrible', 'fine']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 489.28it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1164.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.6715138442009447, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.6137157797584223, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.38552796499482156, Eval score=0.875\n",
      "Epoch 4: Train loss=0.34600197029577373, Eval score=0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [6:41:58<00:00, 1205.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.6120385159649686, Eval score=0.78125\n",
      "final best label words: ['terrible', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verbalizer generation\n",
    "from openprompt.prompts.prompt_generator import RobertaVerbalizerGenerator\n",
    "if auto_v:\n",
    "    print('performing auto_v...')\n",
    "    # Load generation model for verbalizer generation\n",
    "    if cuda:\n",
    "        plm = plm.cuda()\n",
    "    verbalizer_generator = RobertaVerbalizerGenerator(model=plm, tokenizer=tokenizer, candidate_num=20, label_word_num_per_class=20)\n",
    "    # To improve performance, try larger numbers\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, batch_size=32)\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        verbalizer_generator.register_buffer(data)\n",
    "    label_words_list = verbalizer_generator.generate()\n",
    "    verbalizer_generator.release_memory()\n",
    "\n",
    "    # Iterate over each candidate and select the best one\n",
    "    current_verbalizer = copy.deepcopy(verbalizer)\n",
    "    best_metrics = 0.0\n",
    "    best_label_words = None\n",
    "    for label_words in tqdm(label_words_list):\n",
    "        print(f\"current label_words: {label_words}\")\n",
    "        current_verbalizer.label_words = label_words\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, current_verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "\n",
    "        #######################################################\n",
    "        # TODO: Use score to find your best_label_word        #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_label_words = label_words\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # use the best verbalizer\n",
    "    print(\"final best label words:\", best_label_words)\n",
    "    verbalizer = ManualVerbalizer(tokenizer, num_classes=2, label_words=best_label_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:18:43.541949400Z",
     "start_time": "2023-12-10T11:58:04.849042700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 383.01it/s]\n",
      "tokenizing: 32it [00:00, 1142.78it/s]\n",
      "tokenizing: 872it [00:00, 1095.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.2980121186483302, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.38737686289732665, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.08258779709103692, Eval score=0.875\n",
      "Epoch 4: Train loss=0.3668047572554656, Eval score=0.84375\n",
      "Epoch 5: Train loss=0.00465579945631589, Eval score=0.84375\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "test_dataloader = PromptDataLoader(dataset['test'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "\n",
    "model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# It's always good practice to set no decay to bias and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/5b8876ed26fd495b8353ad7ce94b6f65)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:46:57.884699300Z",
     "start_time": "2023-12-10T12:18:43.474013600Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "allpreds = []\n",
    "for step, inputs in tqdm(enumerate(test_dataloader)):\n",
    "    if cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = model(inputs)\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(allpreds):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report (15 points)\n",
    "\n",
    "- Task 1: Compare **two** different models you employed and provide a brief discussion of your implementation.\n",
    "\n",
    "- Task 2: You need to try at least **three** different templates and verbalizers to compare how your prompts work with the model. Report your performance in zero-shot, one-shot, and few-shot scenarios, with examples drawn from the training set.\n",
    "\n",
    "- Task 3: Try at least three different manually crafted templates to compare them with auto-generated templates. Evaluate the performance with different numbers of demonstrations and plot the graph from Figure 3 in the paper (https://arxiv.org/pdf/2012.15723.pdf). Also, report your best template and verbalizer. . \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
