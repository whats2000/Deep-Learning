{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6Z1Snuk7rIK"
   },
   "source": [
    "# MIS 583 Assignment 6: Text Sentiment Classification with Prompt Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSwr9MgZogRZ"
   },
   "source": [
    "Before we start, please put your name and SID in following format: <br>\n",
    ": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷, M114020035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DzsjuDhlz_k"
   },
   "source": [
    "**Your Answer:**   \n",
    "Hi I'm 鄔仁迪, B104020009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d-Zzebq7rIM"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kc9gd_Wk7rIN"
   },
   "source": [
    "**Sentiment Classification** is an automated process of identifying opinions in text and labeling them as positive or negative based on the emotions customers express within them.\n",
    "\n",
    "In Task 1, you need to fine-tune a pre-trained language model (e.g., BERT) to predict the sentiment of given tweets.\n",
    "\n",
    "In Task 2, we employ prompts to enable the model to perform sentiment analysis through in-context learning, eliminating the need for additional training.\n",
    "\n",
    "In Task 3, you will use the method called LM-BFF to utilize the model in generating the optimal template and verbalizer autonomously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice \n",
    "**You are not allow to use the model like GPT family or pre-trained weight using SST-2 and twitter dataset!!!!!!!!!!!!!!!!!**\n",
    "\n",
    "You can use BERT and RoBERTa encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giUId1Naqacs",
    "tags": []
   },
   "source": [
    "##  Versions of used packages\n",
    "\n",
    "We will check PyTorch version to make sure everything work properly.  \n",
    "We use `python==3.7.14`, `torch==1.12.1+cu113` and `torchvision==0.13.1+cu113`.  \n",
    "This is the default version in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Vuw-gNvjqcYe",
    "ExecuteTime": {
     "end_time": "2023-12-13T17:33:40.318973300Z",
     "start_time": "2023-12-13T17:33:34.666964400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]\n",
      "torch 2.1.0+cu118\n",
      "torchvision 0.16.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "print('python', sys.version.split('\\n')[0])\n",
    "print('torch', torch.__version__)\n",
    "print('torchvision', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Text Sentiment Classification (40 points)\n",
    "\n",
    "In this task, you need to fine-tune a pre-trained language model (e.g., BERT or RoBERTa encoder) to predict the sentiment of given tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a4s_a5D7rIR"
   },
   "source": [
    "## Loading Model and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPUkTbnL7rIR"
   },
   "source": [
    "First, let's talk about the model. The Hugging Face team has created an amazing framework called \"transformers\" for NLP tasks. It includes many state-of-the-art machine learning models for PyTorch, TensorFlow, and JAX.\n",
    "\n",
    "To start with this package, follow [this link to installation and a basic tutorial](https://pytorch.org/hub/huggingface_pytorch-transformers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rK0ouXa09pDU",
    "ExecuteTime": {
     "end_time": "2023-12-13T17:33:40.319974300Z",
     "start_time": "2023-12-13T17:33:40.302732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'!echo happy installation\\n!pip -V\\n!pip install grpcio\\n!pip install google-auth\\n!pip install protobuf==3.9.2\\n!pip install pyprind\\n!pip install tqdm boto3 requests regex sentencepiece sacremoses'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you might need some additional installations there\n",
    "\"\"\"!echo happy installation\n",
    "!pip -V\n",
    "!pip install grpcio\n",
    "!pip install google-auth\n",
    "!pip install protobuf==3.9.2\n",
    "!pip install pyprind\n",
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dmGCAevi7rIS",
    "ExecuteTime": {
     "end_time": "2023-12-13T17:33:46.706851400Z",
     "start_time": "2023-12-13T17:33:40.318973300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "#########################################################################\n",
    "#            Loading tokenizer and model from transformer               #\n",
    "#########################################################################\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, AutoConfig\n",
    "\n",
    "bert_type = 'roberta-base'\n",
    "\n",
    "# ---------- 1. load from torch.hub ----------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# create a Bert-extended task (classification)\n",
    "model = RobertaForSequenceClassification.from_pretrained(bert_type)\n",
    "\n",
    "# finetune from the output from bert to your task\n",
    "# Replace the out_proj layer in the classifier with a new Linear layer for 3 classes\n",
    "model.classifier.out_proj = nn.Linear(in_features=768, out_features=3, bias=True)\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiMThsYeDa2O"
   },
   "source": [
    "## How to Get Data\n",
    "\n",
    "Please open the file `twitter_sentiment.zip`, creat shortcut to your Google Drive.\n",
    "\n",
    "1. open [LINK of Google Drive](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
    "3. Select the location where you want to place the shortcut.\n",
    "4. Click Add shortcut.\n",
    "\n",
    "After above procedures, we have a shortcut of zip file of dataset.  \n",
    "We can access this in colab after granting the permission of Google Drive.\n",
    "\n",
    "---\n",
    "\n",
    "請先到共用雲端硬碟將檔案 `twitter_sentiment.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lZnFgi5i_2oA",
    "ExecuteTime": {
     "end_time": "2023-12-13T17:33:46.706851400Z",
     "start_time": "2023-12-13T17:33:46.695176400Z"
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqO8DiB6VRQZ"
   },
   "source": [
    "## Unzip Data\n",
    "\n",
    "解壓縮 `twitter_sentiment.zip` 後可以發現裡面有三個csv檔。\n",
    "\n",
    "- `train.csv`, `test.csv` and `val.csv`\n",
    "\n",
    "Training set 有 **10248** 筆資料.  \n",
    "Validation set 有 **1317** 筆資料.  \n",
    "Testing set 有 **3075** 筆資料.  \n",
    "\n",
    "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OSlTMdxf8Zd7",
    "ExecuteTime": {
     "end_time": "2023-12-13T17:33:46.723436200Z",
     "start_time": "2023-12-13T17:33:46.699084800Z"
    }
   },
   "outputs": [],
   "source": [
    "# !unzip -qq ./drive/MyDrive/twitter_sentiment.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wf5GXTme7rIT",
    "ExecuteTime": {
     "end_time": "2023-12-13T17:33:46.723436200Z",
     "start_time": "2023-12-13T17:33:46.711859Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to extract text and label from csv file\n",
    "def get_texts(f_name='./twitter_sentiment', mode='train'):\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "\n",
    "    f_path = os.path.join(f_name, '{}.csv'.format(mode))\n",
    "    with open(f_path, encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for line in reader:\n",
    "            text_list.append(line['text'])\n",
    "            if mode != 'test':\n",
    "                label_list.append(int(line['sentiment_label']))\n",
    "\n",
    "    return text_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6fpY0ZrK7rIV",
    "ExecuteTime": {
     "end_time": "2023-12-13T17:33:46.737227100Z",
     "start_time": "2023-12-13T17:33:46.723436200Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, f_name='./twitter_sentiment', mode='train'):\n",
    "        self.mode = mode\n",
    "\n",
    "        text_list, label_list = get_texts(f_name, mode)\n",
    "        print('mode', mode, 'has', len(text_list), 'datas')\n",
    "        text_list = tokenizer(text_list,\n",
    "                             truncation=True, padding=True,\n",
    "                             return_tensors='pt')\n",
    "\n",
    "        self.text_list = text_list['input_ids']\n",
    "        self.mask_list = text_list['attention_mask']\n",
    "\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_list[idx]\n",
    "        mask = self.mask_list[idx]\n",
    "        if self.mode == 'test':\n",
    "            return text, mask\n",
    "        label = torch.tensor(self.label_list[idx])\n",
    "        return text, mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nCmM4FSw7rIW",
    "ExecuteTime": {
     "end_time": "2023-12-13T17:33:47.630688100Z",
     "start_time": "2023-12-13T17:33:46.728227800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode train has 10248 datas\n",
      "mode val has 1317 datas\n",
      "mode test has 3075 datas\n"
     ]
    }
   ],
   "source": [
    "dataset_train = TwitterDataset(mode='train')\n",
    "dataset_val = TwitterDataset(mode='val')\n",
    "dataset_test = TwitterDataset(mode='test')\n",
    "\n",
    "batch_size = 32\n",
    "train_data = DataLoader(dataset_train, batch_size=batch_size,\n",
    "                       shuffle=True)\n",
    "val_data = DataLoader(dataset_val, batch_size=batch_size // 2,\n",
    "                       shuffle=False)\n",
    "test_data = DataLoader(dataset_test, batch_size=batch_size // 2,\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bqkvofHc7rIY",
    "ExecuteTime": {
     "end_time": "2023-12-13T17:33:47.638829Z",
     "start_time": "2023-12-13T17:33:47.630688100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['<s>', '@', 'united', 'ĠI', 'Ġhave', 'Ġnever', 'Ġbeen', 'Ġmislead', 'Ġby', 'Ġa', 'Ġcompany', 'Ġas', 'Ġmany', 'Ġtimes', 'Ġas', 'ĠI', 'Ġhave', 'Ġthis', 'Ġweek', 'Ġby', 'ĠUnited', 'ĠAirlines', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "token to s <s>@united I have never been mislead by a company as many times as I have this week by United Airlines!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(dataset_train[0][0])\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DxZrfCqW7rIY",
    "ExecuteTime": {
     "end_time": "2023-12-13T17:33:47.916056800Z",
     "start_time": "2023-12-13T17:33:47.639837400Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "# 定義標籤平滑化的KL損失函數 Paper: https://arxiv.org/pdf/2312.06522.pdf\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = -1\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n",
    "# 設定標籤平滑化水平\n",
    "\"\"\"\n",
    "LS2: smoothing=0.03（即3%平滑化）\n",
    "LS3: smoothing=0.075（即7.5%平滑化）\n",
    "LS4: smoothing=0.15（即15%平滑化）\n",
    "LS5: smoothing=0.3（即30%平滑化）\n",
    "\"\"\"\n",
    "criterion = LabelSmoothingLoss(classes=3, smoothing=0.03)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpwgE2Gd7rIZ"
   },
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zlaiAZAD7rIa",
    "ExecuteTime": {
     "end_time": "2023-12-13T17:33:47.919363400Z",
     "start_time": "2023-12-13T17:33:47.917056100Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(raw_preds, y):\n",
    "    preds = raw_preds.argmax(dim=1)\n",
    "    acc = (preds == y).sum()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dmc_Gms97rIa",
    "ExecuteTime": {
     "end_time": "2023-12-13T17:33:48.365785Z",
     "start_time": "2023-12-13T17:33:47.924362200Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Testing process                              #\n",
    "        #########################################################################\n",
    "        # 1. Clean the gradients of optimizer\n",
    "        # 2. Put correct variables into model\n",
    "        # 3. Get prediction\n",
    "        # 4. Evalutate by criterion and accuracy\n",
    "\n",
    "        # Clean the gradients of optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(text, attention_mask=mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.logits, label)\n",
    "\n",
    "        # Compute accuracy using the provided function\n",
    "        acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "def test(model, data, criterion, log_loss=False):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Training process                             #\n",
    "        #########################################################################\n",
    "        # 1. Put correct variables into model\n",
    "        # 2. Get prediction\n",
    "        # 3. Evalutate by criterion and accuracy\n",
    "\n",
    "        # No gradient calculation for evaluation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(text, attention_mask=mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.logits, label)\n",
    "\n",
    "            # Compute accuracy\n",
    "            acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if log_loss:\n",
    "            val_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "# class for monitoring train and test acc/loss\n",
    "class Meter:\n",
    "    def __init__(self):\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def update(self, train_loss, train_acc, val_loss, val_acc):\n",
    "        self.train_loss_list.append(train_loss)\n",
    "        self.train_acc_list.append(train_acc)\n",
    "        self.val_loss_list.append(val_loss)\n",
    "        self.val_acc_list.append(val_acc)\n",
    "\n",
    "    def plot(self):\n",
    "        x = range(len(self.train_loss_list))\n",
    "        plt.plot(x, self.train_loss_list)\n",
    "        plt.plot(x, self.val_loss_list, color='r')\n",
    "        plt.legend(['train_loss', 'val_loss'])\n",
    "        plt.show()\n",
    "        plt.plot(x, self.train_acc_list)\n",
    "        plt.plot(x, self.val_acc_list, color='r')\n",
    "        plt.legend(['train_acc', 'val_acc'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExZyrKd57rIb"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bVDe-fRe7rIc",
    "ExecuteTime": {
     "end_time": "2023-12-13T18:00:21.363830900Z",
     "start_time": "2023-12-13T17:33:48.367785700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:32<00:00,  2.11it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 28.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 0.01861684309910462 train_acc: 0.7799570647931303\n",
      "Epoch 1 val_loss:  0.032101274588928135 val_acc : 0.8367501898253606\n",
      "---------- e 1 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:31<00:00,  2.12it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 27.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss: 0.013802299760194797 train_acc: 0.8677790788446527\n",
      "Epoch 2 val_loss:  0.029805403839211257 val_acc : 0.8572513287775246\n",
      "---------- e 2 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:34<00:00,  2.08it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 28.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss: 0.011809611547619081 train_acc: 0.9017369242779079\n",
      "Epoch 3 val_loss:  0.03154696460782169 val_acc : 0.8466211085801063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:32<00:00,  2.10it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 27.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss: 0.010066343494672779 train_acc: 0.9296448087431693\n",
      "Epoch 4 val_loss:  0.03180278210859002 val_acc : 0.8481397114654518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:36<00:00,  2.05it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 27.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss: 0.008399165294349054 train_acc: 0.9547228727556596\n",
      "Epoch 5 val_loss:  0.03670269611801446 val_acc : 0.8291571753986332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:37<00:00,  2.04it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 28.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train_loss: 0.007865937554643175 train_acc: 0.961455893832943\n",
      "Epoch 6 val_loss:  0.034040060315787476 val_acc : 0.8648443432042521\n",
      "---------- e 6 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:36<00:00,  2.06it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 26.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train_loss: 0.007421923081545417 train_acc: 0.9680913348946136\n",
      "Epoch 7 val_loss:  0.039200779177876434 val_acc : 0.8428246013667426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:36<00:00,  2.06it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 28.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train_loss: 0.007141375909853875 train_acc: 0.9708235753317721\n",
      "Epoch 8 val_loss:  0.03830734444053769 val_acc : 0.8511769172361427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:37<00:00,  2.04it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 27.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train_loss: 0.006544157614019772 train_acc: 0.9800936768149883\n",
      "Epoch 9 val_loss:  0.042068776379685195 val_acc : 0.8390280941533789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:31<00:00,  2.11it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 27.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train_loss: 0.006425146074568639 train_acc: 0.9812646370023419\n",
      "Epoch 10 val_loss:  0.0370557003271969 val_acc : 0.856492027334852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#                          Hyper-parameters                             #\n",
    "#########################################################################\n",
    "max_epoch = 10\n",
    "log_interval = 1\n",
    "best_acc = 0\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################\n",
    "\n",
    "m = Meter()\n",
    "\n",
    "for epoch in range(1, max_epoch + 1):\n",
    "    train_loss, train_acc = train(model, train_data, optimizer, criterion)\n",
    "    val_loss, val_acc = test(model, val_data, criterion, log_loss=True)\n",
    "\n",
    "    if epoch % log_interval == 0:\n",
    "        print('Epoch {} train_loss: {} train_acc: {}'.format(\n",
    "            epoch, train_loss, train_acc\n",
    "        ))\n",
    "        print('Epoch {} val_loss:  {} val_acc : {}'.format(\n",
    "            epoch, val_loss, val_acc\n",
    "        ))\n",
    "\n",
    "    m.update(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "    # model checkpoint\n",
    "    if val_acc > best_acc:\n",
    "        best_model = model\n",
    "        best_acc = val_acc\n",
    "        print('-'*10, 'e', epoch, 'save best model', '-'*10)\n",
    "        torch.save(model.state_dict(), 'ckpts/e{}.pt'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SmtW58OR7rIc",
    "ExecuteTime": {
     "end_time": "2023-12-13T18:01:08.096959100Z",
     "start_time": "2023-12-13T18:01:07.864021800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot them out\n",
    "m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcJcHf7n7rId"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/f072e95f51bc48978225941dba218241).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.load_state_dict(torch.load('ckpts/e2.pt'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:03:00.534919100Z",
     "start_time": "2023-12-13T18:02:59.272288100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Sf5UTlMZ7rId",
    "ExecuteTime": {
     "end_time": "2023-12-13T18:03:07.680281600Z",
     "start_time": "2023-12-13T18:03:01.321152400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:06<00:00, 31.05it/s]\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "\n",
    "total_out = []\n",
    "for text, mask in tqdm(test_data, total=len(test_data)):\n",
    "    text = text.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    output = best_model(text, mask)\n",
    "    pred = output.logits\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    total_out.append(pred)\n",
    "\n",
    "total_out = torch.cat(total_out).cpu().numpy().tolist()\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(total_out):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Save best model\n",
    "torch.save(best_model.state_dict(), 'ckpts/best_model.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T18:03:51.107060Z",
     "start_time": "2023-12-13T18:03:49.503594900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: In-Context learning (32 points)\n",
    "\n",
    "In this task, you will learn how to perform sentiment classification using prompts without the need for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T17:20:33.671371500Z",
     "start_time": "2023-12-13T17:20:25.621846200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyprind\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertTokenizer, BertForMaskedLM\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T17:20:40.662163100Z",
     "start_time": "2023-12-13T17:20:33.674374400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#         TODO: Design your own template(prefix) and verbalizer         #\n",
    "#########################################################################\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # you can modify this line\n",
    "        self.prefix = 'This sentence is [MASK].'  # you can modify this line\n",
    "\n",
    "        self.verbalizer = {\n",
    "            'negative': 0,\n",
    "            'neutral': 1,\n",
    "            'positive': 2,\n",
    "        }\n",
    "\n",
    "        self.max_seq_length = 512\n",
    "        self.batch_size = 32\n",
    "\n",
    "\n",
    "config = Config()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bert_type = 'bert-base-uncased'\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(bert_type, num_labels=3)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(bert_type)\n",
    "\n",
    "bert = model.from_pretrained(bert_type, config=bert_config).to(device)\n",
    "\n",
    "#######################################################################\n",
    "#                        End of your code                             #\n",
    "#######################################################################\n",
    "\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T17:20:40.665168900Z",
     "start_time": "2023-12-13T17:20:40.663171100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to obtaion verbalizer ids\n",
    "def obtain_verbalizer_ids(verbalizer, tokenizer):\n",
    "    verbalizer_ids = tokenizer.convert_tokens_to_ids(list(verbalizer.keys()))\n",
    "    index2ids = {i: verbalizer_ids[i] for i in range(len(verbalizer_ids))}\n",
    "    return verbalizer_ids, index2ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T17:20:40.675722Z",
     "start_time": "2023-12-13T17:20:40.666168800Z"
    }
   },
   "outputs": [],
   "source": [
    "verbalizer_ids, index2ids = obtain_verbalizer_ids(config.verbalizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T17:20:40.682600200Z",
     "start_time": "2023-12-13T17:20:40.676730800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to concatenate prefix and text\n",
    "def concatenate_prefix(texts, config):\n",
    "    ##################################################\n",
    "    #   TODO: concatenate your own prefix and text   #                               \n",
    "    ##################################################\n",
    "    prefix_texts = [config.prefix + \" \" + text for text in texts]\n",
    "    ##################################################\n",
    "    #                 End of your code               #                               \n",
    "    ##################################################\n",
    "    return prefix_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T17:20:40.716410800Z",
     "start_time": "2023-12-13T17:20:40.683585400Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(config):\n",
    "    # ['texts', 'labels']\n",
    "    df = pd.read_csv('./twitter_sentiment/train.csv')\n",
    "    original_texts = df['text'].tolist()\n",
    "    labels = df['sentiment_label'].tolist()\n",
    "\n",
    "    texts = concatenate_prefix(original_texts, config)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "texts, labels = load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['[CLS]', 'this', 'sentence', 'is', '[MASK]', '.', '@', 'united', 'i', 'have', 'never', 'been', 'mis', '##lea', '##d', 'by', 'a', 'company', 'as', 'many', 'times', 'as', 'i', 'have', 'this', 'week', 'by', 'united', 'airlines', '!', '[SEP]']\n",
      "token to s [CLS] this sentence is [MASK] . @ united i have never been mislead by a company as many times as i have this week by united airlines ! [SEP]\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(tokenizer.encode(texts[0], add_special_tokens=True))\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T17:20:40.723614Z",
     "start_time": "2023-12-13T17:20:40.717407300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T17:20:40.731481300Z",
     "start_time": "2023-12-13T17:20:40.724616700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Batching of texts and labels for training or processing in batches\n",
    "def pack_batch(texts, labels, batch_size):\n",
    "    \"\"\"\n",
    "    :param texts: list\n",
    "    :param labels: list\n",
    "    :param batch_size: int\n",
    "    :return batch_X: list\n",
    "            [[text11, text12, ...], [text21, text22, ...], ...]\n",
    "    :return batch_y: list\n",
    "            [[label11, label12, ...], [label21, label22, ...], ...]\n",
    "    :return batch_count: int\n",
    "    \"\"\"\n",
    "    assert len(texts) == len(labels)\n",
    "\n",
    "    if len(texts) % batch_size != 0:\n",
    "        flag = False\n",
    "        batch_count = int(len(texts) / batch_size) + 1\n",
    "    else:\n",
    "        flag = True\n",
    "        batch_count = int(len(texts) / batch_size)\n",
    "\n",
    "    batch_X, batch_y = [], []\n",
    "\n",
    "    if flag:\n",
    "        for i in range(batch_count):\n",
    "            batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "            batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "    else:\n",
    "        for i in range(batch_count):\n",
    "            if i == batch_count - 1:\n",
    "                batch_X.append(texts[i * batch_size:])\n",
    "                batch_y.append(labels[i * batch_size:])\n",
    "            else:\n",
    "                batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "                batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "\n",
    "    return batch_X, batch_y, batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T17:20:40.740601900Z",
     "start_time": "2023-12-13T17:20:40.732483Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_X, batch_y, batch_count = pack_batch(texts, labels, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-13T17:25:48.427689300Z",
     "start_time": "2023-12-13T17:20:40.744602Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[100 %] Time elapsed: 00:05:07 | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.669399 | precision: 0.533643 | recall: 0.669399 | f1: 0.593294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:05:07\n",
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    pper = pyprind.ProgPercent(batch_count)\n",
    "    for i in range(batch_count):\n",
    "        inputs = batch_X[i]\n",
    "        labels = batch_y[i]\n",
    "\n",
    "        tokens = tokenizer.batch_encode_plus(inputs, add_special_tokens=True,\n",
    "                                             max_length=config.max_seq_length,\n",
    "                                             padding='max_length', truncation=True)\n",
    "        \n",
    "        ids = torch.tensor(tokens['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(tokens['attention_mask']).to(device)\n",
    "\n",
    "        # Shape: (batch_size, max_seq_length, vocab_size)\n",
    "        logits = bert(ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        mask_token_index = (ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "\n",
    "        # Find [MASK] logits\n",
    "        # shape: (batch_size, vocab_size)\n",
    "        masked_logits = logits[mask_token_index[0], mask_token_index[1], :]\n",
    "\n",
    "        # Extract the logits of the word in the verbalizer at the [MASK] position\n",
    "        # shape: (batch_size, verbalizer_size)\n",
    "        verbalizer_logits = masked_logits[:, verbalizer_ids]\n",
    "\n",
    "        # Construct a pseudo-distribution from the logits in these verbalizers\n",
    "        pseudo_distribution = softmax(verbalizer_logits)\n",
    "\n",
    "        #################################################################################\n",
    "        #   1. Find the index with the maximum probability in the pseudo-distribution   #\n",
    "        #   2. Convert the index to the corresponding word ID                           #\n",
    "        #   3. Convert the ID to a token                                                #\n",
    "        #   4. Find the label corresponding to the token                                #\n",
    "        #################################################################################\n",
    "        # 1. Find the index with the maximum probability in the pseudo-distribution\n",
    "        pred_indices = torch.argmax(pseudo_distribution, dim=1)\n",
    "\n",
    "        # 2. Convert the index to the corresponding word ID\n",
    "        pred_ids = [verbalizer_ids[index] for index in pred_indices]\n",
    "\n",
    "        # 3. Convert the ID to a token\n",
    "        pred_tokens = tokenizer.convert_ids_to_tokens(pred_ids)\n",
    "\n",
    "        # 4. Find the label corresponding to the token\n",
    "        pred_labels = [config.verbalizer[token] if token in config.verbalizer else -1 for token in pred_tokens]\n",
    "        pred_labels = np.array(pred_labels)\n",
    "        #################################################################################\n",
    "        #                             End of your code                                  #\n",
    "        #################################################################################\n",
    "\n",
    "        predict_all = np.append(predict_all, pred_labels)\n",
    "        labels_all = np.append(labels_all, labels)\n",
    "\n",
    "        pper.update()\n",
    "\n",
    "    acc = accuracy_score(labels_all, predict_all)\n",
    "    p = precision_score(labels_all, predict_all, average=\"weighted\")\n",
    "    r = recall_score(labels_all, predict_all, average=\"weighted\")\n",
    "    f1 = f1_score(labels_all, predict_all, average=\"weighted\")\n",
    "\n",
    "    print('accuracy: %f | precision: %f | recall: %f | f1: %f' % (acc, p, r, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "       0  1\n0      0  0\n1      0  0\n2      2  2\n3      1  0\n4      0  0\n...   .. ..\n10243  0  0\n10244  0  0\n10245  0  0\n10246  2  2\n10247  1  0\n\n[10248 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10243</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10244</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10245</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10246</th>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10247</th>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10248 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([labels_all, predict_all]).T"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T17:25:48.681832600Z",
     "start_time": "2023-12-13T17:25:48.428696300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: LM-BFF (45 points)\n",
    "\n",
    "https://arxiv.org/pdf/2012.15723.pdf\n",
    "\n",
    "Unlike the previous task, LM-BFF can generate templates and verbalizers automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請先到共用雲端硬碟將檔案 `SST-2.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/14MDYFasXU94dUE9DjgfcZE61iTRI2007/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install openprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T03:31:13.543193800Z",
     "start_time": "2023-12-09T03:30:21.808076200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openprompt\n",
      "  Downloading openprompt-1.0.1-py3-none-any.whl (146 kB)\n",
      "     ---------------------------------------- 0.0/146.4 kB ? eta -:--:--\n",
      "     ---------------- ---------------------- 61.4/146.4 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 146.4/146.4 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting transformers>=4.10.0 (from openprompt)\n",
      "  Obtaining dependency information for transformers>=4.10.0 from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 123.5/123.5 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting sentencepiece==0.1.96 (from openprompt)\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 0.1/1.1 MB 2.2 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.2/1.1 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.3/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.4/1.1 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.5/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 0.6/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.6/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 0.7/1.1 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 0.8/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 0.9/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.0/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.1/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.1/1.1 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting tqdm>=4.62.2 (from openprompt)\n",
      "  Obtaining dependency information for tqdm>=4.62.2 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting tensorboardX (from openprompt)\n",
      "  Obtaining dependency information for tensorboardX from https://files.pythonhosted.org/packages/44/71/f3e7c9b2ab67e28c572ab4e9d5fa3499e0d252650f96d8a3a03e26677f53/tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nltk (from openprompt)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting yacs (from openprompt)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting dill (from openprompt)\n",
      "  Obtaining dependency information for dill from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting datasets (from openprompt)\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting rouge==1.0.0 (from openprompt)\n",
      "  Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting pyarrow (from openprompt)\n",
      "  Obtaining dependency information for pyarrow from https://files.pythonhosted.org/packages/28/82/9adfafaf0de581a39a1c86002cafd1a55a1255c9e0d362dc3e970aab0656/pyarrow-14.0.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading pyarrow-14.0.1-cp39-cp39-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openprompt) (1.8.0)\n",
      "Requirement already satisfied: six in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rouge==1.0.0->openprompt) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\eddie\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.62.2->openprompt) (0.4.4)\n",
      "Collecting filelock (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/fc/85/0d1038f068900896a8590d6d0da198b90d31f731a39166a432aa2b92249b/regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (2.25.1)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/9f/90/a6821e7757d2db194c16cbca78c80e206f30f6cc62c7f15fb27428f8c6dd/tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/4e/96/f4ee4434d8b6452fe7d5d44df2e72d1c6b2add1c3a5fb5c81aae83cb90c6/safetensors-0.4.1-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.1-cp39-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->openprompt)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets->openprompt) (1.4.1)\n",
      "Collecting xxhash (from datasets->openprompt)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/33/db/e07aa80e39a7ee82e9ca6f5a0fa9bf0b4465934272116a1b578eaee7f4b3/xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->openprompt)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/c6/c9/820b5ab056f4ada76fbe05bd481a948f287957d6cbfd59e2dd2618b408c1/multiprocess-0.70.15-py39-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets->openprompt)\n",
      "  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets->openprompt) (3.7.4.post0)\n",
      "Requirement already satisfied: click in c:\\users\\eddie\\appdata\\roaming\\python\\python39\\site-packages (from nltk->openprompt) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->openprompt) (1.1.0)\n",
      "Collecting protobuf>=3.20 (from tensorboardX->openprompt)\n",
      "  Obtaining dependency information for protobuf>=3.20 from https://files.pythonhosted.org/packages/b6/4b/f4f3334784576822d7817a664b757030ebb35b981978baf9c2eb3c5f33a8/protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (21.2.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (3.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (5.2.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (1.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers>=4.10.0->openprompt) (3.0.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets->openprompt) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets->openprompt) (2021.3)\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 41.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/7.9 MB 3.2 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.2/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.4/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.4/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.6/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.7/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.8/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.9/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.0/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.1/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.2/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.3/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.4/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.5/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.6/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.7/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.8/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.9/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.2/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.3/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.4/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.5/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.6/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.7/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.8/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.9/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.2/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.3/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.4/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.6/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.7/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.9/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.0/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.1/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.2/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.3/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.5/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.6/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.9/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.0/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.1/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.2/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.3/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.5/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.6/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.7/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.9/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.0/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.1/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.3/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.4/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.5/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.6/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.7/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.8/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.9/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.0/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.3/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.4/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.5/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.6/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.7/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.8/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "   ---------------------------------------- 0.0/521.2 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 92.2/521.2 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 174.1/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 276.5/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 368.6/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 471.0/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 521.2/521.2 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 61.4/115.3 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 115.3/115.3 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow-14.0.1-cp39-cp39-win_amd64.whl (24.6 MB)\n",
      "   ---------------------------------------- 0.0/24.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/24.6 MB 3.2 MB/s eta 0:00:08\n",
      "   ---------------------------------------- 0.2/24.6 MB 2.6 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 0.3/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.4/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.5/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.6/24.6 MB 2.2 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.7/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.8/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.8/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.9/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.0/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.1/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.2/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.3/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.4/24.6 MB 1.9 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.4/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.7/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.8/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.9/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.0/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.1/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.2/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.3/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.4/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.5/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.6/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.7/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.8/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.9/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 3.0/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.2/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.3/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.4/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.5/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.6/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.7/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.8/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.9/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.0/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.2/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.3/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.4/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.5/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.6/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.8/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.9/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.0/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.1/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.2/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.3/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.4/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.5/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.6/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.8/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.9/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.0/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.1/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.2/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.3/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.4/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.5/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.6/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.7/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 6.8/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 6.9/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.0/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.1/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.2/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.3/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.5/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.6/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.7/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.8/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.9/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.0/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.1/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.2/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.3/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.5/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 8.6/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.7/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.8/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.3/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.4/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.5/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.6/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.7/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.8/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 9.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.0/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.3/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.4/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 10.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.6/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.7/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.8/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.9/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 11.0/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.1/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.2/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.4/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.6/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.7/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 11.8/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 11.9/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.0/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.1/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.2/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.7/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.8/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.0/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.1/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.3/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.7/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.8/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.1/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.3/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 14.8/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 14.9/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.0/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.1/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.2/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.3/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.4/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.5/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.8/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.9/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.0/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.1/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.2/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.3/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.4/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.5/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 16.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 16.8/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 16.9/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.0/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.1/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.3/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.4/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.5/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.6/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.7/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.8/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 17.9/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.0/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.1/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.3/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.4/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.5/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.6/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.7/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.7/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.8/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 19.0/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.1/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.2/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.3/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.4/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.5/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.7/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.8/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.0/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.1/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.2/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.3/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.4/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.5/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.7/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.9/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.0/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.1/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.2/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.3/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.4/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.5/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.9/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.0/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.1/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.2/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.3/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.4/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.5/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.7/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.8/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.2/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.3/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.4/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.5/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.6/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.7/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.8/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.2/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.5/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.6/24.6 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.7 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 92.2/101.7 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 101.7/101.7 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 0.0/311.7 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 112.6/311.7 kB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 194.6/311.7 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  307.2/311.7 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 311.7/311.7 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 112.6/413.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 194.6/413.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 307.2/413.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 368.6/413.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 413.4/413.4 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 92.2/269.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 174.1/269.6 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/269.6 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 269.6/269.6 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp39-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.8 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 112.6/277.8 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 194.6/277.8 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 277.8/277.8 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.2 MB 1.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 1.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.4/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.3 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 92.2/133.3 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 133.3/133.3 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl (29 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 71.7/166.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 166.4/166.4 kB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, yacs, xxhash, tqdm, safetensors, rouge, regex, pyarrow-hotfix, pyarrow, protobuf, fsspec, filelock, dill, tensorboardX, nltk, multiprocess, huggingface-hub, tokenizers, transformers, datasets, openprompt\n",
      "Successfully installed datasets-2.15.0 dill-0.3.7 filelock-3.13.1 fsspec-2023.10.0 huggingface-hub-0.19.4 multiprocess-0.70.15 nltk-3.8.1 openprompt-1.0.1 protobuf-4.25.1 pyarrow-14.0.1 pyarrow-hotfix-0.6 regex-2023.10.3 rouge-1.0.0 safetensors-0.4.1 sentencepiece-0.1.96 tensorboardX-2.6.2.2 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.35.2 xxhash-3.4.1 yacs-0.1.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install openprompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import openprompt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.181341Z",
     "start_time": "2023-12-10T03:35:52.654085900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.194342100Z",
     "start_time": "2023-12-10T03:36:10.182341900Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda = True\n",
    "auto_t = True # Whether to perform automatic template generation\n",
    "auto_v = True # Whether to perform automatic verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.226340200Z",
     "start_time": "2023-12-10T03:36:10.195343500Z"
    }
   },
   "outputs": [],
   "source": [
    "from openprompt.data_utils.text_classification_dataset import SST2Processor\n",
    "dataset = {'train': SST2Processor().get_train_examples(\"SST-2/\"),\n",
    "           'validation': SST2Processor().get_dev_examples(\"SST-2/\"),\n",
    "           'test': SST2Processor().get_test_examples(\"SST-2/\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.607530300Z",
     "start_time": "2023-12-10T03:36:10.228340900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: {\n",
      "  \"guid\": \"train-0\",\n",
      "  \"label\": 0,\n",
      "  \"meta\": {\n",
      "    \"labelword\": \"terrible\"\n",
      "  },\n",
      "  \"text_a\": \"nothing happens , and it happens to flat characters .\",\n",
      "  \"text_b\": \"\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print('load model...')\n",
    "from openprompt.plms import load_plm\n",
    "# load mlm model for main tasks\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")\n",
    "\n",
    "# load generation model for template generation\n",
    "template_generate_model, template_generate_tokenizer, template_generate_model_config, template_tokenizer_wrapper = load_plm('t5', 't5-large')\n",
    "\n",
    "from openprompt.prompts import ManualVerbalizer, ManualTemplate\n",
    "verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "#   TODO: You need to switch LMBFFTemplateGenerationTemplate or ManualTemplate to                                 #\n",
    "#         compare auto generate template and manual generate template                                             #\n",
    "###################################################################################################################\n",
    "from openprompt.prompts.prompt_generator import LMBFFTemplateGenerationTemplate\n",
    "########################################\n",
    "#   LMBFFTemplateGenerationTemplate    #\n",
    "########################################\n",
    "import random\n",
    "\n",
    "# number of demonstrations\n",
    "num_demonstrations = 1  # try different number\n",
    "\n",
    "demonstrations = []\n",
    "\n",
    "for _ in range(num_demonstrations):\n",
    "    # random choice training set example with label 0 \n",
    "    random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n",
    "\n",
    "    # random choice training set example with label 1\n",
    "    random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n",
    "    \n",
    "    demonstration = f'{random_example_1.text_a} It was terrible. {random_example_2.text_a} It was great.'\n",
    "    demonstrations.append(demonstration)\n",
    "\n",
    "# You can modify the demonstrations and try different combinations\n",
    "template_text = '{\"placeholder\": \"text_a\"} {\"mask\"} {\"meta\": \"labelword\"} {\"mask\"}.' + ' '.join(demonstrations)\n",
    "template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "\n",
    "#############################################\n",
    "#   End of LMBFFTemplateGenerationTemplate  #\n",
    "#############################################\n",
    "\n",
    "########################################\n",
    "#          ManualTemplate              #\n",
    "########################################\n",
    "\n",
    "#template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} It was {\"mask\"}.')\n",
    "\n",
    "#############################################\n",
    "#          End of ManualTemplate            # \n",
    "#############################################\n",
    "\n",
    "###################################################################################################################\n",
    "#                                           End of your code                                                      #\n",
    "###################################################################################################################\n",
    "\n",
    "\n",
    "# view wrapped example\n",
    "wrapped_example = template.wrap_one_example(dataset['train'][0])\n",
    "print(\"dataset:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.673523500Z",
     "start_time": "2023-12-10T03:36:46.625523700Z"
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "\n",
    "# Returns the best evaluation score achieved during training\n",
    "def fit(model, train_dataloader, val_dataloader, loss_func, optimizer, epochs=5):\n",
    "    best_score = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_dataloader, loss_func, optimizer)\n",
    "        score = evaluate(model, val_dataloader)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "        print(f\"Epoch {epoch+1}: Train loss={train_loss}, Eval score={score}\")\n",
    "    return best_score\n",
    "\n",
    "# Trains the model on the training data and computes the training loss\n",
    "def train_epoch(model, train_dataloader, loss_func, optimizer):\n",
    "    model.train()\n",
    "    loss_all = []\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits #\n",
    "        # 2. Get labels                                     #\n",
    "        # 3. Evalutate using loss_func                      #\n",
    "        # 4. Append loss to loss_all                        #\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits\n",
    "        logits = model(batch=inputs)\n",
    "\n",
    "        # 2. Get labels\n",
    "        labels = inputs['label']\n",
    "\n",
    "        # 3. Evalutate using loss_func\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Append loss to loss_all\n",
    "        loss_all.append(loss.item())\n",
    "        #####################################################\n",
    "        #                 End of your code                  #\n",
    "        #####################################################\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return np.mean(loss_all)\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    with torch.no_grad():\n",
    "        for step, inputs in enumerate(val_dataloader):\n",
    "            if cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits #\n",
    "            # 2. Get labels                                     #\n",
    "            # 3. Extend labels to list                          #\n",
    "            # 4. Get predictions and extend preds to list       #\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits\n",
    "            logits = model(batch=inputs)\n",
    "\n",
    "            # 2. Get labels\n",
    "            labels = inputs['label']\n",
    "\n",
    "            # 3. Extend labels to list\n",
    "            alllabels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # 4. Get predictions and extend preds to list\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            allpreds.extend(preds.cpu().numpy())\n",
    "            #####################################################\n",
    "            #                 End of your code                  #\n",
    "            #####################################################\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generated template from TemplateGenerator and find the best template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:13:36.594464200Z",
     "start_time": "2023-12-10T03:36:46.673523500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 1454.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:37<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"placeholder\": \"text_a\"} It was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} it was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001B[A\n",
      "tokenizing: 32it [00:00, 202.53it/s]A\n",
      "\n",
      "tokenizing: 32it [00:00, 727.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.562330920593297, Eval score=0.5\n",
      "Epoch 2: Train loss=1.029085214715451, Eval score=0.5\n",
      "Epoch 3: Train loss=0.671642615867313, Eval score=0.71875\n",
      "Epoch 4: Train loss=0.2566610455396585, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [15:59<1:03:57, 959.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.2469192902863142, Eval score=0.78125\n",
      "Current template: {\"placeholder\": \"text_a\"} it was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' it was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 379.37it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.511358927668084, Eval score=0.5\n",
      "Epoch 2: Train loss=0.7635227439459413, Eval score=0.5\n",
      "Epoch 3: Train loss=0.3154368309772053, Eval score=0.53125\n",
      "Epoch 4: Train loss=0.8977778584977472, Eval score=0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [36:43<56:21, 1127.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.7606429383413342, Eval score=0.5\n",
      "Current template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 571.42it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1180.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.765889931773188, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.5693292848736746, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.041312409681268036, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.2322409824218994, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [56:36<38:33, 1156.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.22881872234574985, Eval score=0.84375\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 1066.65it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1523.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.7746381501195, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9883591458201408, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8319057337939739, Eval score=0.625\n",
      "Epoch 4: Train loss=0.508084288907412, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [1:16:21<19:27, 1167.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.39746711112820776, Eval score=0.53125\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 864.68it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1454.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.6042319842349002, Eval score=0.5\n",
      "Epoch 2: Train loss=1.069442194304429, Eval score=0.5\n",
      "Epoch 3: Train loss=0.47684725234284997, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.2768472472046142, Eval score=0.59375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:36:08<00:00, 1153.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.07557142606344769, Eval score=0.625\n",
      "Final best template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class ManualTemplateWithoutParse(ManualTemplate):\n",
    "    \"\"\"The generated template from TemplateGenerator is a list of dict of parsed template_text. So no further parsing is needed.\"\"\"\n",
    "    def on_text_set(self):\n",
    "        pass\n",
    "\n",
    "# Template generation\n",
    "if auto_t:\n",
    "    print('performing auto_t...')\n",
    "\n",
    "    if cuda:\n",
    "        template_generate_model = template_generate_model.cuda()\n",
    "    template_generator = T5TemplateGenerator(template_generate_model, template_generate_tokenizer, template_tokenizer_wrapper, verbalizer, beam_width=5) # Beam_width is set to 5 here for efficiency; to improve performance, try a larger number.\n",
    "\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=template_generate_tokenizer, tokenizer_wrapper_class=template_tokenizer_wrapper, batch_size=len(dataset['train']), decoder_max_length=128, max_seq_length=128, shuffle=False, teacher_forcing=False) # Register all data at once\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        template_generator._register_buffer(data)\n",
    "\n",
    "    template_generate_model.eval()\n",
    "    print('generating...')\n",
    "    template_texts = template_generator._get_templates()\n",
    "\n",
    "    original_template = template.text\n",
    "    template_texts = [template_generator.convert_template(template_text, original_template) for template_text in template_texts]\n",
    "    # template_generator._show_template()\n",
    "    template_generator.release_memory()\n",
    "    # Generate a number of candidate template text\n",
    "    print(template_texts)\n",
    "    \n",
    "    # Iterate over each candidate and select the best one\n",
    "    best_metrics = 0.0\n",
    "    best_template_text = None\n",
    "    for template_text in tqdm(template_texts):\n",
    "        verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "        template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "        print(f\"Current template: {template_text}\\n\\nWrapped example: {template.wrap_one_example(dataset['train'][0])}\")\n",
    "\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "        \n",
    "        #######################################################\n",
    "        # TODO: Use score to Find your best template_text     #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_template_text = template_text\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # Use the best template\n",
    "    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "    template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=best_template_text)\n",
    "    print(\"Final best template:\", best_template_text)\n",
    "    print()\n",
    "    print(\"Wrapped example:\", template.wrap_one_example(dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verbalizer template from VerbalizerGenerator and find the best verbalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T11:58:04.838076700Z",
     "start_time": "2023-12-10T05:13:36.638466100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_v...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 1391.30it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current label_words: ['terrible', 'beautiful']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001B[A\n",
      "tokenizing: 3it [00:00, 21.48it/s]\u001B[A\n",
      "tokenizing: 6it [00:01,  5.15it/s]\u001B[A\n",
      "tokenizing: 8it [00:01,  6.27it/s]\u001B[A\n",
      "tokenizing: 32it [00:01, 23.45it/s]\u001B[A\n",
      "\n",
      "tokenizing: 32it [00:00, 969.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.1593105591260544, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.5407680265489034, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.17979280870349612, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.006995583800744498, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [21:05<6:40:40, 1265.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.002051841642924046, Eval score=0.84375\n",
      "current label_words: ['horrible', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 695.63it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1388.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.5843039118335565, Eval score=0.84375\n",
      "Epoch 2: Train loss=0.0798521326687478, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.0026221817748819376, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.0010607897513068565, Eval score=0.78125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [42:02<6:18:07, 1260.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0004625524882726495, Eval score=0.78125\n",
      "current label_words: ['horrible', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 762.07it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1333.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.0936600251085586, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.849827597849071, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.414547988853883, Eval score=0.75\n",
      "Epoch 4: Train loss=0.0500250239797424, Eval score=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [1:02:46<5:55:04, 1253.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.004471211226245941, Eval score=0.8125\n",
      "current label_words: ['sad', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 415.60it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 941.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4185917818103917, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.5065413688316767, Eval score=0.875\n",
      "Epoch 3: Train loss=0.012464412650388113, Eval score=0.875\n",
      "Epoch 4: Train loss=0.002708091426967485, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [1:23:35<5:33:44, 1251.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0004699288858773798, Eval score=0.875\n",
      "current label_words: ['bad', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 551.73it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=3.1230944280390602, Eval score=0.5\n",
      "Epoch 2: Train loss=1.270681380527094, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8551086119841784, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.6224154182709754, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [1:45:07<5:16:30, 1266.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.22766433987999335, Eval score=0.8125\n",
      "current label_words: ['horrible', 'remarkable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 426.62it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1031.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.2276666148868003, Eval score=0.6875\n",
      "Epoch 2: Train loss=0.4889321715090773, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2634941844644345, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.25736280560994373, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [2:07:36<5:01:58, 1294.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.3903749104914027, Eval score=0.625\n",
      "current label_words: ['terrible', 'brilliant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 493.33it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.128300400949797, Eval score=0.5\n",
      "Epoch 2: Train loss=1.0674331626432831, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.6011688623111695, Eval score=0.875\n",
      "Epoch 4: Train loss=0.6810656589186692, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [2:28:49<4:38:53, 1287.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.1570308672144165, Eval score=0.84375\n",
      "current label_words: ['terrible', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 451.65it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1033.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.1662085960851982, Eval score=0.84375\n",
      "Epoch 2: Train loss=0.046194135922632995, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.20158991879031873, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.009880203132524912, Eval score=0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [2:51:16<4:21:16, 1306.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.27703922392970526, Eval score=0.90625\n",
      "current label_words: ['disappointing', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 415.57it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1032.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.3897865504303581, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.6985758165101288, Eval score=0.5625\n",
      "Epoch 3: Train loss=0.20094251398768392, Eval score=0.875\n",
      "Epoch 4: Train loss=0.021359096241212683, Eval score=0.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [3:11:17<3:53:28, 1273.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0014102687238164435, Eval score=0.90625\n",
      "current label_words: ['strange', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 410.23it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4916955009493904, Eval score=0.5\n",
      "Epoch 2: Train loss=0.5601507005485473, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.050391235166898696, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.04447055474645367, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [3:31:10<3:28:06, 1248.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.5673459974156145, Eval score=0.78125\n",
      "current label_words: ['terrible', 'good']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 363.21it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.9351653824437118, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9128216816243366, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.16854792425147025, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.004711857527581742, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [3:50:34<3:03:23, 1222.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0015127657302400621, Eval score=0.84375\n",
      "current label_words: ['bad', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 524.59it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.8022562539517715, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9302898038731655, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.43625156552297994, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.04196147369020764, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [4:10:06<2:40:57, 1207.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.012102704274127518, Eval score=0.65625\n",
      "current label_words: ['short', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 412.44it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1203.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.8797737168388267, Eval score=0.75\n",
      "Epoch 2: Train loss=0.3757321042244257, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2311989847357836, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.43366863449273296, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [4:30:04<2:20:30, 1204.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.620734619528541, Eval score=0.59375\n",
      "current label_words: ['disappointing', 'delightful']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 426.69it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1143.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.324861448905718, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.5381804386270232, Eval score=0.5625\n",
      "Epoch 3: Train loss=0.36967586419450527, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.12384688404563349, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [4:49:18<1:58:55, 1189.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.18691727038913086, Eval score=0.6875\n",
      "current label_words: ['bad', 'brilliant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 490.61it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1103.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.5005056243027184, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.7745201710495166, Eval score=0.875\n",
      "Epoch 3: Train loss=0.13810731278863386, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.004411970109288177, Eval score=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [5:08:29<1:38:08, 1177.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.00313117326692236, Eval score=0.78125\n",
      "current label_words: ['bad', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 492.79it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.182302282977588, Eval score=0.5\n",
      "Epoch 2: Train loss=0.5332906207768247, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.08191546555644891, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.35411459776105403, Eval score=0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [5:26:58<1:17:08, 1157.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.13951104862388775, Eval score=0.46875\n",
      "current label_words: ['horrible', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 466.99it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1219.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4121702450024713, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.32750329683221935, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.01347528245059948, Eval score=0.90625\n",
      "Epoch 4: Train loss=0.3011642301885331, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [5:45:44<57:23, 1147.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.022153147599055956, Eval score=0.875\n",
      "current label_words: ['fine', 'remarkable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 492.08it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1185.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.7259275259780793, Eval score=0.5\n",
      "Epoch 2: Train loss=0.7986743192886934, Eval score=0.65625\n",
      "Epoch 3: Train loss=0.6269949703128077, Eval score=0.5\n",
      "Epoch 4: Train loss=0.16056611831118062, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [6:04:43<38:10, 1145.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.003870869544044808, Eval score=0.78125\n",
      "current label_words: ['disappointing', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 454.61it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1183.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4480454477061357, Eval score=0.5\n",
      "Epoch 2: Train loss=0.8791331336833537, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.4247932309117459, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.2258107879970339, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [6:23:21<18:56, 1137.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0734178872121447, Eval score=0.875\n",
      "current label_words: ['terrible', 'fine']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 489.28it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1164.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.6715138442009447, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.6137157797584223, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.38552796499482156, Eval score=0.875\n",
      "Epoch 4: Train loss=0.34600197029577373, Eval score=0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [6:41:58<00:00, 1205.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.6120385159649686, Eval score=0.78125\n",
      "final best label words: ['terrible', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verbalizer generation\n",
    "from openprompt.prompts.prompt_generator import RobertaVerbalizerGenerator\n",
    "if auto_v:\n",
    "    print('performing auto_v...')\n",
    "    # Load generation model for verbalizer generation\n",
    "    if cuda:\n",
    "        plm = plm.cuda()\n",
    "    verbalizer_generator = RobertaVerbalizerGenerator(model=plm, tokenizer=tokenizer, candidate_num=20, label_word_num_per_class=20)\n",
    "    # To improve performance, try larger numbers\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, batch_size=32)\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        verbalizer_generator.register_buffer(data)\n",
    "    label_words_list = verbalizer_generator.generate()\n",
    "    verbalizer_generator.release_memory()\n",
    "\n",
    "    # Iterate over each candidate and select the best one\n",
    "    current_verbalizer = copy.deepcopy(verbalizer)\n",
    "    best_metrics = 0.0\n",
    "    best_label_words = None\n",
    "    for label_words in tqdm(label_words_list):\n",
    "        print(f\"current label_words: {label_words}\")\n",
    "        current_verbalizer.label_words = label_words\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, current_verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "\n",
    "        #######################################################\n",
    "        # TODO: Use score to find your best_label_word        #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_label_words = label_words\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # use the best verbalizer\n",
    "    print(\"final best label words:\", best_label_words)\n",
    "    verbalizer = ManualVerbalizer(tokenizer, num_classes=2, label_words=best_label_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:18:43.541949400Z",
     "start_time": "2023-12-10T11:58:04.849042700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 383.01it/s]\n",
      "tokenizing: 32it [00:00, 1142.78it/s]\n",
      "tokenizing: 872it [00:00, 1095.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.2980121186483302, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.38737686289732665, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.08258779709103692, Eval score=0.875\n",
      "Epoch 4: Train loss=0.3668047572554656, Eval score=0.84375\n",
      "Epoch 5: Train loss=0.00465579945631589, Eval score=0.84375\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "test_dataloader = PromptDataLoader(dataset['test'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "\n",
    "model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# It's always good practice to set no decay to bias and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/5b8876ed26fd495b8353ad7ce94b6f65)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:46:57.884699300Z",
     "start_time": "2023-12-10T12:18:43.474013600Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "allpreds = []\n",
    "for step, inputs in tqdm(enumerate(test_dataloader)):\n",
    "    if cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = model(inputs)\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(allpreds):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report (15 points)\n",
    "\n",
    "- Task 1: Compare **two** different models you employed and provide a brief discussion of your implementation.\n",
    "\n",
    "- Task 2: You need to try at least **three** different templates and verbalizers to compare how your prompts work with the model. Report your performance in zero-shot, one-shot, and few-shot scenarios, with examples drawn from the training set.\n",
    "\n",
    "- Task 3: Try at least three different manually crafted templates to compare them with auto-generated templates. Evaluate the performance with different numbers of demonstrations and plot the graph from Figure 3 in the paper (https://arxiv.org/pdf/2012.15723.pdf). Also, report your best template and verbalizer. . \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
