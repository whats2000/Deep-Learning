{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6Z1Snuk7rIK"
   },
   "source": [
    "# MIS 583 Assignment 6: Text Sentiment Classification with Prompt Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSwr9MgZogRZ"
   },
   "source": [
    "Before we start, please put your name and SID in following format: <br>\n",
    ": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷, M114020035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DzsjuDhlz_k"
   },
   "source": [
    "**Your Answer:**   \n",
    "Hi I'm 鄔仁迪, B104020009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d-Zzebq7rIM"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kc9gd_Wk7rIN"
   },
   "source": [
    "**Sentiment Classification** is an automated process of identifying opinions in text and labeling them as positive or negative based on the emotions customers express within them.\n",
    "\n",
    "In Task 1, you need to fine-tune a pre-trained language model (e.g., BERT) to predict the sentiment of given tweets.\n",
    "\n",
    "In Task 2, we employ prompts to enable the model to perform sentiment analysis through in-context learning, eliminating the need for additional training.\n",
    "\n",
    "In Task 3, you will use the method called LM-BFF to utilize the model in generating the optimal template and verbalizer autonomously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice \n",
    "**You are not allow to use the model like GPT family or pre-trained weight using SST-2 and twitter dataset!!!!!!!!!!!!!!!!!**\n",
    "\n",
    "You can use BERT and RoBERTa encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giUId1Naqacs",
    "tags": []
   },
   "source": [
    "##  Versions of used packages\n",
    "\n",
    "We will check PyTorch version to make sure everything work properly.  \n",
    "We use `python==3.7.14`, `torch==1.12.1+cu113` and `torchvision==0.13.1+cu113`.  \n",
    "This is the default version in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Vuw-gNvjqcYe",
    "ExecuteTime": {
     "end_time": "2023-12-09T00:29:03.120972700Z",
     "start_time": "2023-12-09T00:28:58.772369Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\beartype\\_util\\module\\utilmodimport.py:149: BeartypeModuleUnimportableWarning: Ignoring module \"onnx\" importation exception:\n",
      "\tImportError: cannot import name 'builder' from 'google.protobuf.internal' (C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\protobuf\\internal\\__init__.py)\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]\n",
      "torch 2.1.0+cu118\n",
      "torchvision 0.16.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "print('python', sys.version.split('\\n')[0])\n",
    "print('torch', torch.__version__)\n",
    "print('torchvision', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Text Sentiment Classification (40 points)\n",
    "\n",
    "In this task, you need to fine-tune a pre-trained language model (e.g., BERT or RoBERTa encoder) to predict the sentiment of given tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a4s_a5D7rIR"
   },
   "source": [
    "## Loading Model and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPUkTbnL7rIR"
   },
   "source": [
    "First, let's talk about the model. The Hugging Face team has created an amazing framework called \"transformers\" for NLP tasks. It includes many state-of-the-art machine learning models for PyTorch, TensorFlow, and JAX.\n",
    "\n",
    "To start with this package, follow [this link to installation and a basic tutorial](https://pytorch.org/hub/huggingface_pytorch-transformers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rK0ouXa09pDU",
    "ExecuteTime": {
     "end_time": "2023-12-09T00:29:03.133121900Z",
     "start_time": "2023-12-09T00:29:03.120972700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'!echo happy installation\\n!pip -V\\n!pip install grpcio\\n!pip install google-auth\\n!pip install protobuf==3.9.2\\n!pip install pyprind\\n!pip install tqdm boto3 requests regex sentencepiece sacremoses'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you might need some additional installations there\n",
    "\"\"\"!echo happy installation\n",
    "!pip -V\n",
    "!pip install grpcio\n",
    "!pip install google-auth\n",
    "!pip install protobuf==3.9.2\n",
    "!pip install pyprind\n",
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dmGCAevi7rIS",
    "ExecuteTime": {
     "end_time": "2023-12-09T00:29:09.450890800Z",
     "start_time": "2023-12-09T00:29:03.127125100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "#########################################################################\n",
    "#            Loading tokenizer and model from transformer               #\n",
    "#########################################################################\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification, AutoConfig\n",
    "\n",
    "bert_type = 'roberta-base'\n",
    "\n",
    "# ---------- 1. load from torch.hub ----------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# create a Bert-extended task (classification)\n",
    "model = RobertaForSequenceClassification.from_pretrained(bert_type)\n",
    "\n",
    "# finetune from the output from bert to your task\n",
    "# Replace the out_proj layer in the classifier with a new Linear layer for 3 classes\n",
    "model.classifier.out_proj = nn.Linear(in_features=768, out_features=3, bias=True)\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiMThsYeDa2O"
   },
   "source": [
    "## How to Get Data\n",
    "\n",
    "Please open the file `twitter_sentiment.zip`, creat shortcut to your Google Drive.\n",
    "\n",
    "1. open [LINK of Google Drive](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
    "3. Select the location where you want to place the shortcut.\n",
    "4. Click Add shortcut.\n",
    "\n",
    "After above procedures, we have a shortcut of zip file of dataset.  \n",
    "We can access this in colab after granting the permission of Google Drive.\n",
    "\n",
    "---\n",
    "\n",
    "請先到共用雲端硬碟將檔案 `twitter_sentiment.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lZnFgi5i_2oA",
    "ExecuteTime": {
     "end_time": "2023-12-09T00:29:09.463569700Z",
     "start_time": "2023-12-09T00:29:09.427169400Z"
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqO8DiB6VRQZ"
   },
   "source": [
    "## Unzip Data\n",
    "\n",
    "解壓縮 `twitter_sentiment.zip` 後可以發現裡面有三個csv檔。\n",
    "\n",
    "- `train.csv`, `test.csv` and `val.csv`\n",
    "\n",
    "Training set 有 **10248** 筆資料.  \n",
    "Validation set 有 **1317** 筆資料.  \n",
    "Testing set 有 **3075** 筆資料.  \n",
    "\n",
    "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OSlTMdxf8Zd7",
    "ExecuteTime": {
     "end_time": "2023-12-09T00:29:09.463569700Z",
     "start_time": "2023-12-09T00:29:09.450890800Z"
    }
   },
   "outputs": [],
   "source": [
    "# !unzip -qq ./drive/MyDrive/twitter_sentiment.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wf5GXTme7rIT",
    "ExecuteTime": {
     "end_time": "2023-12-09T00:29:09.463569700Z",
     "start_time": "2023-12-09T00:29:09.450890800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to extract text and label from csv file\n",
    "def get_texts(f_name='./twitter_sentiment', mode='train'):\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "\n",
    "    f_path = os.path.join(f_name, '{}.csv'.format(mode))\n",
    "    with open(f_path, encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for line in reader:\n",
    "            text_list.append(line['text'])\n",
    "            if mode != 'test':\n",
    "                label_list.append(int(line['sentiment_label']))\n",
    "\n",
    "    return text_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6fpY0ZrK7rIV",
    "ExecuteTime": {
     "end_time": "2023-12-09T00:29:09.464568900Z",
     "start_time": "2023-12-09T00:29:09.450890800Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, f_name='./twitter_sentiment', mode='train'):\n",
    "        self.mode = mode\n",
    "\n",
    "        text_list, label_list = get_texts(f_name, mode)\n",
    "        print('mode', mode, 'has', len(text_list), 'datas')\n",
    "        text_list = tokenizer(text_list,\n",
    "                             truncation=True, padding=True,\n",
    "                             return_tensors='pt')\n",
    "\n",
    "        self.text_list = text_list['input_ids']\n",
    "        self.mask_list = text_list['attention_mask']\n",
    "\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_list[idx]\n",
    "        mask = self.mask_list[idx]\n",
    "        if self.mode == 'test':\n",
    "            return text, mask\n",
    "        label = torch.tensor(self.label_list[idx])\n",
    "        return text, mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nCmM4FSw7rIW",
    "ExecuteTime": {
     "end_time": "2023-12-09T00:29:10.313058600Z",
     "start_time": "2023-12-09T00:29:09.459569100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode train has 10248 datas\n",
      "mode val has 1317 datas\n",
      "mode test has 3075 datas\n"
     ]
    }
   ],
   "source": [
    "dataset_train = TwitterDataset(mode='train')\n",
    "dataset_val = TwitterDataset(mode='val')\n",
    "dataset_test = TwitterDataset(mode='test')\n",
    "\n",
    "batch_size = 32\n",
    "train_data = DataLoader(dataset_train, batch_size=batch_size,\n",
    "                       shuffle=True)\n",
    "val_data = DataLoader(dataset_val, batch_size=batch_size // 2,\n",
    "                       shuffle=False)\n",
    "test_data = DataLoader(dataset_test, batch_size=batch_size // 2,\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bqkvofHc7rIY",
    "ExecuteTime": {
     "end_time": "2023-12-09T00:29:10.318059600Z",
     "start_time": "2023-12-09T00:29:10.313058600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['<s>', '@', 'united', 'ĠI', 'Ġhave', 'Ġnever', 'Ġbeen', 'Ġmislead', 'Ġby', 'Ġa', 'Ġcompany', 'Ġas', 'Ġmany', 'Ġtimes', 'Ġas', 'ĠI', 'Ġhave', 'Ġthis', 'Ġweek', 'Ġby', 'ĠUnited', 'ĠAirlines', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "token to s <s>@united I have never been mislead by a company as many times as I have this week by United Airlines!</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(dataset_train[0][0])\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DxZrfCqW7rIY",
    "ExecuteTime": {
     "end_time": "2023-12-09T00:29:10.557301900Z",
     "start_time": "2023-12-09T00:29:10.318059600Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpwgE2Gd7rIZ"
   },
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zlaiAZAD7rIa",
    "ExecuteTime": {
     "end_time": "2023-12-09T00:29:10.560250100Z",
     "start_time": "2023-12-09T00:29:10.558304500Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(raw_preds, y):\n",
    "    preds = raw_preds.argmax(dim=1)\n",
    "    acc = (preds == y).sum()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dmc_Gms97rIa",
    "ExecuteTime": {
     "end_time": "2023-12-09T00:29:11.019861600Z",
     "start_time": "2023-12-09T00:29:10.565260400Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Testing process                              #\n",
    "        #########################################################################\n",
    "        # 1. Clean the gradients of optimizer\n",
    "        # 2. Put correct variables into model\n",
    "        # 3. Get prediction\n",
    "        # 4. Evalutate by criterion and accuracy\n",
    "\n",
    "        # Clean the gradients of optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(text, attention_mask=mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.logits, label)\n",
    "\n",
    "        # Compute accuracy using the provided function\n",
    "        acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "def test(model, data, criterion, log_loss=False):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Training process                             #\n",
    "        #########################################################################\n",
    "        # 1. Put correct variables into model\n",
    "        # 2. Get prediction\n",
    "        # 3. Evalutate by criterion and accuracy\n",
    "\n",
    "        # No gradient calculation for evaluation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(text, attention_mask=mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.logits, label)\n",
    "\n",
    "            # Compute accuracy\n",
    "            acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if log_loss:\n",
    "            val_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "# class for monitoring train and test acc/loss\n",
    "class Meter:\n",
    "    def __init__(self):\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def update(self, train_loss, train_acc, val_loss, val_acc):\n",
    "        self.train_loss_list.append(train_loss)\n",
    "        self.train_acc_list.append(train_acc)\n",
    "        self.val_loss_list.append(val_loss)\n",
    "        self.val_acc_list.append(val_acc)\n",
    "\n",
    "    def plot(self):\n",
    "        x = range(len(self.train_loss_list))\n",
    "        plt.plot(x, self.train_loss_list)\n",
    "        plt.plot(x, self.val_loss_list, color='r')\n",
    "        plt.legend(['train_loss', 'val_loss'])\n",
    "        plt.show()\n",
    "        plt.plot(x, self.train_acc_list)\n",
    "        plt.plot(x, self.val_acc_list, color='r')\n",
    "        plt.legend(['train_acc', 'val_acc'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExZyrKd57rIb"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bVDe-fRe7rIc",
    "ExecuteTime": {
     "end_time": "2023-12-09T00:54:20.638822700Z",
     "start_time": "2023-12-09T00:29:11.019861600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:18<00:00,  2.31it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 31.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 0.01992705163373601 train_acc: 0.8017174082747853\n",
      "Epoch 1 val_loss:  0.035184049389085356 val_acc : 0.8496583143507973\n",
      "---------- e 1 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:22<00:00,  2.26it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 28.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss: 0.016433586263963712 train_acc: 0.8732435597189696\n",
      "Epoch 2 val_loss:  0.03658325027677265 val_acc : 0.8572513287775246\n",
      "---------- e 2 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:26<00:00,  2.19it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 27.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss: 0.014661248433087413 train_acc: 0.9086651053864169\n",
      "Epoch 3 val_loss:  0.03668538236038741 val_acc : 0.8587699316628702\n",
      "---------- e 3 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:27<00:00,  2.17it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 28.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss: 0.01315948787050076 train_acc: 0.9364754098360656\n",
      "Epoch 4 val_loss:  0.03795387800118193 val_acc : 0.8488990129081245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:28<00:00,  2.16it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 28.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss: 0.012229908179203856 train_acc: 0.9516978922716628\n",
      "Epoch 5 val_loss:  0.040891755867294044 val_acc : 0.8443432042520881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:28<00:00,  2.16it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 28.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train_loss: 0.01170153479665448 train_acc: 0.9606752537080406\n",
      "Epoch 6 val_loss:  0.04030353267625506 val_acc : 0.8443432042520881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:29<00:00,  2.15it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 28.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train_loss: 0.011195561523599424 train_acc: 0.9690671350507416\n",
      "Epoch 7 val_loss:  0.03978176508876711 val_acc : 0.85041761579347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:29<00:00,  2.14it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 28.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train_loss: 0.010790126341497591 train_acc: 0.9770686963309914\n",
      "Epoch 8 val_loss:  0.04232948690358671 val_acc : 0.8511769172361427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:29<00:00,  2.15it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 28.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train_loss: 0.01097582010181503 train_acc: 0.9721896955503513\n",
      "Epoch 9 val_loss:  0.0410093410999918 val_acc : 0.8451025056947609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [02:30<00:00,  2.14it/s]\n",
      "100%|██████████| 83/83 [00:02<00:00, 28.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train_loss: 0.010521760112973138 train_acc: 0.980191256830601\n",
      "Epoch 10 val_loss:  0.04213308066061797 val_acc : 0.85041761579347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#                          Hyper-parameters                             #\n",
    "#########################################################################\n",
    "max_epoch = 10\n",
    "log_interval = 1\n",
    "best_acc = 0\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################\n",
    "\n",
    "m = Meter()\n",
    "\n",
    "for epoch in range(1, max_epoch + 1):\n",
    "    train_loss, train_acc = train(model, train_data, optimizer, criterion)\n",
    "    val_loss, val_acc = test(model, val_data, criterion, log_loss=True)\n",
    "\n",
    "    if epoch % log_interval == 0:\n",
    "        print('Epoch {} train_loss: {} train_acc: {}'.format(\n",
    "            epoch, train_loss, train_acc\n",
    "        ))\n",
    "        print('Epoch {} val_loss:  {} val_acc : {}'.format(\n",
    "            epoch, val_loss, val_acc\n",
    "        ))\n",
    "\n",
    "    m.update(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "    # model checkpoint\n",
    "    if val_acc > best_acc:\n",
    "        best_model = model\n",
    "        best_acc = val_acc\n",
    "        print('-'*10, 'e', epoch, 'save best model', '-'*10)\n",
    "        torch.save(model.state_dict(), 'ckpts/e{}.pt'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SmtW58OR7rIc",
    "ExecuteTime": {
     "end_time": "2023-12-09T00:59:44.391969400Z",
     "start_time": "2023-12-09T00:59:44.090521700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAGdCAYAAADqsoKGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABcHUlEQVR4nO3deVxU9f4/8NfMwDDsyDYsgigquKC4wIiaWVJcLW4pdr1quWb5Df2l3L6lhVtdpbo3o5tat0Vb1O+1xazULKXy5q6ouQEKLhA7LgyLbDPn98eRwRFUBoHDzLyej8c8gjNnzryHMebF5/M+nyMTBEEAERERkZmTS10AERERUWtgqCEiIiKLwFBDREREFoGhhoiIiCwCQw0RERFZBIYaIiIisggMNURERGQRGGqIiIjIIthIXUB70ev1yMvLg7OzM2QymdTlEBERUTMIgoCysjL4+flBLr/zWIzVhJq8vDwEBARIXQYRERG1QE5ODjp37nzHfawm1Dg7OwMQfyguLi4SV0NERETNodVqERAQYPgcvxOrCTX1U04uLi4MNURERGamOa0jbBQmIiIii8BQQ0RERBaBoYaIiIgsgtX01DSHIAioq6uDTqeTuhQykUKhgI2NDU/XJyKyYgw1N9TU1CA/Px+VlZVSl0It5ODgAF9fXyiVSqlLISIiCTDUQFyY78KFC1AoFPDz84NSqeRf/GZEEATU1NSguLgYFy5cQI8ePe66QBMREVkehhqIozR6vR4BAQFwcHCQuhxqAXt7e9ja2uLSpUuoqamBSqWSuiQiImpn/HP2Jvzr3rzx/SMism78FCAiIiKLwFBDREREFoGhhgyCgoKQnJwsdRlEREQtwkZhMzdy5EiEh4e3Shg5fPgwHB0d770oIiIiCTDUWDhBEKDT6WBjc/e32svLqx0qIiIiS1Kr0+PQhSvYlVaIUB9nTIgIlKwWTj/dhiAIqKypk+QmCEKzapw2bRp2796Nd955BzKZDDKZDJ988glkMhl++OEHDBo0CHZ2dtizZw+ysrLw2GOPQa1Ww8nJCREREdi1a5fR8W6dfpLJZPjoo48wduxYODg4oEePHvjuu++aVZtOp8PMmTPRtWtX2NvbIyQkBO+8806j/dauXYs+ffrAzs4Ovr6+mDNnjuG+a9eu4dlnn4VarYZKpULfvn2xdevWZj0/ERG1nasVNfjm2B+I33gUA1/dickfHcS6vRfxxZE/JK2LIzW3cb1Wh96Lf5Tkuc+8GgMH5d3fmnfeeQdnz55F37598eqrrwIATp8+DQBYsGAB/vnPf6Jbt27o1KkTcnJyMGbMGCxfvhx2dnb47LPPEBsbi4yMDAQG3j5VL1u2DG+++Sb+8Y9/4N1338XkyZNx6dIluLu737E2vV6Pzp0748svv4SHhwf27duHZ555Br6+vvjLX/4CAHjvvfeQkJCA119/HaNHj0ZpaSn27t1rePzo0aNRVlaG9evXIzg4GGfOnIFCoWjWz5CIiFqPIAjIKq5ASlohUtKKcOTSFehv+vvb00mJB0K88VBvtXRFgqHGrLm6ukKpVMLBwQE+Pj4AgPT0dADAq6++ioceesiwr7u7O/r372/4/rXXXsM333yD7777zmh05FbTpk3DxIkTAQArVqzAv/71Lxw6dAh/+tOf7libra0tli1bZvi+a9eu2L9/P7744gtDqPn73/+Ov/3tb3j++ecN+0VERAAAdu3ahUOHDiEtLQ09e/YEAHTr1u3uPxQiImoVtTo9Dl+8gpS0IqSkFeLiZePLCIX6OGNUL2+M6qVGeGc3yOXSr8TPUHMb9rYKnHk1RrLnvleDBw82+r68vBxLly7Ftm3bkJ+fj7q6Oly/fh3Z2dl3PE6/fv0MXzs6OsLFxQVFRUXNqmH16tVYu3YtsrOzcf36ddTU1CA8PBwAUFRUhLy8PIwaNarJxx4/fhydO3c2BBoiImp7pZW1+PVsEXalFWF3RhG0VXWG+2wVMgzp5oHoXmo8GOqNAPeOtwI/Q81tyGSyZk0BdVS3nsX0wgsvYOfOnfjnP/+J7t27w97eHuPHj0dNTc0dj2Nra2v0vUwmg16vv+vz/+c//8ELL7yAt956C1FRUXB2dsY//vEPHDx4EIB4WYM7udv9RETUOs4XlyMlrQi70gpx5NJV6G6aV3J3FKeVont5476eXnCy69ifix27OrorpVIJnU531/327t2LadOmYezYsQDEkZuLFy+2WV179+7F0KFD8dxzzxm2ZWVlGb52dnZGUFAQUlJS8MADDzR6fL9+/fDHH3/g7NmzHK0hIiMXSyqwK60QJ3NL0bmTPXr5uqCXrwuCPByh6ABTIB1dnU6PI5euGvpjzpdUGN3fU+2EUb3UiO7ljfCATmb1M21RqFm9ejX+8Y9/oKCgAP3798e7776LyMjIJvetra1FUlISPv30U+Tm5iIkJARvvPGGUU9GUFAQLl261Oixzz33HFavXg1AXI9l9+7dRvc/++yzeP/991vyEixGUFAQDh48iIsXL8LJyem2oyg9evTA5s2bERsbC5lMhkWLFjVrxKWlevTogc8++ww//vgjunbtis8//xyHDx9G165dDfssXboUs2fPhre3t6EpeO/evZg7dy7uv/9+jBgxAnFxcVi5ciW6d++O9PR0yGSyu/bzEJFlqdPpcTT7GlLSCrErrRBZxRVN7mdvq0BPH2f09nU2BJ1QH2c4q2yb3N+alF6vxe6zxUhJK8SvGcUovV5ruM9WIYOmqwdG9fJGdC91h5xWai6TQ82mTZuQkJCA999/HxqNBsnJyYiJiUFGRga8vb0b7Z+YmIj169fjww8/RGhoKH788UeMHTsW+/btw4ABAwCIi77dPNpw6tQpPPTQQ3jiiSeMjjVr1izDWT4AeEVtiNNKU6dORe/evXH9+nWsW7euyf1WrlyJGTNmYOjQofD09MRLL70ErVbbZnU9++yzOHbsGCZMmACZTIaJEyfiueeeww8//GDYZ+rUqaiqqsLbb7+NF154AZ6enhg/frzh/q+//hovvPACJk6ciIqKCnTv3h2vv/56m9VMRB2HtqoW/z1bjJS0IvySUYRrlQ0fwjZyGSK7umNINw/kl17HmfwyZBRocb1Wh99zruH3nGtGxwpwt0cvHxdD0Ont64LOnew7RGNrW6of0UpJK8Lhi1dQd9O0UicHWzwQIjb5jujpaTHBTyY0d1GUGzQaDSIiIrBq1SoA4qm3AQEBmDt3LhYsWNBofz8/P7zyyiuIj483bIuLi4O9vT3Wr1/f5HPMmzcPW7duxblz5yCTif/o7nXlXK1WC1dXV5SWlsLFxcXovqqqKly4cAFdu3aFSqVq0fFJenwficxb9uVK8UM4vRAHzxt/CLsZPoS9MaKnF1xu+RDW6QVcvFyBtHztjVsZ0vK1yC+tavK5nOxsEOrjjNBbRnXMuZfybiNa3b2dDKMxAwPNZ1rpTp/ftzLp3aupqUFqaioWLlxo2CaXyxEdHY39+/c3+Zjq6upGHzD29vbYs2fPbZ9j/fr1SEhIMASaehs2bMD69evh4+OD2NhYLFq06LajNdXV1aiurjZ835ajEkREZDqdXsDR7KuGU4bPFZUb3R/s5YjoXmqM6qXGwEA32Chuv16sQi5DsJcTgr2c8Gg/P8P2qxU1SCtoCDlp+VqcKyxHeXUdjly6iiOXrhr2lcmAIA9H9PJ1bhjZ8XOBn6uq0edRR6GtqsXujBvTSmeLG41oabq5Y1SoGqN6eaOLh+VfBsekUFNSUgKdTge12nhxHbVabVgf5VYxMTFYuXIlRowYgeDgYKSkpGDz5s23bW7dsmULrl27hmnTphltnzRpErp06QI/Pz+cOHECL730EjIyMrB58+Ymj5OUlGS0Tgq1rtmzZ992pO3JJ5+0+l4nImpaWVUt/nu2BClphfglowhXb/oQVshliAxyN4wmBHne+4dwJ0clhgZ7Ymiwp2FbrU6PCyXiqM6Zm0Z1isuqcaGkAhdKKrD9ZIFhf1d7W4T6OBumrnr5uqCH2gmqVlh+oyUuXa7ArhtB8NAF00a0LJ1J0095eXnw9/fHvn37EBUVZdj+4osvYvfu3YbTdW9WXFyMWbNm4fvvv4dMJkNwcDCio6Oxdu1aXL9+vdH+MTExUCqV+P777+9Yy88//4xRo0YhMzMTwcHBje5vaqQmICCA00+tpKio6LajXy4uLk32V7U1vo9EHVPOlUpDb8fBC5dRq2v42HG1t8XIEC+M6qXG/T294Gov3YdwSXl1o+mrzKJyo9BQTyGXoaun442pq4bA4+1s1+qjOvUjWvU/w8x7GNEyR202/eTp6QmFQoHCwkKj7YWFhYYVbW/l5eWFLVu2oKqqCpcvX4afnx8WLFjQ5Oqwly5dwq5du247+nIzjUYDALcNNXZ2drCzs2vOy6IW8Pb2liS4EFHHp9MLOJ5z1TCacLbQ+EO4m6ejYSXawV06dZgPYU8nO9zXwwv39Wi4uG91nQ6ZReVG01dp+VpcraxFZlE5MovK8f3vDcdwd1QaT1/5uqC7txOUNqa9xvYe0bIUJoUapVKJQYMGISUlBY8//jgAsVE4JSXljkvtA4BKpYK/vz9qa2vx9ddfG5bKv9m6devg7e2NRx555K61HD9+HADg6+tryksgIqI2UF5dh9/OFmNXWhF+zSjC5YqGhT0VchkGd+l0YzTBG928nCSs1DR2Ngr08XNFHz9XwzZBEFCovTGqc1O/zvniclypqMHezMvYm3nZsL+tQuz3qZ+6qh/d8XAy/sPbXEa0OjKT27wTEhIwdepUDB48GJGRkUhOTkZFRQWmT58OAJgyZQr8/f2RlJQEADh48CByc3MRHh6O3NxcLF26FHq9Hi+++KLRcfV6PdatW4epU6fCxsa4rKysLGzcuBFjxoyBh4cHTpw4gfnz52PEiBFGy/gTEVH7+eNqpWEl2oPnr6BG17D2lbPKBiNvrEQ7sqc3XB0s50NYJpPBx1UFH1cVHghtGLGuqtXhbGGZYfrqzI1RnbKqOqQXlCG9oAw4lmvY38vZDr18XRDQyR6HL14xmxGtjszkUDNhwgQUFxdj8eLFKCgoQHh4OHbs2GFoHs7OzoZc3vCDr6qqQmJiIs6fPw8nJyeMGTMGn3/+Odzc3IyOu2vXLmRnZ2PGjBmNnlOpVGLXrl2GABUQEIC4uDgkJiaaWj4REbWQXi/g+B/XDCvRpheUGd3f1dMRo0K98WAvb0QEucPWyj6EVbYK9Ovshn6d3QzbBEFA7rXrjaavLl2pRHFZNYrLig37mvOIVkdh8jo15orr1Fg+vo9Era+iug6/nWvo7Sgpb5hWksuAwUHuiL4xmhDMD+Fmq6iuQ8aNUZ1LlyvRx8/F4ka0WkubNQoTEZHly712HT+nFWJXWhH2Z11uNK10f08vRPdSY2SIF9wclBJWar4c7WwwMLATBgZ2kroUi8JQY+WCgoIwb948zJs3T+pSiEgier2AE7mlN1aiLUJavvFyDV08HDAqVLzAYURX65tWIvPBUENEZIUqaxqmlX5OL0ZJecO6XnIZMKhLJ8OVmoO9nDrsirpEN2OoISKyEkVlVdh1pgg7zxRgb9Zl1NQ1TCs52YnTSqN6eWNkiDfcHTmtROaHY4i3IwhARYU0t2b2bn/wwQfw8/ODXq832v7YY49hxowZyMrKwmOPPQa1Wg0nJydERERg165dLf6RrFy5EmFhYXB0dERAQACee+45lJcbn4K4d+9ejBw5Eg4ODujUqRNiYmJw9ap4bRW9Xo8333wT3bt3h52dHQIDA7F8+fIW10NEd3fpcgU++G8W4t7bB82KFLz8zUn8klGMmjo9AtztMW1oENbP1ODoooewevJAjBvYmYGGzBZHam6nshJwkqiTv7wccLz7CpFPPPEE5s6di19++QWjRo0CAFy5cgU7duzA9u3bUV5ejjFjxmD58uWws7PDZ599htjYWGRkZCAwMNDksuRyOf71r3+ha9euOH/+PJ577jm8+OKLWLNmDQBxQcRRo0ZhxowZeOedd2BjY4NffvnFcJ2vhQsX4sMPP8Tbb7+N4cOHIz8//7bXDCOilhEEAafztPjpdAF+PF2IjELj0677B7jh4d5qPNRbjR7enFYiy8JTunGbU4ErKjp8qAGAxx9/HB4eHvj4448BiKM3y5YtQ05OjtF6QfX69u2L2bNnG1aAvpdG4a+++gqzZ89GSUkJAPGio9nZ2U1egb2srAxeXl5YtWoVnn76aZOfqzl4SjdZK51ewOGLV/Dj6QL8dLoQudcarqtnI5dhSDcPxPRRI7q3Gr6u9hJWSmQ6ntLdGhwcxHAh1XM30+TJkzFr1iysWbMGdnZ22LBhA/76179CLpejvLwcS5cuxbZt25Cfn4+6ujpcv34d2dnZLSpr165dSEpKQnp6OrRaLerq6lBVVYXKyko4ODjg+PHjeOKJJ5p8bFpaGqqrqw0jSkR0b6pqddhzrgQ/ni5ASnoRrtx0WQKVrRz39/RCTB8fjApVc+0TshoMNbcjkzV7tERKsbGxEAQB27ZtQ0REBH777Te8/fbbAIAXXngBO3fuxD//+U90794d9vb2GD9+PGpqau5y1MYuXryIRx99FP/zP/+D5cuXw93dHXv27MHMmTNRU1MDBwcH2Nvf/i/AO91HRM1Ter0Wv6QX4aczBfg1oxiVNTrDfW4OthgVqkZMHzXu6+EFe6VCwkqJpMFQY+ZUKhXGjRuHDRs2IDMzEyEhIRg4cCAAsWl32rRpGDt2LACgvLwcFy9ebNHzpKamQq/X46233jJMa33xxRdG+/Tr1w8pKSlYtmxZo8f36NED9vb2SElJabPpJyJLVKStwk9nCvHj6QLsz7qMOn1Dx4CfqwoP9/HBw33UiAxy57WByOox1FiAyZMn49FHH8Xp06fx5JNPGrb36NEDmzdvRmxsLGQyGRYtWtToTKnm6t69O2pra/Huu+8iNjYWe/fuxfvvv2+0z8KFCxEWFobnnnsOs2fPhlKpxC+//IInnngCnp6eeOmll/Diiy9CqVRi2LBhKC4uxunTpzFz5sx7ev1EluZCSQV+PF2AH08X4Fj2NaP7eng7IaaPD2L6+KCvvwsbfYluwlBjAR588EG4u7sjIyMDkyZNMmxfuXIlZsyYgaFDhxpChVarvcORbq9///5YuXIl3njjDSxcuBAjRoxAUlISpkyZYtinZ8+e+Omnn/Dyyy8jMjIS9vb20Gg0mDhxIgBg0aJFsLGxweLFi5GXlwdfX1/Mnj373l48kQUQBAGncrWGIHOuyLifb0CgG2L6+ODh3mpe5JDoDnj2E3jWjKXg+0jmpE6nx6GLV/DT6UL8dLoAeaVVhvts5DJEBXuIU0u91VC78N8zWS+e/URE1AFV1erw37PF+PF0IVLSC3GtstZwn72tAiNDxDOWHgj1hqs9z1giMhVDDQEANmzYgGeffbbJ+7p06YLTp0+3c0VElqG0shYp6YX46XQhdp8txvXahjOWOjnYIrqXGjF9fDC8hydUtjxjieheMNQQAODPf/4zNBpNk/fZ2vIvRiJTFJRWYecZcUXfA+eNz1jyd7PHw33EIDO4SyeesUTUihhqCADg7OwMZ2dnqcsgMluZReX46UaQ+T3nmtF9IWpnxPRR4+E+PujjxzOWiNoKQ81NrKRn2mLx/aP2JAgCfv+j9MY1lgqQVVxhuE8mAwYEuBlOvQ7y7PgLeRJZAoYaNEyvVFZWcuVbM1ZZWQmA02XUdmp1ehy60HCNpQJtwxlLtgoZooI9EdNHjYd6qeHNM5aI2h1DDQCFQgE3NzcUFRUBABwcHDg8bEYEQUBlZSWKiorg5uYGhYLNltR6anV67MkswbYT+dh5phCl1xvOWHJUKjAyxBsP91HjgVBvuKgYqImkxFBzg4+PDwAYgg2ZHzc3N8P7SHQv6nR67Mu6jG0n8rHjdIFRkPFwVIpnLPVVY2gwz1gi6kgYam6QyWTw9fWFt7c3amtr7/4A6lBsbW05QkP3pO7G1NL3J/Lx4+kCo6teezrZYUyYD8aE+SIiyB0KOUdyiToihppbKBQKfjgSWQmdXsDhi1ew9UQedpwqQEl5Q5Bxd1RidF8fPNLPF5quHgwyRGaAoYaIrIpeLyA1+yq2/p6H7acKUFxWbbjPzcFWDDJhfhjSjVe9JjI3DDVEZPH0egHHcq5h64k8bD+Zj0JtQ5BxUdkgpo8PHu3vh6HBHrBlkCEyWww1RGSR6teR2fq7GGRuvmCks50NHuqjRmw/Pwzr7gmlDYMMkSVgqCEiiyEIAk7larH1RB62nshH7rXrhvsclQo81FuNR/v54b6enrCzYe8ckaVhqCEisyYIAs7ka7H1RD62nchH9pVKw30OSgVG9VLj0X6+uL+nF0+/JrJwDDVEZHYEQUBGYRm2/p6PbSfzcaGk4RIFKls5RoWKQWZkiDfslQwyRNaCoYaIzMa5wjJ8fyIf207kGV1ryc5GjgdCvPFof188GOoNByV/tRFZI/6fT0QdWlZxObadyMfWE3k4W1hu2K5UyHF/iBce7eeLUb3UcLLjrzMia8ffAkTU4VwsqcC2k/n4/vc8pBeUGbbbKmQY0cMLj/YXgwyvtUREN2OoIaIOIftyJbadFEdkTudpDdtt5DIM7+GJR/v54aHearjaM8gQUdMYaohIMn9crcT2k/nYeiIfJ/4oNWxXyGUYGuyB2H5+eLiPGm4OSgmrJCJzwVBDRO0q79p1Q5A5nnPNsF0uA6KCPfBImB/+1NcH7o4MMkRkGoYaImpzhdoqQ5BJvXTVsF0mAzRd3fFIPz+M7usDTyc7CaskInPXorXBV69ejaCgIKhUKmg0Ghw6dOi2+9bW1uLVV19FcHAwVCoV+vfvjx07dhjts3TpUshkMqNbaGio0T5VVVWIj4+Hh4cHnJycEBcXh8LCwpaUT0TtJKu4HE9/egRDklKw7PszSL10FTIZEBnkjmV/7oODL4/Cf56JwlNDujDQENE9M3mkZtOmTUhISMD7778PjUaD5ORkxMTEICMjA97e3o32T0xMxPr16/Hhhx8iNDQUP/74I8aOHYt9+/ZhwIABhv369OmDXbt2NRRmY1za/PnzsW3bNnz55ZdwdXXFnDlzMG7cOOzdu9fUl0BEbay0shb/+vkcPt13EXV6AQAwqEsnPBLmizFhvvBxVUlcIRFZIpkgCIIpD9BoNIiIiMCqVasAAHq9HgEBAZg7dy4WLFjQaH8/Pz+88soriI+PN2yLi4uDvb091q9fD0AcqdmyZQuOHz/e5HOWlpbCy8sLGzduxPjx4wEA6enp6NWrF/bv348hQ4bctW6tVgtXV1eUlpbCxcXFlJdMRM1Up9Pj/w7nYOVPGbhaWQsAeDDUGy+P6YXu3k4SV0dE5siUz2+TRmpqamqQmpqKhQsXGrbJ5XJER0dj//79TT6muroaKpXxX2X29vbYs2eP0bZz587Bz88PKpUKUVFRSEpKQmBgIAAgNTUVtbW1iI6ONuwfGhqKwMDA24aa6upqVFdXG77XarWN9iGi1rPnXAle23oGGYXiujI9vJ2Q+Ghv3N/TS+LKiMhamBRqSkpKoNPpoFarjbar1Wqkp6c3+ZiYmBisXLkSI0aMQHBwMFJSUrB582bodDrDPhqNBp988glCQkKQn5+PZcuW4b777sOpU6fg7OyMgoICKJVKuLm5NXregoKCJp83KSkJy5YtM+XlEVELXCipwPJtZ7ArrQgA4OZgi4SHemJSZCBsFC1q2yMiapE2P/vpnXfewaxZsxAaGgqZTIbg4GBMnz4da9euNewzevRow9f9+vWDRqNBly5d8MUXX2DmzJktet6FCxciISHB8L1Wq0VAQEDLXwgRGSm9Xot3U87h0/0XUasTYCOX4amoLnh+VA+uK0NEkjAp1Hh6ekKhUDQ666iwsBA+Pj5NPsbLywtbtmxBVVUVLl++DD8/PyxYsADdunW77fO4ubmhZ8+eyMzMBAD4+PigpqYG165dMxqtudPz2tnZwc6OZ1MQtbY6nR7/OZyDlTvP4kpFDQDggRAvvPJIb/bNEJGkTBobViqVGDRoEFJSUgzb9Ho9UlJSEBUVdcfHqlQq+Pv7o66uDl9//TUee+yx2+5bXl6OrKws+Pr6AgAGDRoEW1tbo+fNyMhAdnb2XZ+XiFrP3swSPPKvPUjccgpXKmrQ3dsJn0yPwLrpkQw0RCQ5k6efEhISMHXqVAwePBiRkZFITk5GRUUFpk+fDgCYMmUK/P39kZSUBAA4ePAgcnNzER4ejtzcXCxduhR6vR4vvvii4ZgvvPACYmNj0aVLF+Tl5WHJkiVQKBSYOHEiAMDV1RUzZ85EQkIC3N3d4eLigrlz5yIqKqpZZz4R0b0R+2bSsCtNHKV1c7DF/OiemKQJhC37ZoiogzA51EyYMAHFxcVYvHgxCgoKEB4ejh07dhiah7OzsyGXN/ySq6qqQmJiIs6fPw8nJyeMGTMGn3/+udE00h9//IGJEyfi8uXL8PLywvDhw3HgwAF4eTWcNfH2229DLpcjLi4O1dXViImJwZo1a+7hpRPR3ZRer8Wqn8/hk31i34xCLsNTQ7pgXjT7Zoio4zF5nRpzxXVqiJpPpxfwn8PZeOunhr6ZkSFeSHykF7p7O0tcHRFZkzZbp4aILN++zBK8uvUM0gvE9WaCvRyR+GhvPBDSeMVwIqKOhKGGiAAAF0sqsHx7GnaeEftmXO1tMT+6ByYP6cK+GSIyCww1RFZOW1WLVT9nYt3eC0Z9M8+P6oFOjuybISLzwVBDZKV0egGbDufgrZ8ycPlG38z9Pb2w6FH2zRCReWKoIbJC+7JK8Or37JshIsvCUENkRS5dFteb+elG34yLygbzH+qJJ9k3Q0QWgKGGyAqUGfpmLqJGp4dCLsOTmkDMi+7JvhkishgMNUQWTKcX8MURsW+mpFzsm7mvhycWPdobPdXsmyEiy8JQQ2Sh9mddxqtbzyAtXwsA6ObpiMRHe+GBEG/IZDKJqyMian0MNUQWJvtyJZZvP4MfTzf0zcyL7omnotg3Q0SWjaGGyEKUVdVi1S+ZWLenoW9m8o2+GXf2zRCRFWCoITJzOr2AL4/k4J/smyEiK8dQQ2TGDpy/jFe/P4MzN/XNvPJILzwYyr4ZIrI+DDVEZij7ciVWbE/DjtMFAABnlQ2eH9UDU6KCoLRh3wwRWSeGGiIzUlZVizW/ZuHj3y6gRqeHXAZM1nTB/IfYN0NExFBDZAZ0egFfpebgHz+eRUl5NQBgeHexbybEh30zREQAQw1Rh3fwvLjezOk8sW+mq6cjXhnTC6N6sW+GiOhmDDVEHVTOlUok/ZCG7SfZN0NE1BwMNUQdUFq+Fn/94ABKr9dCLgMmRgYi4aGe8HCyk7o0IqIOi6GGqIPJLCrHkx8dROn1WoT5u+IfT/RDqI+L1GUREXV4DDVEHUj25UpM/ugALlfUoI+fC9Y/rYGrva3UZRERmQVOzBN1EHnXrmPSRwdQqK1GD28nfD6TgYaIyBQMNUQdQHFZNZ786CD+uHodQR4O2PC0huvOEBGZiKGGSGJXK2rw5EcHcb6kAv5u9tgwawi8XVRSl0VEZHYYaogkpK2qxZS1h5BRWAZvZztseFoDfzd7qcsiIjJLDDVEEqmsqcOMdYdxMrcU7o5KbHhagyBPR6nLIiIyWww1RBKoqtVh1mdHcOTSVbiobPDZjEj0UPNyB0RE94Khhqid1dTp8dyGo9ibeRmOSgU+mRGJvv6uUpdFRGT2GGqI2lGdTo95m47h5/Qi2NnI8fG0CAwM7CR1WUREFoGhhqid6PUCXvzqBLafLICtQoZ/PzUIQ7p5SF0WEZHFYKghageCIGDRt6ew+VguFHIZVk0aiJEh3lKXRURkURhqiNqYIAhYvi0NGw5mQyYDVv6lP2L6+EhdFhGRxWGoIWpjb+88i4/2XAAAvD4uDI+F+0tcERGRZWKoIWpD7/2ahX/9nAkAWBrbGxMiAiWuiIjIcjHUELWRT/ddxBs70gEAL/4pBNOGdZW4IiIiy8ZQQ9QGvjicgyXfnQYAzH2wO54b2V3iioiILF+LQs3q1asRFBQElUoFjUaDQ4cO3Xbf2tpavPrqqwgODoZKpUL//v2xY8cOo32SkpIQEREBZ2dneHt74/HHH0dGRobRPiNHjoRMJjO6zZ49uyXlE7Wp737Pw0ubTwAAZg7vioSHekpcERGRdTA51GzatAkJCQlYsmQJjh49iv79+yMmJgZFRUVN7p+YmIh///vfePfdd3HmzBnMnj0bY8eOxbFjxwz77N69G/Hx8Thw4AB27tyJ2tpaPPzww6ioqDA61qxZs5Cfn2+4vfnmm6aWT9SmfjpdgPmbjkMQgEmaQCQ+0gsymUzqsoiIrIJMEATBlAdoNBpERERg1apVAAC9Xo+AgADMnTsXCxYsaLS/n58fXnnlFcTHxxu2xcXFwd7eHuvXr2/yOYqLi+Ht7Y3du3djxIgRAMSRmvDwcCQnJ5tSroFWq4WrqytKS0vh4uLSomMQ3cnus8WY9ekR1Oj0GDfAH/98oj/kcgYaIqJ7Ycrnt0kjNTU1NUhNTUV0dHTDAeRyREdHY//+/U0+prq6GiqVymibvb099uzZc9vnKS0tBQC4u7sbbd+wYQM8PT3Rt29fLFy4EJWVlbc9RnV1NbRardGNqK0cPH8Zz34uBpoxYT54c3w/BhoionZmY8rOJSUl0Ol0UKvVRtvVajXS09ObfExMTAxWrlyJESNGIDg4GCkpKdi8eTN0Ol2T++v1esybNw/Dhg1D3759DdsnTZqELl26wM/PDydOnMBLL72EjIwMbN68ucnjJCUlYdmyZaa8PKIWOZZ9FTM+OYyqWj0eDPVG8oQBsFGwB5+IqL2ZFGpa4p133sGsWbMQGhoKmUyG4OBgTJ8+HWvXrm1y//j4eJw6darRSM4zzzxj+DosLAy+vr4YNWoUsrKyEBwc3Og4CxcuREJCguF7rVaLgICAVnpVRKLTeaWYuvYQKmp0GBrsgTWTB0Jpw0BDRCQFk377enp6QqFQoLCw0Gh7YWEhfHyaXvbdy8sLW7ZsQUVFBS5duoT09HQ4OTmhW7dujfadM2cOtm7dil9++QWdO3e+Yy0ajQYAkJmZ2eT9dnZ2cHFxMboRtabMojI89fEhaKvqMKhLJ3w4ZTBUtgqpyyIislomhRqlUolBgwYhJSXFsE2v1yMlJQVRUVF3fKxKpYK/vz/q6urw9ddf47HHHjPcJwgC5syZg2+++QY///wzuna9+yJlx48fBwD4+vqa8hKIWsWlyxWY9OFBXKmoQZi/K9ZNj4CjXZsPfBIR0R2Y/Fs4ISEBU6dOxeDBgxEZGYnk5GRUVFRg+vTpAIApU6bA398fSUlJAICDBw8iNzcX4eHhyM3NxdKlS6HX6/Hiiy8ajhkfH4+NGzfi22+/hbOzMwoKCgAArq6usLe3R1ZWFjZu3IgxY8bAw8MDJ06cwPz58zFixAj069evNX4ORM2We+06Jn14EEVl1QhRO+OzGZFwUdlKXRYRkdUzOdRMmDABxcXFWLx4MQoKChAeHo4dO3YYmoezs7MhlzcMAFVVVSExMRHnz5+Hk5MTxowZg88//xxubm6Gfd577z0A4mnbN1u3bh2mTZsGpVKJXbt2GQJUQEAA4uLikJiY2IKXTNRyRWVVePKjg8i9dh3dPB3x+dOR6OSolLosIiJCC9apMVdcp4bu1ZWKGvz1g/04W1gOfzd7fDk7Cn5u9lKXRURk0dpsnRoia1V6vRZT1h7E2cJyqF3s8H+zhjDQEBF1MAw1RHdRUV2H6esO4VSuFh6OSmx4eggCPRykLouIiG7BUEN0B1W1Ojz96REczb4GV3tbfD5Tg+7eTlKXRURETWCoIbqNmjo9/md9KvafvwwnOxt8OiMSvf3Yj0VE1FEx1BA1oU6nx/P/OYZfMoqhspVj7bQIhAe4SV0WERHdAUMN0S30egH/+9UJ/HCqAEqFHB9OGYzIru53fyAREUmKoYboJoIg4JUtp/DNsVzYyGVYPXkg7uvhJXVZRETUDAw1RDcIgoDXtqbh/w5lQy4D3p4Qjod6q+/+QCIi6hAYaohueOuns1i79wIA4I24fojt7ydxRUREZAqGGiIAq3/JxKpfxCu+v/ZYHzwxOEDiioiIyFQMNWT11u65gH/8mAEAWDg6FE9FBUlbEBERtQhDDVm1/xzKxqtbzwAAnh/VA8/eHyxxRURE1FIMNWS1vj2ei4XfnAQAPDOiG+ZF95C4IiIiuhcMNWSVdpwqQMIXv0MQgKeGdMHC0aGQyWRSl0VERPeAoYaszq8ZRZj7f0eh0wsYP6gzlv25DwMNEZEFYKghq7I/6zKe/TwVtToBj/TzxRtx/SCXM9AQEVkChhqyGkezr2Lmp4dRXadHdC9vJE8Ih4KBhojIYjDUkFU4lVuKqWsPobJGh+HdPbFq0kDYKvjPn4jIkvC3Olm8c4VlmLL2EMqq6hAR1AkfTBkEla1C6rKIiKiVMdSQRbtYUoHJHx3ElYoa9OvsirXTIuCgtJG6LCIiagMMNWSx/rhaickfHURRWTVCfZzx2YxIOKtspS6LiIjaCEMNWaQibRWe/Oggcq9dRzcvR3w+UwM3B6XUZRERURtiqCGLc7m8GpM/OoiLlysR4G6PDU9r4OVsJ3VZRETUxhhqyKJcr9Fh6rpDOFdUDh8XFTY+PQS+rvZSl0VERO2AoYYsytLvTuNUrhYejkpsmKVBgLuD1CUREVE7Yaghi7H56B/YdCQHchnw7sQBCPZykrokIiJqRww1ZBEyi8rwyjenAADPj+qJod09Ja6IiIjaG0MNmb3rNTo8t+EortfqMKy7B+Y82F3qkoiISAIMNWT2Fn97CmcLy+HlbIfkCQN4PSciIivFUENm7avUP/Bl6h+Qy4B3/hrOU7eJiKwYQw2ZrXOFZVi0ReyjmRfdE0OD2UdDRGTNGGrILFXW1Bn6aIZ390T8A+yjISKydgw1ZJYWf3sa54rK4e1sh+S/hrOPhoiIGGrI/Hx5JAdfGfpoBsDTiX00RETEUENm5mxhGRZ9K/bRzI/uiahgD4krIiKijoKhhsxGfR9NVa0e9/VgHw0RERlrUahZvXo1goKCoFKpoNFocOjQodvuW1tbi1dffRXBwcFQqVTo378/duzYYfIxq6qqEB8fDw8PDzg5OSEuLg6FhYUtKZ/M1KItp5F5o4/m7QnhkLOPhoiIbmJyqNm0aRMSEhKwZMkSHD16FP3790dMTAyKioqa3D8xMRH//ve/8e677+LMmTOYPXs2xo4di2PHjpl0zPnz5+P777/Hl19+id27dyMvLw/jxo1rwUsmc/TFkRx8fVTso/nXRPbREBFREwQTRUZGCvHx8YbvdTqd4OfnJyQlJTW5v6+vr7Bq1SqjbePGjRMmT57c7GNeu3ZNsLW1Fb788kvDPmlpaQIAYf/+/c2qu7S0VAAglJaWNmt/6jjS87VCSOJ2octLW4VVP5+TuhwiImpHpnx+mzRSU1NTg9TUVERHRxu2yeVyREdHY//+/U0+prq6GiqVymibvb099uzZ0+xjpqamora21mif0NBQBAYG3vF5tVqt0Y3MT0V1HZ7bkGroo/mf+4OlLomIiDook0JNSUkJdDod1Gq10Xa1Wo2CgoImHxMTE4OVK1fi3Llz0Ov12LlzJzZv3oz8/PxmH7OgoABKpRJubm7Nft6kpCS4uroabgEBAaa8VOoABEHAoi2nkFVcAbWLHZLZR0NERHfQ5mc/vfPOO+jRowdCQ0OhVCoxZ84cTJ8+HXJ52z71woULUVpaarjl5OS06fNR6/vyyB/YfCwXchnw7sSB8GAfDRER3YFJycLT0xMKhaLRWUeFhYXw8fFp8jFeXl7YsmULKioqcOnSJaSnp8PJyQndunVr9jF9fHxQU1ODa9euNft57ezs4OLiYnQj85FeoDWsR/O3h0MQ2dVd4oqIiKijMynUKJVKDBo0CCkpKYZter0eKSkpiIqKuuNjVSoV/P39UVdXh6+//hqPPfZYs485aNAg2NraGu2TkZGB7Ozsuz4vmZ+K6jrEbziK6jo97u/pxT4aIiJqFhtTH5CQkICpU6di8ODBiIyMRHJyMioqKjB9+nQAwJQpU+Dv74+kpCQAwMGDB5Gbm4vw8HDk5uZi6dKl0Ov1ePHFF5t9TFdXV8ycORMJCQlwd3eHi4sL5s6di6ioKAwZMqQ1fg7UQQiCgMQbfTQ+Liqs/Et/9tEQEVGzmBxqJkyYgOLiYixevBgFBQUIDw/Hjh07DI2+2dnZRv0yVVVVSExMxPnz5+Hk5IQxY8bg888/N2r6vdsxAeDtt9+GXC5HXFwcqqurERMTgzVr1tzDS6eO6IsjOfjmWC4UchnenTSAfTRERNRsMkEQBKmLaA9arRaurq4oLS1lf00HlZavxeOr96K6To8X/xSC50byMghERNbOlM9vXvuJOoTym/poRoZ4YfYI9tEQEZFpGGpIcoIg4JVvTuJ8SX0fDdejISIi0zHUkOT+czgH3x7Pg0Iuw6pJA+DuqJS6JCIiMkMMNSSpM3laLPnuNADgf2NCMDiI69EQEVHLMNSQZMqr6zBn41HU1OnxQIgXnrmvm9QlERGRGWOoIUkIgoCXN4t9NL6uKrzFPhoiIrpHDDUkif87lIPvfmcfDRERtR6GGmp3Z/K0WPq92EfzYkwIBnVhHw0REd07hhpqV2VVtYi/0UfzYKg3ZrGPhoiIWglDDbUbQRCwcPNJXCipgJ+rCm89wes6ERFR62GooXaz8VA2tp7Ih41chncnDUQn9tEQEVErYqihdnEqtxTLvj8DAHjxTyEY1KWTxBUREZGlYaihNldWVWtYjya6F/toiIiobTDUUJuq76O5eLkS/m72+OcT/SGTsY+GiIhaH0MNtan1B2/uoxkANwf20RARUdtgqKE2cyq3FK/d6KNZMDoUAwPZR0NERG2HoYbahGE9Gp3YRzNzeFepSyIiIgvHUEOtThAELPj6JC6xj4aIiNoRQw21uvUHLmHbSbGPZhX7aIiIqJ0w1FCrOpVbite2pgEQ+2gGsI+GiIjaCUMNtRptVS2e2yD20TzUW80+GiIialcMNdQqxD6aE8i+cqOPZjz7aIiIqH0x1FCr+PzAJWw/WQBbhQyrJw+Eq4Ot1CUREZGVYaihe3byj1L83dBH0wvhAW7SFkRERFaJoYbuSen1Wjy3MRU1Oj0e7q3GjGFBUpdERERWiqGGWqy+jybnynV07mSPf7CPhoiIJMRQQy326b6L+OHUjT6aSeyjISIiaTHUUIuc+OMalm8X+2heHtML/dlHQ0REEmOoIZOVXhev61SrExDTR41pQ4OkLomIiIihhkwjCAJe/Op35Fy5jgB3e7zJPhoiIuogGGrIJJ/su4gfTxc29NHYs4+GiIg6BoYaarbfc65hxY0+mlfG9EK/zm7SFkRERHQThhpqltLKhj6a0X19MJV9NERE1MEw1NBdCYKA//3qd/xx9ToC3R3wxvh+7KMhIqIOh6GG7mrd3ov46UwhlAo5Vk8aCBcV+2iIiKjjYaihOzqecw1JP9zoo3mkF8I6u0pcERERUdNaFGpWr16NoKAgqFQqaDQaHDp06I77JycnIyQkBPb29ggICMD8+fNRVVVluD8oKAgymazRLT4+3rDPyJEjG90/e/bslpRPzVRaWYv4DWIfzZgwH0yJ6iJ1SURERLdlY+oDNm3ahISEBLz//vvQaDRITk5GTEwMMjIy4O3t3Wj/jRs3YsGCBVi7di2GDh2Ks2fPYtq0aZDJZFi5ciUA4PDhw9DpdIbHnDp1Cg899BCeeOIJo2PNmjULr776quF7BwcHU8unZhIEAS989Ttyr4l9NK/HsY+GiIg6NpNDzcqVKzFr1ixMnz4dAPD+++9j27ZtWLt2LRYsWNBo/3379mHYsGGYNGkSAHFUZuLEiTh48KBhHy8vL6PHvP766wgODsb9999vtN3BwQE+Pj6mlkwt8PGeC9jJPhoiIjIjJk0/1dTUIDU1FdHR0Q0HkMsRHR2N/fv3N/mYoUOHIjU11TBFdf78eWzfvh1jxoy57XOsX78eM2bMaDQysGHDBnh6eqJv375YuHAhKisrTSmfmulY9lW8/kM6ACDxUfbREBGReTBppKakpAQ6nQ5qtdpou1qtRnp6epOPmTRpEkpKSjB8+HAIgoC6ujrMnj0bL7/8cpP7b9myBdeuXcO0adMaHadLly7w8/PDiRMn8NJLLyEjIwObN29u8jjV1dWorq42fK/Vak14pdbrWmUN5mw8hjq9gEfCfPHUEPbREBGReTB5+slUv/76K1asWIE1a9ZAo9EgMzMTzz//PF577TUsWrSo0f4ff/wxRo8eDT8/P6PtzzzzjOHrsLAw+Pr6YtSoUcjKykJwcHCj4yQlJWHZsmWt/4IsmCAIeOHLE8i9dh1dPByQFBfGPhoiIjIbJk0/eXp6QqFQoLCw0Gh7YWHhbXtdFi1ahKeeegpPP/00wsLCMHbsWKxYsQJJSUnQ6/VG+166dAm7du3C008/fddaNBoNACAzM7PJ+xcuXIjS0lLDLScnpzkv0ap9vOcCdqWxj4aIiMyTSaFGqVRi0KBBSElJMWzT6/VISUlBVFRUk4+prKyEXG78NAqFAoA4MnCzdevWwdvbG4888shdazl+/DgAwNfXt8n77ezs4OLiYnSj28u5Umnoo1kU2xt9/dlHQ0RE5sXk6aeEhARMnToVgwcPRmRkJJKTk1FRUWE4G2rKlCnw9/dHUlISACA2NhYrV67EgAEDDNNPixYtQmxsrCHcAGI4WrduHaZOnQobG+OysrKysHHjRowZMwYeHh44ceIE5s+fjxEjRqBfv3738vrphq9S/0CdXsCQbu54UhModTlEREQmMznUTJgwAcXFxVi8eDEKCgoQHh6OHTt2GJqHs7OzjUZmEhMTIZPJkJiYiNzcXHh5eSE2NhbLly83Ou6uXbuQnZ2NGTNmNHpOpVKJXbt2GQJUQEAA4uLikJiYaGr51AS9XsDmY38AAP4aEcg+GiIiMksy4dY5IAul1Wrh6uqK0tJSTkXd4sD5y/jrBwfgZGeDw69Ew16puPuDiIiI2oEpn9+89hPh61RxlOaRMF8GGiIiMlsMNVausqYO20/mAwDiBnWWuBoiIqKWY6ixcj+eLkBFjQ6B7g6ICOokdTlEREQtxlBj5b66MfU0bqA/G4SJiMisMdRYsbxr17Ev6zIAIG4gp56IiMi8MdRYsW+O5UIQAE1XdwS4O0hdDhER0T1hqLFSgiAYznpigzAREVkChhordTT7Gs6XVMDeVoExYU1faoKIiMicMNRYqa+PiqM0o/v6wMmuzS/WTkRE1OYYaqxQVa0OW3/PA8CpJyIishwMNVZoV1ohtFV18HNVIaqbh9TlEBERtQqGGitUvzbN2IH+kMu5Ng0REVkGhhorU6Stwn/PFgPg2jRERGRZGGqszJbjudALwMBAN3TzcpK6HCIiolbDUGNFxLVpcgGwQZiIiCwPQ40VOZWrRUZhGZQ2cjzaz0/qcoiIiFoVQ40VqV+b5uHearja20pcDRERUetiqLESNXV6fHucU09ERGS5GGqsxC8ZRbhaWQsvZzvc191T6nKIiIhaHUONlTCsTTPAHzYKvu1ERGR5+OlmBS6XV+OX9CIAXJuGiIgsF0ONFfju9zzU6QWE+bsixMdZ6nKIiIjaBEONFag/6yluoL/ElRAREbUdG6kLoLaVXqDFqVwtbBUy/DncDEKNIADXrwOlpYBWa/zf23196zatFggIAJ58EnjqKfFrIiKyeAw1Fu7rGw3CD4Z6w91R2bZPVlfXECruFjzutK2u7t5rSUsDXnkFSEwEHnwQmDoVGDcOcHS892MTEVGHxFBjwep0enxzLA9AMxuEtVogL6/lYaSiovWKl8kAFxfA1bXhvzd/fadtTk7Avn3Ap58Cv/4KpKSIt+eeA8aPFwPOiBGAnLOvRESWhKHGgv12rgQl5dVwd1RiZIh34x30euDYMeCHH8TbgQPitnulUpkWRpq639Hx3kJHz57AtGnAxYvA55+LAScrC/jkE/HWpQswZYp469793l8zERFJTiYIgiB1Ee1Bq9XC1dUVpaWlcHFxkbqcdhG/8Si2ncjHtKFBWPrnPuLGK1eAn34SQ8yPPwKFhcYPqg8ZzQkeTW1zcQGUbTzN1RKC0DB6s2mTOLJUb+hQcfTmL38B3NwkK5GIiBoz5fObocZClVbWImL5LtTW1WHnSBd0P/JfMcgcPGg8GuPkBERHA6NHA3/6ExAYKF3R7eX6deDbb8WA89NPDT8PlQp47DEx4Dz0EGDDgUwiIqkx1DTBqkLN5cvYs3oDCr/8Fg9ePIZO5VeN7+/TRwwxo0cDw4d3zJGV9pKXB2zYIAac06cbtvv4iGdPTZ0K9O0rXX1ERFaOoaYJFh1q9Hrg6NGG3hiOxphOEMSf4aefAhs3ApcvN9w3cKAYbiZOBLy8pKuRiMgKMdQ0weJCzeXLxr0xRUVGd6d7dsF/gwfjiUWz0OmhB6x7NMZUNTXA9u1iwNm2DaitFbfb2ACPPCIGnEce4c+UiKgdMNQ0wexDjV4PpKY2jMYcOmQ8GuPsbBiNeU/VHW+crsSDod5YOy1CupotQUkJ8H//Jwac1NSG7e7u4sjN1KnA4MHiKehERNTqGGqaYJah5vJlcRSmfjSmuNj4/r59xSmlMWPEM3iUSuj0Aoa/8TPyS6uwetJAPNLPV5raLdHp02K4Wb8eyM9v2N67t3hq+JNPAv5msGozEZEZYahpglmEmvrRmO3bG0Zjbn57bhqNwZ/+1OTy/3vOleDJjw/CRWWDQ69EQ2WraMcXYCXq6oBdu8SAs2ULUFUlbpfLxfdn6lTg8ccBBwcpqyQisgimfH7znFWplZQY98bcOhoTFtZwptKN0Zg7qb94ZWx/PwaatmJjI4bKP/1JXE35iy/EgLN3r/he/vSTuF7PE0+IAWf4cE5PERG1gxYt2bp69WoEBQVBpVJBo9Hg0KFDd9w/OTkZISEhsLe3R0BAAObPn4+q+r9uASxduhQymczoFhoaanSMqqoqxMfHw8PDA05OToiLi0PhrQvHmQO9XhyBWbYMGDIE8PYGJk8WpzSKi8XRmHHjgA8/BHJygBMngDfeAEaOvGugKauqxQ+nxGmR8YOacVkEuneursCsWcCePUBmJrB4MRAUJC7u9/HH4uUYuncX3+8LF6SulojIopk8UrNp0yYkJCTg/fffh0ajQXJyMmJiYpCRkQFv78ZL8W/cuBELFizA2rVrMXToUJw9exbTpk2DTCbDypUrDfv16dMHu3btaijsloXP5s+fj23btuHLL7+Eq6sr5syZg3HjxmHv3r2mvoT2V1Ji3BtTUmJ8f79+xqMxtrYtepofThagqlaPbl6OCA9wu/e6yTTBwWJ4WbIE+O03cfTmyy+B8+eBpUvF24gR4ujN+PHiaA6RJRME8ezBqiqgurrhdvP3zfn6dvcJAtC/v/h7MyKCU75kek+NRqNBREQEVq1aBQDQ6/UICAjA3LlzsWDBgkb7z5kzB2lpaUhJSTFs+9vf/oaDBw9iz549AMSRmi1btuD48eNNPmdpaSm8vLywceNGjB8/HgCQnp6OXr16Yf/+/RgyZMhd627XnhqdDjhypOFMpcOHjXtjXFzEFWvre2Naqbn0L//ej0MXruB/Y0IQ/wCvZ9QhVFQA33wjBpyUlIZ/B/b24ojc1KniVcQVnCqkdlJcDJw5I66sbWqIaMl+7cXGBhgwQAw4w4aJ/2XjfvspLxevJWhnB0RGtuqh26ynpqamBqmpqVi4cKFhm1wuR3R0NPbv39/kY4YOHYr169fj0KFDiIyMxPnz57F9+3Y89dRTRvudO3cOfn5+UKlUiIqKQlJSEgJvLBKXmpqK2tpaREdHG/YPDQ1FYGBgs0NNmysuNh6NuXnxNqBhNGbMGCAqqsWjMbeTfbkShy5cgUwGjBvI/5E7DEdH8ayoJ58UpxPXrxcDTkaGuJLxhg3iL9761Yt79ZK6YrIkej2QliZe92zvXvG/585JV4+trfihp1KJ/73T1825r6ZGnM7fu1c8I/HwYfH2zjvi83XpYhxywsJ4+ZPWUB9gjhwRT25JTRV/pwkCMHYssHmzZKWZ9O6WlJRAp9NBrVYbbVer1UhPT2/yMZMmTUJJSQmGDx8OQRBQV1eH2bNn4+WXXzbso9Fo8MknnyAkJAT5+flYtmwZ7rvvPpw6dQrOzs4oKCiAUqmE2y0XG1Sr1SgoKGjyeaurq1F9018J2psvYNiaUlKAl19ut9GY26lvEB7e3RO+rvZt+lzUQgEBwMKFwIIF4i/iTz8F/vMfIDdX7Jt64w1xCH3qVOCvfwU8PKSumMxNRYX4b6s+xOzfD1y71ni/4GDxd9S9hgtT9lMqxTME24IgAJcuia+7/rWfOCFuu3RJXGsKEFdX12gags6QIWJfHN1eWZkYYOrDy80B5lb+/mKfqITaPLL++uuvWLFiBdasWQONRoPMzEw8//zzeO2117Bo0SIAwOjRow379+vXDxqNBl26dMEXX3yBmTNntuh5k5KSsGzZslZ5DXdkayv+EgHEud363pg2GI25Hb1ewOZjYqiJG8gG4Q5PJhN/sWo0wNtvA99/Lwac+qnKw4eB+fOB2Fix96Z/f6BHj3b790RmJCfHeBTm+HFx+vtmDg7idED9aEVUFNCpkyTlthmZTGzQDwoCJk0St5WViZeMqf/5HDggNvCnpIi3+sf17Ws8mtOtm/WerXhrgDlyBDh7tukA07kzMGiQ8e2WAQ8pmBRqPD09oVAoGp11VFhYCB8fnyYfs2jRIjz11FN4+umnAQBhYWGoqKjAM888g1deeQXyJpK7m5sbevbsiczMTACAj48PampqcO3aNaPRmjs978KFC5GQkGD4XqvVIqCJdV3uWVQUsHYtEBMD+Pm1/vGb4fDFK8i5ch1OdjaI6dP0z4M6KDs7MbiMHw8UForXnfr0U+D338Uh3PphXFtbIDRU/AXcp4/43759ga5d2+6vX+pY6urEfxc3h5icnMb7de7c8AE9bJg49W2Ngbh+Xa/6tgWdTlxA8+bRnPPngZMnxdu//y3up1aLP7v6n9/AgeL/p5amPsDcPIXUnAAzeLD4M+kAAaYpJoUapVKJQYMGISUlBY8//jgAsVE4JSUFc+bMafIxlZWVjYKL4kZT5O16lMvLy5GVlWXouxk0aBBsbW2RkpKCuLg4AEBGRgays7MRFRXV5DHs7Oxg1x7/EG1tgenT2/557qB+6mlMmA/slWw4NVtqtThCM3+++OH12WfiL95Tp8RphfpfvjdzcBBXNK4POfWBx9/fev/atBRXr4qjC/UB5uBBoLLSeB+FQhzJuznEtMUfb5ZAoRADXr9+wOzZ4raCAuOQk5oq/nHxzTfiDRADzeDBDUFn6FDJp1hMptU2nkK6U4AZPNh4BMaMXq/J008JCQmYOnUqBg8ejMjISCQnJ6OiogLTb3ywT5kyBf7+/khKSgIAxMbGYuXKlRgwYIBh+mnRokWIjY01hJsXXngBsbGx6NKlC/Ly8rBkyRIoFApMnDgRAODq6oqZM2ciISEB7u7ucHFxwdy5cxEVFdUxmoQlVFlTh20n6tem4S8zi9G/P/DWW+LXej2QnS2Gm1OnxL82T50SG0ArK8W/tI4cMX68q2tD0Lk57PAq4x2TIIjrHN08CnP6dOP93NzE0eH6ABMRIfaJUMv4+IhnIY4bJ35fVSV+4Ne/B3v3iktw7N0r3up1724cJHv16jgjprcGmCNHxObwpgJMQEDjKSQzCjBNMTnUTJgwAcXFxVi8eDEKCgoQHh6OHTt2GJqHs7OzjUZmEhMTIZPJkJiYiNzcXHh5eSE2NhbLly837PPHH39g4sSJuHz5Mry8vDB8+HAcOHAAXjf9An777bchl8sRFxeH6upqxMTEYM2aNffy2i3Cj6cLUFGjQ6C7AyKCLGyenERyeUO/wKOPNmyvqwOysoyDzqlT4l9gpaWNfxED4i+sW8NOnz5slmxvt3547tvXeDVxoGN/eFoilUr8OQ8bJn5fHzZvfp9Onxa3ZWaKU8WAcdgcOlTsl3N0bPt66wPMrVNITbk5wNRPIZl5gGkKr/1k5p786CD2ZJZgXnQPzIvuKXU51BFUV4tnJ9wads6fv/1jAgIaj+r06sXFzFpLYaHxKExqqng68s2USnHkpT7AREVZ5IeO2ZNqWlCrBY4ebTyF1JSAAOMpJDMPMLygZRMsMdTkXbuOYW/8DEEAfnvxAQS48wOI7qC8XJyyunUaKze36f1lMvHU31vDTs+ed71kh1XT6cTF7W6ewmgqUHp7G3/oWWpDqqWrrRVPHzelgXvoUDH03K6B+9YAUz+F1JTAwMZTSBY2zcxQ0wRLDDWrf8nEP37MgKarOzY923TDNNFdXb1qPKJz+rTYkHzrApL1bGyAkJDGYadbN+tcGfnmU4f37RPXhrl1XSyeOmxdTD3VfsAA4OLFhmmk5gSY+ikkCwswTWGoaYKlhRpBEDDqrd04X1KBN8f3w18Gs0mYWpEgAEVFjUd1Tp0SP8SbolI1fSZWQIDlfHgLgti0ffMozIkTYjP3zRwdxYXd6kOMRiP2XZB1Ki8X15+626KINwsMbDyFZAUBpikMNU2wtFBzNPsqxq3ZB3tbBQ4nRsPJjkt/UzsQBPGv0FvDzpkzYvNrU1xcxOXqLaHBtbgYyMtrvJ3L8ZMpbr58xb59YjAOCjKeQvL0lLrKDqPNrv1EHcfXqeLaNKP7+jDQUPuRycS/IAMDxeuY1dPpxL6RW8NORoY4FXPr+jrmrP7CiTf3R/DCiWQKubzhzMNZs6SuxqLw09AMVdXq8P3v4l+LcYN4WQTqABQK8VIOPXqIF7SrV1MjBpvbXKPN7Dg6AuHhPCuMqINiqDFDu9IKoa2qg5+rClHdeNFD6sCUSnEqJixM6kqIyApYwCS39amfeho70B9yuYU0YBIREd0jhhozU6Stwn/PlQDgFbmJiIhuxlBjZrYcz4VOL2BgoBu6efGaL0RERPUYasyIIAj4OlVc/ZUNwkRERMYYaszI6TwtMgrLoLSR49F+flKXQ0RE1KEw1JiRr240CD/cWw1X+9tcM4SIiMhKMdSYiZo6Pb7j2jRERES3xVBjJn7JKMKVihp4Odvhvu5cPpuIiOhWDDVmwrA2zQB/2Cj4thEREd2Kn45m4EpFDX7JKALAtWmIiIhuh6HGDHx3PBe1OgFh/q4I8XGWuhwiIqIOiaHGDHx1VJx6ihvIKwETERHdDkNNB5dRUIZTuVrYKmT4czhDDRER0e0w1HRwX98YpXkw1BvujkqJqyEiIuq4GGo6sDqdHt8cu3FZBDYIExER3RFDTQf227kSFJdVw91RiZEh3lKXQ0RE1KEx1HRg9Q3Cf+7vB6UN3yoiIqI74SdlB1VaWYudZwoBAON5WQQiIqK7YqjpoLaezENNnR6hPs7o4+cidTlEREQdHkNNB1V/Re64gZ0hk8kkroaIiKjjY6jpgLKKy3Es+xoUchkeG+AndTlERERmgaGmA9p8o0H4/p5e8HZWSVwNERGReWCo6WB0egGbj3JtGiIiIlMx1HQw+7MuI7+0Ci4qG4zqxbVpiIiImouhpoOpvyxCbH8/qGwVEldDRERkPhhqOpDy6jrsOFUAgGvTEBERmYqhpgPZfjIf12t16ObliPAAN6nLISIiMisMNR0I16YhIiJqOYaaDiLnSiUOXbgCmQwYN9Bf6nKIiIjMTotCzerVqxEUFASVSgWNRoNDhw7dcf/k5GSEhITA3t4eAQEBmD9/Pqqqqgz3JyUlISIiAs7OzvD29sbjjz+OjIwMo2OMHDkSMpnM6DZ79uyWlN8h1TcID+/uCV9Xe4mrISIiMj8mh5pNmzYhISEBS5YswdGjR9G/f3/ExMSgqKioyf03btyIBQsWYMmSJUhLS8PHH3+MTZs24eWXXzbss3v3bsTHx+PAgQPYuXMnamtr8fDDD6OiosLoWLNmzUJ+fr7h9uabb5pafoek1wuGUMO1aYiIiFrGxtQHrFy5ErNmzcL06dMBAO+//z62bduGtWvXYsGCBY3237dvH4YNG4ZJkyYBAIKCgjBx4kQcPHjQsM+OHTuMHvPJJ5/A29sbqampGDFihGG7g4MDfHx8TC25wzt88QpyrlyHk50NYvpY3usjIiJqDyaN1NTU1CA1NRXR0dENB5DLER0djf379zf5mKFDhyI1NdUwRXX+/Hls374dY8aMue3zlJaWAgDc3d2Ntm/YsAGenp7o27cvFi5ciMrKSlPK77DqR2nGhPnAXsm1aYiIiFrCpJGakpIS6HQ6qNVqo+1qtRrp6elNPmbSpEkoKSnB8OHDIQgC6urqMHv2bKPpp5vp9XrMmzcPw4YNQ9++fY2O06VLF/j5+eHEiRN46aWXkJGRgc2bNzd5nOrqalRXVxu+12q1przUdnO9RoftJ+vXpgmQuBoiIiLzZfL0k6l+/fVXrFixAmvWrIFGo0FmZiaef/55vPbaa1i0aFGj/ePj43Hq1Cns2bPHaPszzzxj+DosLAy+vr4YNWoUsrKyEBwc3Og4SUlJWLZsWeu/oFb24+kClFfXIdDdARFBnaQuh4iIyGyZNP3k6ekJhUKBwsJCo+2FhYW37XVZtGgRnnrqKTz99NMICwvD2LFjsWLFCiQlJUGv1xvtO2fOHGzduhW//PILOne+c8OsRqMBAGRmZjZ5/8KFC1FaWmq45eTkNPdltqv6tWnGDfTn2jRERET3wKRQo1QqMWjQIKSkpBi26fV6pKSkICoqqsnHVFZWQi43fhqFQuwbEQTB8N85c+bgm2++wc8//4yuXbvetZbjx48DAHx9fZu8387ODi4uLka3jibv2nXszSoBwLOeiIiI7pXJ008JCQmYOnUqBg8ejMjISCQnJ6OiosJwNtSUKVPg7++PpKQkAEBsbCxWrlyJAQMGGKafFi1ahNjYWEO4iY+Px8aNG/Htt9/C2dkZBQVij4mrqyvs7e2RlZWFjRs3YsyYMfDw8MCJEycwf/58jBgxAv369Wutn0W7++ZYLgQB0HR1R4C7g9TlEBERmTWTQ82ECRNQXFyMxYsXo6CgAOHh4dixY4eheTg7O9toZCYxMREymQyJiYnIzc2Fl5cXYmNjsXz5csM+7733HgBxgb2brVu3DtOmTYNSqcSuXbsMASogIABxcXFITExsyWvuEAThprVpePFKIiKieyYT6ueALJxWq4WrqytKS0s7xFTUseyrGLtmH+xtFTicGA0nuzbv2SYiIjI7pnx+89pPEqlvEP5TXx8GGiIiolbAUCOBqlodvv89DwAwnlNPRERErYKhRgIpaUXQVtXBz1WFqG4eUpdDRERkERhqJFDfIDx2oD/kcq5NQ0RE1BoYatpZUVkVdp8tBgCM49o0RERErYahpp19eywPOr2AgYFuCPZykrocIiIii8FQ0464Ng0REVHbYahpR6fztEgvKIPSRo5H+/lJXQ4REZFFYahpR/Vr0zzUWw1Xe1uJqyEiIrIsDDXtpKZOj++4Ng0REVGbYahpJ79mFOFKRQ28nO1wX3dPqcshIiKyOAw17cSwNs0Af9go+GMnIiJqbfx0bQdXKmrwc3oRACCOa9MQERG1CYaadvDd8VzU6gSE+bsixMdZ6nKIiIgsEkNNO/j6aC4AIG6gv8SVEBERWS6GmjaWUVCGk7mlsFXI8OdwhhoiIqK2wlDTxuobhB8I8Ya7o1LiaoiIiCwXQ00bqtPp8c0xceqJa9MQERG1LYaaNvRbZgmKy6rh7qjEyBBvqcshIiKyaAw1bejrG5dF+HN/Pyht+KMmIiJqS/ykbSOllbX46UwhAE49ERERtQeGmjay9WQeaur0CPVxRh8/F6nLISIisngMNW2kfuopbmBnyGQyiashIiKyfAw1beB8cTmOZl+DQi7DYwP8pC6HiIjIKjDUtIH6tWlG9PCEt7NK4mqIiIisA0NNK9PrBXxztH5tmgCJqyEiIrIeDDWtbP/5y8grrYKLygajenFtGiIiovbCUNPK6huEY/v7QWWrkLgaIiIi68FQ04rKq+vww6kCAEAc16YhIiJqVww1rWj7yXxcr9Whm5cjBgS4SV0OERGRVWGoaUVcm4aIiEg6DDWtJOdKJQ5euAKZDBg30F/qcoiIiKwOQ00r2XzjNO5hwZ7wdbWXuBoiIiLrw1DTCgRBMCy4x4tXEhERSYOhphUcvngV2Vcq4WRng5g+PlKXQ0REZJUYalpBfYPwmDAf2Cu5Ng0REZEUGGru0fUaHbadzAcgnvVERERE0mhRqFm9ejWCgoKgUqmg0Whw6NChO+6fnJyMkJAQ2NvbIyAgAPPnz0dVVZVJx6yqqkJ8fDw8PDzg5OSEuLg4FBYWtqT8VvXj6QKUV9ch0N0BEUHuUpdDRERktUwONZs2bUJCQgKWLFmCo0ePon///oiJiUFRUVGT+2/cuBELFizAkiVLkJaWho8//hibNm3Cyy+/bNIx58+fj++//x5ffvkldu/ejby8PIwbN64FL7l1RXZ1R8JDPfHMiG6Qy7k2DRERkVRkgiAIpjxAo9EgIiICq1atAgDo9XoEBARg7ty5WLBgQaP958yZg7S0NKSkpBi2/e1vf8PBgwexZ8+eZh2ztLQUXl5e2LhxI8aPHw8ASE9PR69evbB//34MGTLkrnVrtVq4urqitLQULi4uprxkIiIikogpn98mjdTU1NQgNTUV0dHRDQeQyxEdHY39+/c3+ZihQ4ciNTXVMJ10/vx5bN++HWPGjGn2MVNTU1FbW2u0T2hoKAIDA2/7vNXV1dBqtUY3IiIislw2puxcUlICnU4HtVpttF2tViM9Pb3Jx0yaNAklJSUYPnw4BEFAXV0dZs+ebZh+as4xCwoKoFQq4ebm1mifgoKCJp83KSkJy5YtM+XlERERkRlr87Offv31V6xYsQJr1qzB0aNHsXnzZmzbtg2vvfZamz7vwoULUVpaarjl5OS06fMRERGRtEwaqfH09IRCoWh01lFhYSF8fJpedG7RokV46qmn8PTTTwMAwsLCUFFRgWeeeQavvPJKs47p4+ODmpoaXLt2zWi05k7Pa2dnBzs7O1NeHhEREZkxk0ZqlEolBg0aZNT0q9frkZKSgqioqCYfU1lZCbnc+GkUCnGBOkEQmnXMQYMGwdbW1mifjIwMZGdn3/Z5iYiIyLqYNFIDAAkJCZg6dSoGDx6MyMhIJCcno6KiAtOnTwcATJkyBf7+/khKSgIAxMbGYuXKlRgwYAA0Gg0yMzOxaNEixMbGGsLN3Y7p6uqKmTNnIiEhAe7u7nBxccHcuXMRFRXVrDOfiIiIyPKZHGomTJiA4uJiLF68GAUFBQgPD8eOHTsMjb7Z2dlGIzOJiYmQyWRITExEbm4uvLy8EBsbi+XLlzf7mADw9ttvQy6XIy4uDtXV1YiJicGaNWvu5bUTERGRBTF5nRpzxXVqiIiIzE+brVNDRERE1FEx1BAREZFFYKghIiIii8BQQ0RERBaBoYaIiIgsgsmndJur+pO8eGFLIiIi81H/ud2ck7WtJtSUlZUBAAICAiSuhIiIiExVVlYGV1fXO+5jNevU6PV65OXlwdnZGTKZrFWPrdVqERAQgJycHK6B0wHw/ehY+H50LHw/Oh6+J3cmCALKysrg5+fX6LJLt7KakRq5XI7OnTu36XO4uLjwH2QHwvejY+H70bHw/eh4+J7c3t1GaOqxUZiIiIgsAkMNERERWQSGmlZgZ2eHJUuWwM7OTupSCHw/Ohq+Hx0L34+Oh+9J67GaRmEiIiKybBypISIiIovAUENEREQWgaGGiIiILAJDDREREVkEhpp7tHr1agQFBUGlUkGj0eDQoUNSl2S1kpKSEBERAWdnZ3h7e+Pxxx9HRkaG1GURgNdffx0ymQzz5s2TuhSrlpubiyeffBIeHh6wt7dHWFgYjhw5InVZVkmn02HRokXo2rUr7O3tERwcjNdee61Z1zei22OouQebNm1CQkIClixZgqNHj6J///6IiYlBUVGR1KVZpd27dyM+Ph4HDhzAzp07UVtbi4cffhgVFRVSl2bVDh8+jH//+9/o16+f1KVYtatXr2LYsGGwtbXFDz/8gDNnzuCtt95Cp06dpC7NKr3xxht47733sGrVKqSlpeGNN97Am2++iXfffVfq0swaT+m+BxqNBhEREVi1ahUA8fpSAQEBmDt3LhYsWCBxdVRcXAxvb2/s3r0bI0aMkLocq1ReXo6BAwdizZo1+Pvf/47w8HAkJydLXZZVWrBgAfbu3YvffvtN6lIIwKOPPgq1Wo2PP/7YsC0uLg729vZYv369hJWZN47UtFBNTQ1SU1MRHR1t2CaXyxEdHY39+/dLWBnVKy0tBQC4u7tLXIn1io+PxyOPPGL0/wlJ47vvvsPgwYPxxBNPwNvbGwMGDMCHH34odVlWa+jQoUhJScHZs2cBAL///jv27NmD0aNHS1yZebOaC1q2tpKSEuh0OqjVaqPtarUa6enpElVF9fR6PebNm4dhw4ahb9++Updjlf7zn//g6NGjOHz4sNSlEIDz58/jvffeQ0JCAl5++WUcPnwY/+///T8olUpMnTpV6vKszoIFC6DVahEaGgqFQgGdTofly5dj8uTJUpdm1hhqyCLFx8fj1KlT2LNnj9SlWKWcnBw8//zz2LlzJ1QqldTlEMSgP3jwYKxYsQIAMGDAAJw6dQrvv/8+Q40EvvjiC2zYsAEbN25Enz59cPz4ccybNw9+fn58P+4BQ00LeXp6QqFQoLCw0Gh7YWEhfHx8JKqKAGDOnDnYunUr/vvf/6Jz585Sl2OVUlNTUVRUhIEDBxq26XQ6/Pe//8WqVatQXV0NhUIhYYXWx9fXF7179zba1qtXL3z99dcSVWTd/vd//xcLFizAX//6VwBAWFgYLl26hKSkJIaae8CemhZSKpUYNGgQUlJSDNv0ej1SUlIQFRUlYWXWSxAEzJkzB9988w1+/vlndO3aVeqSrNaoUaNw8uRJHD9+3HAbPHgwJk+ejOPHjzPQSGDYsGGNljg4e/YsunTpIlFF1q2yshJyufFHsEKhgF6vl6giy8CRmnuQkJCAqVOnYvDgwYiMjERycjIqKiowffp0qUuzSvHx8di4cSO+/fZbODs7o6CgAADg6uoKe3t7iauzLs7Ozo16mRwdHeHh4cEeJ4nMnz8fQ4cOxYoVK/CXv/wFhw4dwgcffIAPPvhA6tKsUmxsLJYvX47AwED06dMHx44dw8qVKzFjxgypSzNvAt2Td999VwgMDBSUSqUQGRkpHDhwQOqSrBaAJm/r1q2TujQSBOH+++8Xnn/+eanLsGrff/+90LdvX8HOzk4IDQ0VPvjgA6lLslparVZ4/vnnhcDAQEGlUgndunUTXnnlFaG6ulrq0swa16khIiIii8CeGiIiIrIIDDVERERkERhqiIiIyCIw1BAREZFFYKghIiIii8BQQ0RERBaBoYaIiIgsAkMNERERWQSGGiIiIrIIDDVERERkERhqiIiIyCIw1BAREZFF+P/NB/6TwGE6NQAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot them out\n",
    "m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcJcHf7n7rId"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/f072e95f51bc48978225941dba218241).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Sf5UTlMZ7rId",
    "ExecuteTime": {
     "end_time": "2023-12-09T01:00:04.610641600Z",
     "start_time": "2023-12-09T00:59:58.370762100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:06<00:00, 31.61it/s]\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "\n",
    "total_out = []\n",
    "for text, mask in tqdm(test_data, total=len(test_data)):\n",
    "    text = text.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    output = best_model(text, mask)\n",
    "    pred = output.logits\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    total_out.append(pred)\n",
    "\n",
    "total_out = torch.cat(total_out).cpu().numpy().tolist()\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(total_out):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Save best model\n",
    "torch.save(best_model.state_dict(), 'ckpts/best_model.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T01:01:07.887703300Z",
     "start_time": "2023-12-09T01:01:05.318150300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: In-Context learning (32 points)\n",
    "\n",
    "In this task, you will learn how to perform sentiment classification using prompts without the need for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T02:19:08.586305Z",
     "start_time": "2023-12-09T02:18:59.040334200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\beartype\\_util\\module\\utilmodimport.py:149: BeartypeModuleUnimportableWarning: Ignoring module \"onnx\" importation exception:\n",
      "\tImportError: cannot import name 'builder' from 'google.protobuf.internal' (C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\protobuf\\internal\\__init__.py)\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyprind\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertTokenizer, BertForMaskedLM\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T02:19:12.854024100Z",
     "start_time": "2023-12-09T02:19:08.590304300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#         TODO: Design your own template(prefix) and verbalizer         #\n",
    "#########################################################################\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.prefix = 'It was [MASK] sentence'  # you can modify this line\n",
    "        self.verbalizer = {\n",
    "            'positive': 1,\n",
    "            'negative': 0,\n",
    "            'neutral': 2\n",
    "        }\n",
    "\n",
    "        self.max_seq_length = 512\n",
    "        self.batch_size = 32\n",
    "\n",
    "\n",
    "config = Config()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bert_type = 'bert-base-uncased'\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(bert_type, num_labels=3)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(bert_type)\n",
    "\n",
    "bert = model.from_pretrained(bert_type, config=bert_config).to(device)\n",
    "\n",
    "#######################################################################\n",
    "#                        End of your code                             #\n",
    "#######################################################################\n",
    "\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T02:19:12.857496100Z",
     "start_time": "2023-12-09T02:19:12.855983200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to obtaion verbalizer ids\n",
    "def obtain_verbalizer_ids(verbalizer, tokenizer):\n",
    "    verbalizer_ids = tokenizer.convert_tokens_to_ids(list(verbalizer.keys()))\n",
    "    index2ids = {i: verbalizer_ids[i] for i in range(len(verbalizer_ids))}\n",
    "    return verbalizer_ids, index2ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T02:19:12.867556300Z",
     "start_time": "2023-12-09T02:19:12.858495100Z"
    }
   },
   "outputs": [],
   "source": [
    "verbalizer_ids, index2ids = obtain_verbalizer_ids(config.verbalizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T02:19:12.889699700Z",
     "start_time": "2023-12-09T02:19:12.867556300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to concatenate prefix and text\n",
    "def concatenate_prefix(texts, config):\n",
    "    ##################################################\n",
    "    #   TODO: concatenate your own prefix and text   #                               \n",
    "    ##################################################\n",
    "    prefix_texts = [config.prefix + \" \" + text for text in texts]\n",
    "    ##################################################\n",
    "    #                 End of your code               #                               \n",
    "    ##################################################\n",
    "    return prefix_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T02:19:12.947585900Z",
     "start_time": "2023-12-09T02:19:12.882693400Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(config):\n",
    "    # ['texts', 'labels']\n",
    "    df = pd.read_csv('./twitter_sentiment/train.csv')\n",
    "    original_texts = df['text'].tolist()\n",
    "    labels = df['sentiment_label'].tolist()\n",
    "\n",
    "    texts = concatenate_prefix(original_texts, config)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "texts, labels = load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T02:19:12.955523Z",
     "start_time": "2023-12-09T02:19:12.952585500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Batching of texts and labels for training or processing in batches\n",
    "def pack_batch(texts, labels, batch_size):\n",
    "    \"\"\"\n",
    "    :param texts: list\n",
    "    :param labels: list\n",
    "    :param batch_size: int\n",
    "    :return batch_X: list\n",
    "            [[text11, text12, ...], [text21, text22, ...], ...]\n",
    "    :return batch_y: list\n",
    "            [[label11, label12, ...], [label21, label22, ...], ...]\n",
    "    :return batch_count: int\n",
    "    \"\"\"\n",
    "    assert len(texts) == len(labels)\n",
    "\n",
    "    if len(texts) % batch_size != 0:\n",
    "        flag = False\n",
    "        batch_count = int(len(texts) / batch_size) + 1\n",
    "    else:\n",
    "        flag = True\n",
    "        batch_count = int(len(texts) / batch_size)\n",
    "\n",
    "    batch_X, batch_y = [], []\n",
    "\n",
    "    if flag:\n",
    "        for i in range(batch_count):\n",
    "            batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "            batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "    else:\n",
    "        for i in range(batch_count):\n",
    "            if i == batch_count - 1:\n",
    "                batch_X.append(texts[i * batch_size:])\n",
    "                batch_y.append(labels[i * batch_size:])\n",
    "            else:\n",
    "                batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "                batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "\n",
    "    return batch_X, batch_y, batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T02:19:12.965857600Z",
     "start_time": "2023-12-09T02:19:12.955523Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_X, batch_y, batch_count = pack_batch(texts, labels, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T02:24:09.731963700Z",
     "start_time": "2023-12-09T02:19:12.969857600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[100 %] Time elapsed: 00:04:56 | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.412276 | precision: 0.579382 | recall: 0.412276 | f1: 0.410098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:04:56\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    pper = pyprind.ProgPercent(batch_count)\n",
    "    for i in range(batch_count):\n",
    "        inputs = batch_X[i]\n",
    "        labels = batch_y[i]\n",
    "\n",
    "        tokens = tokenizer.batch_encode_plus(inputs, add_special_tokens=True,\n",
    "                                             max_length=config.max_seq_length,\n",
    "                                             padding='max_length', truncation=True)\n",
    "        \n",
    "        ids = torch.tensor(tokens['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(tokens['attention_mask']).to(device)\n",
    "\n",
    "        # Shape: (batch_size, max_seq_length, vocab_size)\n",
    "        logits = bert(ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        mask_token_index = (ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "\n",
    "        # Find [MASK] logits\n",
    "        # shape: (batch_size, vocab_size)\n",
    "        masked_logits = logits[mask_token_index[0], mask_token_index[1], :]\n",
    "\n",
    "        # Extract the logits of the word in the verbalizer at the [MASK] position\n",
    "        # shape: (batch_size, verbalizer_size)\n",
    "        verbalizer_logits = masked_logits[:, verbalizer_ids]\n",
    "\n",
    "        # Construct a pseudo-distribution from the logits in these verbalizers\n",
    "        pseudo_distribution = softmax(verbalizer_logits)\n",
    "\n",
    "        #################################################################################\n",
    "        #   1. Find the index with the maximum probability in the pseudo-distribution   #\n",
    "        #   2. Convert the index to the corresponding word ID                           #\n",
    "        #   3. Convert the ID to a token                                                #\n",
    "        #   4. Find the label corresponding to the token                                #\n",
    "        #################################################################################\n",
    "        # 1. Find the index with the maximum probability in the pseudo-distribution\n",
    "        pred_indices = torch.argmax(pseudo_distribution, dim=1)\n",
    "\n",
    "        # 2. Convert the index to the corresponding word ID\n",
    "        pred_ids = [verbalizer_ids[index] for index in pred_indices]\n",
    "\n",
    "        # 3. Convert the ID to a token\n",
    "        pred_tokens = tokenizer.convert_ids_to_tokens(pred_ids)\n",
    "\n",
    "        # 4. Find the label corresponding to the token\n",
    "        pred_labels = [config.verbalizer[token] if token in config.verbalizer else -1 for token in pred_tokens]\n",
    "        pred_labels = np.array(pred_labels)\n",
    "        #################################################################################\n",
    "        #                             End of your code                                  #                                       \n",
    "        #################################################################################\n",
    "\n",
    "        predict_all = np.append(predict_all, pred_labels)\n",
    "        labels_all = np.append(labels_all, labels)\n",
    "\n",
    "        pper.update()\n",
    "\n",
    "    acc = accuracy_score(labels_all, predict_all)\n",
    "    p = precision_score(labels_all, predict_all, average=\"weighted\")\n",
    "    r = recall_score(labels_all, predict_all, average=\"weighted\")\n",
    "    f1 = f1_score(labels_all, predict_all, average=\"weighted\")\n",
    "\n",
    "    print('accuracy: %f | precision: %f | recall: %f | f1: %f' % (acc, p, r, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: LM-BFF (45 points)\n",
    "\n",
    "https://arxiv.org/pdf/2012.15723.pdf\n",
    "\n",
    "Unlike the previous task, LM-BFF can generate templates and verbalizers automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請先到共用雲端硬碟將檔案 `SST-2.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/14MDYFasXU94dUE9DjgfcZE61iTRI2007/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install openprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T03:31:13.543193800Z",
     "start_time": "2023-12-09T03:30:21.808076200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openprompt\n",
      "  Downloading openprompt-1.0.1-py3-none-any.whl (146 kB)\n",
      "     ---------------------------------------- 0.0/146.4 kB ? eta -:--:--\n",
      "     ---------------- ---------------------- 61.4/146.4 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 146.4/146.4 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting transformers>=4.10.0 (from openprompt)\n",
      "  Obtaining dependency information for transformers>=4.10.0 from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 123.5/123.5 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting sentencepiece==0.1.96 (from openprompt)\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 0.1/1.1 MB 2.2 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.2/1.1 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.3/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.4/1.1 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.5/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 0.6/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.6/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 0.7/1.1 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 0.8/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 0.9/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.0/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.1/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.1/1.1 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting tqdm>=4.62.2 (from openprompt)\n",
      "  Obtaining dependency information for tqdm>=4.62.2 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting tensorboardX (from openprompt)\n",
      "  Obtaining dependency information for tensorboardX from https://files.pythonhosted.org/packages/44/71/f3e7c9b2ab67e28c572ab4e9d5fa3499e0d252650f96d8a3a03e26677f53/tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nltk (from openprompt)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting yacs (from openprompt)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting dill (from openprompt)\n",
      "  Obtaining dependency information for dill from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting datasets (from openprompt)\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting rouge==1.0.0 (from openprompt)\n",
      "  Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting pyarrow (from openprompt)\n",
      "  Obtaining dependency information for pyarrow from https://files.pythonhosted.org/packages/28/82/9adfafaf0de581a39a1c86002cafd1a55a1255c9e0d362dc3e970aab0656/pyarrow-14.0.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading pyarrow-14.0.1-cp39-cp39-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openprompt) (1.8.0)\n",
      "Requirement already satisfied: six in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rouge==1.0.0->openprompt) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\eddie\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.62.2->openprompt) (0.4.4)\n",
      "Collecting filelock (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/fc/85/0d1038f068900896a8590d6d0da198b90d31f731a39166a432aa2b92249b/regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (2.25.1)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/9f/90/a6821e7757d2db194c16cbca78c80e206f30f6cc62c7f15fb27428f8c6dd/tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/4e/96/f4ee4434d8b6452fe7d5d44df2e72d1c6b2add1c3a5fb5c81aae83cb90c6/safetensors-0.4.1-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.1-cp39-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->openprompt)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets->openprompt) (1.4.1)\n",
      "Collecting xxhash (from datasets->openprompt)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/33/db/e07aa80e39a7ee82e9ca6f5a0fa9bf0b4465934272116a1b578eaee7f4b3/xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->openprompt)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/c6/c9/820b5ab056f4ada76fbe05bd481a948f287957d6cbfd59e2dd2618b408c1/multiprocess-0.70.15-py39-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets->openprompt)\n",
      "  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets->openprompt) (3.7.4.post0)\n",
      "Requirement already satisfied: click in c:\\users\\eddie\\appdata\\roaming\\python\\python39\\site-packages (from nltk->openprompt) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->openprompt) (1.1.0)\n",
      "Collecting protobuf>=3.20 (from tensorboardX->openprompt)\n",
      "  Obtaining dependency information for protobuf>=3.20 from https://files.pythonhosted.org/packages/b6/4b/f4f3334784576822d7817a664b757030ebb35b981978baf9c2eb3c5f33a8/protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (21.2.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (3.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (5.2.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (1.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers>=4.10.0->openprompt) (3.0.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets->openprompt) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets->openprompt) (2021.3)\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 41.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/7.9 MB 3.2 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.2/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.4/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.4/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.6/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.7/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.8/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.9/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.0/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.1/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.2/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.3/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.4/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.5/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.6/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.7/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.8/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.9/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.2/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.3/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.4/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.5/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.6/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.7/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.8/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.9/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.2/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.3/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.4/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.6/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.7/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.9/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.0/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.1/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.2/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.3/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.5/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.6/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.9/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.0/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.1/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.2/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.3/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.5/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.6/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.7/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.9/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.0/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.1/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.3/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.4/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.5/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.6/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.7/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.8/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.9/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.0/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.3/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.4/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.5/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.6/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.7/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.8/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "   ---------------------------------------- 0.0/521.2 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 92.2/521.2 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 174.1/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 276.5/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 368.6/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 471.0/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 521.2/521.2 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 61.4/115.3 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 115.3/115.3 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow-14.0.1-cp39-cp39-win_amd64.whl (24.6 MB)\n",
      "   ---------------------------------------- 0.0/24.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/24.6 MB 3.2 MB/s eta 0:00:08\n",
      "   ---------------------------------------- 0.2/24.6 MB 2.6 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 0.3/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.4/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.5/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.6/24.6 MB 2.2 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.7/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.8/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.8/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.9/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.0/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.1/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.2/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.3/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.4/24.6 MB 1.9 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.4/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.7/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.8/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.9/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.0/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.1/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.2/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.3/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.4/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.5/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.6/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.7/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.8/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.9/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 3.0/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.2/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.3/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.4/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.5/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.6/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.7/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.8/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.9/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.0/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.2/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.3/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.4/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.5/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.6/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.8/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.9/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.0/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.1/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.2/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.3/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.4/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.5/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.6/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.8/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.9/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.0/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.1/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.2/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.3/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.4/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.5/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.6/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.7/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 6.8/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 6.9/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.0/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.1/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.2/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.3/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.5/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.6/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.7/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.8/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.9/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.0/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.1/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.2/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.3/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.5/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 8.6/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.7/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.8/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.3/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.4/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.5/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.6/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.7/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.8/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 9.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.0/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.3/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.4/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 10.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.6/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.7/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.8/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.9/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 11.0/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.1/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.2/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.4/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.6/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.7/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 11.8/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 11.9/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.0/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.1/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.2/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.7/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.8/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.0/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.1/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.3/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.7/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.8/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.1/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.3/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 14.8/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 14.9/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.0/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.1/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.2/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.3/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.4/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.5/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.8/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.9/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.0/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.1/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.2/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.3/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.4/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.5/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 16.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 16.8/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 16.9/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.0/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.1/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.3/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.4/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.5/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.6/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.7/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.8/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 17.9/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.0/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.1/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.3/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.4/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.5/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.6/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.7/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.7/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.8/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 19.0/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.1/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.2/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.3/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.4/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.5/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.7/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.8/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.0/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.1/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.2/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.3/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.4/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.5/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.7/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.9/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.0/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.1/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.2/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.3/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.4/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.5/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.9/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.0/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.1/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.2/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.3/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.4/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.5/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.7/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.8/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.2/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.3/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.4/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.5/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.6/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.7/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.8/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.2/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.5/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.6/24.6 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.7 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 92.2/101.7 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 101.7/101.7 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 0.0/311.7 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 112.6/311.7 kB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 194.6/311.7 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  307.2/311.7 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 311.7/311.7 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 112.6/413.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 194.6/413.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 307.2/413.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 368.6/413.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 413.4/413.4 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 92.2/269.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 174.1/269.6 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/269.6 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 269.6/269.6 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp39-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.8 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 112.6/277.8 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 194.6/277.8 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 277.8/277.8 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.2 MB 1.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 1.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.4/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.3 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 92.2/133.3 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 133.3/133.3 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl (29 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 71.7/166.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 166.4/166.4 kB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, yacs, xxhash, tqdm, safetensors, rouge, regex, pyarrow-hotfix, pyarrow, protobuf, fsspec, filelock, dill, tensorboardX, nltk, multiprocess, huggingface-hub, tokenizers, transformers, datasets, openprompt\n",
      "Successfully installed datasets-2.15.0 dill-0.3.7 filelock-3.13.1 fsspec-2023.10.0 huggingface-hub-0.19.4 multiprocess-0.70.15 nltk-3.8.1 openprompt-1.0.1 protobuf-4.25.1 pyarrow-14.0.1 pyarrow-hotfix-0.6 regex-2023.10.3 rouge-1.0.0 safetensors-0.4.1 sentencepiece-0.1.96 tensorboardX-2.6.2.2 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.35.2 xxhash-3.4.1 yacs-0.1.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install openprompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import openprompt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.181341Z",
     "start_time": "2023-12-10T03:35:52.654085900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.194342100Z",
     "start_time": "2023-12-10T03:36:10.182341900Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda = True\n",
    "auto_t = True # Whether to perform automatic template generation\n",
    "auto_v = True # Whether to perform automatic verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.226340200Z",
     "start_time": "2023-12-10T03:36:10.195343500Z"
    }
   },
   "outputs": [],
   "source": [
    "from openprompt.data_utils.text_classification_dataset import SST2Processor\n",
    "dataset = {'train': SST2Processor().get_train_examples(\"SST-2/\"),\n",
    "           'validation': SST2Processor().get_dev_examples(\"SST-2/\"),\n",
    "           'test': SST2Processor().get_test_examples(\"SST-2/\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.607530300Z",
     "start_time": "2023-12-10T03:36:10.228340900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: {\n",
      "  \"guid\": \"train-0\",\n",
      "  \"label\": 0,\n",
      "  \"meta\": {\n",
      "    \"labelword\": \"terrible\"\n",
      "  },\n",
      "  \"text_a\": \"nothing happens , and it happens to flat characters .\",\n",
      "  \"text_b\": \"\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print('load model...')\n",
    "from openprompt.plms import load_plm\n",
    "# load mlm model for main tasks\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")\n",
    "\n",
    "# load generation model for template generation\n",
    "template_generate_model, template_generate_tokenizer, template_generate_model_config, template_tokenizer_wrapper = load_plm('t5', 't5-large')\n",
    "\n",
    "from openprompt.prompts import ManualVerbalizer, ManualTemplate\n",
    "verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "#   TODO: You need to switch LMBFFTemplateGenerationTemplate or ManualTemplate to                                 #\n",
    "#         compare auto generate template and manual generate template                                             #\n",
    "###################################################################################################################\n",
    "from openprompt.prompts.prompt_generator import LMBFFTemplateGenerationTemplate\n",
    "########################################\n",
    "#   LMBFFTemplateGenerationTemplate    #\n",
    "########################################\n",
    "import random\n",
    "\n",
    "# number of demonstrations\n",
    "num_demonstrations = 1  # try different number\n",
    "\n",
    "demonstrations = []\n",
    "\n",
    "for _ in range(num_demonstrations):\n",
    "    # random choice training set example with label 0 \n",
    "    random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n",
    "\n",
    "    # random choice training set example with label 1\n",
    "    random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n",
    "    \n",
    "    demonstration = f'{random_example_1.text_a} It was terrible. {random_example_2.text_a} It was great.'\n",
    "    demonstrations.append(demonstration)\n",
    "\n",
    "# You can modify the demonstrations and try different combinations\n",
    "template_text = '{\"placeholder\": \"text_a\"} {\"mask\"} {\"meta\": \"labelword\"} {\"mask\"}.' + ' '.join(demonstrations)\n",
    "template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "\n",
    "#############################################\n",
    "#   End of LMBFFTemplateGenerationTemplate  #\n",
    "#############################################\n",
    "\n",
    "########################################\n",
    "#          ManualTemplate              #\n",
    "########################################\n",
    "\n",
    "#template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} It was {\"mask\"}.')\n",
    "\n",
    "#############################################\n",
    "#          End of ManualTemplate            # \n",
    "#############################################\n",
    "\n",
    "###################################################################################################################\n",
    "#                                           End of your code                                                      #\n",
    "###################################################################################################################\n",
    "\n",
    "\n",
    "# view wrapped example\n",
    "wrapped_example = template.wrap_one_example(dataset['train'][0])\n",
    "print(\"dataset:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.673523500Z",
     "start_time": "2023-12-10T03:36:46.625523700Z"
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "\n",
    "# Returns the best evaluation score achieved during training\n",
    "def fit(model, train_dataloader, val_dataloader, loss_func, optimizer, epochs=5):\n",
    "    best_score = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_dataloader, loss_func, optimizer)\n",
    "        score = evaluate(model, val_dataloader)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "        print(f\"Epoch {epoch+1}: Train loss={train_loss}, Eval score={score}\")\n",
    "    return best_score\n",
    "\n",
    "# Trains the model on the training data and computes the training loss\n",
    "def train_epoch(model, train_dataloader, loss_func, optimizer):\n",
    "    model.train()\n",
    "    loss_all = []\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits #\n",
    "        # 2. Get labels                                     #\n",
    "        # 3. Evalutate using loss_func                      #\n",
    "        # 4. Append loss to loss_all                        #\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits\n",
    "        logits = model(batch=inputs)\n",
    "\n",
    "        # 2. Get labels\n",
    "        labels = inputs['label']\n",
    "\n",
    "        # 3. Evalutate using loss_func\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Append loss to loss_all\n",
    "        loss_all.append(loss.item())\n",
    "        #####################################################\n",
    "        #                 End of your code                  #\n",
    "        #####################################################\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return np.mean(loss_all)\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    with torch.no_grad():\n",
    "        for step, inputs in enumerate(val_dataloader):\n",
    "            if cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits #\n",
    "            # 2. Get labels                                     #\n",
    "            # 3. Extend labels to list                          #\n",
    "            # 4. Get predictions and extend preds to list       #\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits\n",
    "            logits = model(batch=inputs)\n",
    "\n",
    "            # 2. Get labels\n",
    "            labels = inputs['label']\n",
    "\n",
    "            # 3. Extend labels to list\n",
    "            alllabels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # 4. Get predictions and extend preds to list\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            allpreds.extend(preds.cpu().numpy())\n",
    "            #####################################################\n",
    "            #                 End of your code                  #\n",
    "            #####################################################\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generated template from TemplateGenerator and find the best template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:13:36.594464200Z",
     "start_time": "2023-12-10T03:36:46.673523500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 1454.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:37<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"placeholder\": \"text_a\"} It was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} it was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001B[A\n",
      "tokenizing: 32it [00:00, 202.53it/s]A\n",
      "\n",
      "tokenizing: 32it [00:00, 727.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.562330920593297, Eval score=0.5\n",
      "Epoch 2: Train loss=1.029085214715451, Eval score=0.5\n",
      "Epoch 3: Train loss=0.671642615867313, Eval score=0.71875\n",
      "Epoch 4: Train loss=0.2566610455396585, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [15:59<1:03:57, 959.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.2469192902863142, Eval score=0.78125\n",
      "Current template: {\"placeholder\": \"text_a\"} it was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' it was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 379.37it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.511358927668084, Eval score=0.5\n",
      "Epoch 2: Train loss=0.7635227439459413, Eval score=0.5\n",
      "Epoch 3: Train loss=0.3154368309772053, Eval score=0.53125\n",
      "Epoch 4: Train loss=0.8977778584977472, Eval score=0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [36:43<56:21, 1127.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.7606429383413342, Eval score=0.5\n",
      "Current template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 571.42it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1180.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.765889931773188, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.5693292848736746, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.041312409681268036, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.2322409824218994, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [56:36<38:33, 1156.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.22881872234574985, Eval score=0.84375\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 1066.65it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1523.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.7746381501195, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9883591458201408, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8319057337939739, Eval score=0.625\n",
      "Epoch 4: Train loss=0.508084288907412, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [1:16:21<19:27, 1167.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.39746711112820776, Eval score=0.53125\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 864.68it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1454.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.6042319842349002, Eval score=0.5\n",
      "Epoch 2: Train loss=1.069442194304429, Eval score=0.5\n",
      "Epoch 3: Train loss=0.47684725234284997, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.2768472472046142, Eval score=0.59375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:36:08<00:00, 1153.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.07557142606344769, Eval score=0.625\n",
      "Final best template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class ManualTemplateWithoutParse(ManualTemplate):\n",
    "    \"\"\"The generated template from TemplateGenerator is a list of dict of parsed template_text. So no further parsing is needed.\"\"\"\n",
    "    def on_text_set(self):\n",
    "        pass\n",
    "\n",
    "# Template generation\n",
    "if auto_t:\n",
    "    print('performing auto_t...')\n",
    "\n",
    "    if cuda:\n",
    "        template_generate_model = template_generate_model.cuda()\n",
    "    template_generator = T5TemplateGenerator(template_generate_model, template_generate_tokenizer, template_tokenizer_wrapper, verbalizer, beam_width=5) # Beam_width is set to 5 here for efficiency; to improve performance, try a larger number.\n",
    "\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=template_generate_tokenizer, tokenizer_wrapper_class=template_tokenizer_wrapper, batch_size=len(dataset['train']), decoder_max_length=128, max_seq_length=128, shuffle=False, teacher_forcing=False) # Register all data at once\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        template_generator._register_buffer(data)\n",
    "\n",
    "    template_generate_model.eval()\n",
    "    print('generating...')\n",
    "    template_texts = template_generator._get_templates()\n",
    "\n",
    "    original_template = template.text\n",
    "    template_texts = [template_generator.convert_template(template_text, original_template) for template_text in template_texts]\n",
    "    # template_generator._show_template()\n",
    "    template_generator.release_memory()\n",
    "    # Generate a number of candidate template text\n",
    "    print(template_texts)\n",
    "    \n",
    "    # Iterate over each candidate and select the best one\n",
    "    best_metrics = 0.0\n",
    "    best_template_text = None\n",
    "    for template_text in tqdm(template_texts):\n",
    "        verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "        template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "        print(f\"Current template: {template_text}\\n\\nWrapped example: {template.wrap_one_example(dataset['train'][0])}\")\n",
    "\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "        \n",
    "        #######################################################\n",
    "        # TODO: Use score to Find your best template_text     #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_template_text = template_text\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # Use the best template\n",
    "    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "    template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=best_template_text)\n",
    "    print(\"Final best template:\", best_template_text)\n",
    "    print()\n",
    "    print(\"Wrapped example:\", template.wrap_one_example(dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verbalizer template from VerbalizerGenerator and find the best verbalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T11:58:04.838076700Z",
     "start_time": "2023-12-10T05:13:36.638466100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_v...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 1391.30it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current label_words: ['terrible', 'beautiful']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001B[A\n",
      "tokenizing: 3it [00:00, 21.48it/s]\u001B[A\n",
      "tokenizing: 6it [00:01,  5.15it/s]\u001B[A\n",
      "tokenizing: 8it [00:01,  6.27it/s]\u001B[A\n",
      "tokenizing: 32it [00:01, 23.45it/s]\u001B[A\n",
      "\n",
      "tokenizing: 32it [00:00, 969.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.1593105591260544, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.5407680265489034, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.17979280870349612, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.006995583800744498, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [21:05<6:40:40, 1265.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.002051841642924046, Eval score=0.84375\n",
      "current label_words: ['horrible', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 695.63it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1388.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.5843039118335565, Eval score=0.84375\n",
      "Epoch 2: Train loss=0.0798521326687478, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.0026221817748819376, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.0010607897513068565, Eval score=0.78125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [42:02<6:18:07, 1260.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0004625524882726495, Eval score=0.78125\n",
      "current label_words: ['horrible', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 762.07it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1333.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.0936600251085586, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.849827597849071, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.414547988853883, Eval score=0.75\n",
      "Epoch 4: Train loss=0.0500250239797424, Eval score=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [1:02:46<5:55:04, 1253.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.004471211226245941, Eval score=0.8125\n",
      "current label_words: ['sad', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 415.60it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 941.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4185917818103917, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.5065413688316767, Eval score=0.875\n",
      "Epoch 3: Train loss=0.012464412650388113, Eval score=0.875\n",
      "Epoch 4: Train loss=0.002708091426967485, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [1:23:35<5:33:44, 1251.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0004699288858773798, Eval score=0.875\n",
      "current label_words: ['bad', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 551.73it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=3.1230944280390602, Eval score=0.5\n",
      "Epoch 2: Train loss=1.270681380527094, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8551086119841784, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.6224154182709754, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [1:45:07<5:16:30, 1266.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.22766433987999335, Eval score=0.8125\n",
      "current label_words: ['horrible', 'remarkable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 426.62it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1031.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.2276666148868003, Eval score=0.6875\n",
      "Epoch 2: Train loss=0.4889321715090773, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2634941844644345, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.25736280560994373, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [2:07:36<5:01:58, 1294.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.3903749104914027, Eval score=0.625\n",
      "current label_words: ['terrible', 'brilliant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 493.33it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.128300400949797, Eval score=0.5\n",
      "Epoch 2: Train loss=1.0674331626432831, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.6011688623111695, Eval score=0.875\n",
      "Epoch 4: Train loss=0.6810656589186692, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [2:28:49<4:38:53, 1287.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.1570308672144165, Eval score=0.84375\n",
      "current label_words: ['terrible', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 451.65it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1033.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.1662085960851982, Eval score=0.84375\n",
      "Epoch 2: Train loss=0.046194135922632995, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.20158991879031873, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.009880203132524912, Eval score=0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [2:51:16<4:21:16, 1306.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.27703922392970526, Eval score=0.90625\n",
      "current label_words: ['disappointing', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 415.57it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1032.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.3897865504303581, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.6985758165101288, Eval score=0.5625\n",
      "Epoch 3: Train loss=0.20094251398768392, Eval score=0.875\n",
      "Epoch 4: Train loss=0.021359096241212683, Eval score=0.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [3:11:17<3:53:28, 1273.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0014102687238164435, Eval score=0.90625\n",
      "current label_words: ['strange', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 410.23it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4916955009493904, Eval score=0.5\n",
      "Epoch 2: Train loss=0.5601507005485473, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.050391235166898696, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.04447055474645367, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [3:31:10<3:28:06, 1248.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.5673459974156145, Eval score=0.78125\n",
      "current label_words: ['terrible', 'good']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 363.21it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.9351653824437118, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9128216816243366, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.16854792425147025, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.004711857527581742, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [3:50:34<3:03:23, 1222.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0015127657302400621, Eval score=0.84375\n",
      "current label_words: ['bad', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 524.59it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.8022562539517715, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9302898038731655, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.43625156552297994, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.04196147369020764, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [4:10:06<2:40:57, 1207.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.012102704274127518, Eval score=0.65625\n",
      "current label_words: ['short', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 412.44it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1203.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.8797737168388267, Eval score=0.75\n",
      "Epoch 2: Train loss=0.3757321042244257, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2311989847357836, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.43366863449273296, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [4:30:04<2:20:30, 1204.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.620734619528541, Eval score=0.59375\n",
      "current label_words: ['disappointing', 'delightful']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 426.69it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1143.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.324861448905718, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.5381804386270232, Eval score=0.5625\n",
      "Epoch 3: Train loss=0.36967586419450527, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.12384688404563349, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [4:49:18<1:58:55, 1189.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.18691727038913086, Eval score=0.6875\n",
      "current label_words: ['bad', 'brilliant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 490.61it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1103.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.5005056243027184, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.7745201710495166, Eval score=0.875\n",
      "Epoch 3: Train loss=0.13810731278863386, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.004411970109288177, Eval score=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [5:08:29<1:38:08, 1177.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.00313117326692236, Eval score=0.78125\n",
      "current label_words: ['bad', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 492.79it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.182302282977588, Eval score=0.5\n",
      "Epoch 2: Train loss=0.5332906207768247, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.08191546555644891, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.35411459776105403, Eval score=0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [5:26:58<1:17:08, 1157.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.13951104862388775, Eval score=0.46875\n",
      "current label_words: ['horrible', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 466.99it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1219.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4121702450024713, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.32750329683221935, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.01347528245059948, Eval score=0.90625\n",
      "Epoch 4: Train loss=0.3011642301885331, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [5:45:44<57:23, 1147.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.022153147599055956, Eval score=0.875\n",
      "current label_words: ['fine', 'remarkable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 492.08it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1185.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.7259275259780793, Eval score=0.5\n",
      "Epoch 2: Train loss=0.7986743192886934, Eval score=0.65625\n",
      "Epoch 3: Train loss=0.6269949703128077, Eval score=0.5\n",
      "Epoch 4: Train loss=0.16056611831118062, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [6:04:43<38:10, 1145.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.003870869544044808, Eval score=0.78125\n",
      "current label_words: ['disappointing', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 454.61it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1183.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4480454477061357, Eval score=0.5\n",
      "Epoch 2: Train loss=0.8791331336833537, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.4247932309117459, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.2258107879970339, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [6:23:21<18:56, 1137.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0734178872121447, Eval score=0.875\n",
      "current label_words: ['terrible', 'fine']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 489.28it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1164.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.6715138442009447, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.6137157797584223, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.38552796499482156, Eval score=0.875\n",
      "Epoch 4: Train loss=0.34600197029577373, Eval score=0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [6:41:58<00:00, 1205.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.6120385159649686, Eval score=0.78125\n",
      "final best label words: ['terrible', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verbalizer generation\n",
    "from openprompt.prompts.prompt_generator import RobertaVerbalizerGenerator\n",
    "if auto_v:\n",
    "    print('performing auto_v...')\n",
    "    # Load generation model for verbalizer generation\n",
    "    if cuda:\n",
    "        plm = plm.cuda()\n",
    "    verbalizer_generator = RobertaVerbalizerGenerator(model=plm, tokenizer=tokenizer, candidate_num=20, label_word_num_per_class=20)\n",
    "    # To improve performance, try larger numbers\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, batch_size=32)\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        verbalizer_generator.register_buffer(data)\n",
    "    label_words_list = verbalizer_generator.generate()\n",
    "    verbalizer_generator.release_memory()\n",
    "\n",
    "    # Iterate over each candidate and select the best one\n",
    "    current_verbalizer = copy.deepcopy(verbalizer)\n",
    "    best_metrics = 0.0\n",
    "    best_label_words = None\n",
    "    for label_words in tqdm(label_words_list):\n",
    "        print(f\"current label_words: {label_words}\")\n",
    "        current_verbalizer.label_words = label_words\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, current_verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "\n",
    "        #######################################################\n",
    "        # TODO: Use score to find your best_label_word        #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_label_words = label_words\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # use the best verbalizer\n",
    "    print(\"final best label words:\", best_label_words)\n",
    "    verbalizer = ManualVerbalizer(tokenizer, num_classes=2, label_words=best_label_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:18:43.541949400Z",
     "start_time": "2023-12-10T11:58:04.849042700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 383.01it/s]\n",
      "tokenizing: 32it [00:00, 1142.78it/s]\n",
      "tokenizing: 872it [00:00, 1095.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.2980121186483302, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.38737686289732665, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.08258779709103692, Eval score=0.875\n",
      "Epoch 4: Train loss=0.3668047572554656, Eval score=0.84375\n",
      "Epoch 5: Train loss=0.00465579945631589, Eval score=0.84375\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "test_dataloader = PromptDataLoader(dataset['test'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "\n",
    "model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# It's always good practice to set no decay to bias and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/5b8876ed26fd495b8353ad7ce94b6f65)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:46:57.884699300Z",
     "start_time": "2023-12-10T12:18:43.474013600Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "allpreds = []\n",
    "for step, inputs in tqdm(enumerate(test_dataloader)):\n",
    "    if cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = model(inputs)\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(allpreds):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report (15 points)\n",
    "\n",
    "- Task 1: Compare **two** different models you employed and provide a brief discussion of your implementation.\n",
    "\n",
    "- Task 2: You need to try at least **three** different templates and verbalizers to compare how your prompts work with the model. Report your performance in zero-shot, one-shot, and few-shot scenarios, with examples drawn from the training set.\n",
    "\n",
    "- Task 3: Try at least three different manually crafted templates to compare them with auto-generated templates. Evaluate the performance with different numbers of demonstrations and plot the graph from Figure 3 in the paper (https://arxiv.org/pdf/2012.15723.pdf). Also, report your best template and verbalizer. . \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
