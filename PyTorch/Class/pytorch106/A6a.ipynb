{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6Z1Snuk7rIK"
   },
   "source": [
    "# MIS 583 Assignment 6: Text Sentiment Classification with Prompt Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSwr9MgZogRZ"
   },
   "source": [
    "Before we start, please put your name and SID in following format: <br>\n",
    ": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷, M114020035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DzsjuDhlz_k"
   },
   "source": [
    "**Your Answer:**   \n",
    "Hi I'm 鄔仁迪, B104020009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d-Zzebq7rIM"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kc9gd_Wk7rIN"
   },
   "source": [
    "**Sentiment Classification** is an automated process of identifying opinions in text and labeling them as positive or negative based on the emotions customers express within them.\n",
    "\n",
    "In Task 1, you need to fine-tune a pre-trained language model (e.g., BERT) to predict the sentiment of given tweets.\n",
    "\n",
    "In Task 2, we employ prompts to enable the model to perform sentiment analysis through in-context learning, eliminating the need for additional training.\n",
    "\n",
    "In Task 3, you will use the method called LM-BFF to utilize the model in generating the optimal template and verbalizer autonomously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice \n",
    "**You are not allow to use the model like GPT family or pre-trained weight using SST-2 and twitter dataset!!!!!!!!!!!!!!!!!**\n",
    "\n",
    "You can use BERT and RoBERTa encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giUId1Naqacs",
    "tags": []
   },
   "source": [
    "##  Versions of used packages\n",
    "\n",
    "We will check PyTorch version to make sure everything work properly.  \n",
    "We use `python==3.7.14`, `torch==1.12.1+cu113` and `torchvision==0.13.1+cu113`.  \n",
    "This is the default version in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:11.995277800Z",
     "start_time": "2023-12-15T17:22:01.289099200Z"
    },
    "id": "Vuw-gNvjqcYe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]\n",
      "torch 2.1.0+cu118\n",
      "torchvision 0.16.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "print('python', sys.version.split('\\n')[0])\n",
    "print('torch', torch.__version__)\n",
    "print('torchvision', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Text Sentiment Classification (40 points)\n",
    "\n",
    "In this task, you need to fine-tune a pre-trained language model (e.g., BERT or RoBERTa encoder) to predict the sentiment of given tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a4s_a5D7rIR"
   },
   "source": [
    "## Loading Model and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPUkTbnL7rIR"
   },
   "source": [
    "First, let's talk about the model. The Hugging Face team has created an amazing framework called \"transformers\" for NLP tasks. It includes many state-of-the-art machine learning models for PyTorch, TensorFlow, and JAX.\n",
    "\n",
    "To start with this package, follow [this link to installation and a basic tutorial](https://pytorch.org/hub/huggingface_pytorch-transformers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:12.037278200Z",
     "start_time": "2023-12-15T17:22:11.997299100Z"
    },
    "id": "rK0ouXa09pDU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!echo happy installation\\n!pip -V\\n!pip install grpcio\\n!pip install google-auth\\n!pip install protobuf==3.9.2\\n!pip install pyprind\\n!pip install tqdm boto3 requests regex sentencepiece sacremoses'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you might need some additional installations there\n",
    "\"\"\"!echo happy installation\n",
    "!pip -V\n",
    "!pip install grpcio\n",
    "!pip install google-auth\n",
    "!pip install protobuf==3.9.2\n",
    "!pip install pyprind\n",
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.040759300Z",
     "start_time": "2023-12-15T17:22:12.010275100Z"
    },
    "id": "dmGCAevi7rIS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "#########################################################################\n",
    "#            Loading tokenizer and model from transformer               #\n",
    "#########################################################################\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, AutoConfig\n",
    "\n",
    "bert_type = 'bert-base-cased'\n",
    "\n",
    "# ---------- 1. load from torch.hub ----------\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_type)\n",
    "\n",
    "# create a Bert-extended task (classification)\n",
    "model = BertForSequenceClassification.from_pretrained(bert_type)\n",
    "\n",
    "# finetune from the output from bert to your task\n",
    "model.classifier = nn.Linear(in_features=768, out_features=3, bias=True)\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiMThsYeDa2O"
   },
   "source": [
    "## How to Get Data\n",
    "\n",
    "Please open the file `twitter_sentiment.zip`, creat shortcut to your Google Drive.\n",
    "\n",
    "1. open [LINK of Google Drive](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
    "3. Select the location where you want to place the shortcut.\n",
    "4. Click Add shortcut.\n",
    "\n",
    "After above procedures, we have a shortcut of zip file of dataset.  \n",
    "We can access this in colab after granting the permission of Google Drive.\n",
    "\n",
    "---\n",
    "\n",
    "請先到共用雲端硬碟將檔案 `twitter_sentiment.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.052762Z",
     "start_time": "2023-12-15T17:22:15.012758200Z"
    },
    "id": "lZnFgi5i_2oA"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqO8DiB6VRQZ"
   },
   "source": [
    "## Unzip Data\n",
    "\n",
    "解壓縮 `twitter_sentiment.zip` 後可以發現裡面有三個csv檔。\n",
    "\n",
    "- `train.csv`, `test.csv` and `val.csv`\n",
    "\n",
    "Training set 有 **10248** 筆資料.  \n",
    "Validation set 有 **1317** 筆資料.  \n",
    "Testing set 有 **3075** 筆資料.  \n",
    "\n",
    "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.052762Z",
     "start_time": "2023-12-15T17:22:15.040759300Z"
    },
    "id": "OSlTMdxf8Zd7"
   },
   "outputs": [],
   "source": [
    "# !unzip -qq ./drive/MyDrive/twitter_sentiment.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.065760100Z",
     "start_time": "2023-12-15T17:22:15.045759400Z"
    },
    "id": "wf5GXTme7rIT"
   },
   "outputs": [],
   "source": [
    "# Utility function to extract text and label from csv file\n",
    "def get_texts(f_name='./twitter_sentiment', mode='train'):\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "\n",
    "    f_path = os.path.join(f_name, '{}.csv'.format(mode))\n",
    "    with open(f_path, encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for line in reader:\n",
    "            text_list.append(line['text'])\n",
    "            if mode != 'test':\n",
    "                label_list.append(int(line['sentiment_label']))\n",
    "\n",
    "    return text_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.102758500Z",
     "start_time": "2023-12-15T17:22:15.061758900Z"
    },
    "id": "6fpY0ZrK7rIV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, f_name='./twitter_sentiment', mode='train'):\n",
    "        self.mode = mode\n",
    "\n",
    "        text_list, label_list = get_texts(f_name, mode)\n",
    "        print('mode', mode, 'has', len(text_list), 'datas')\n",
    "        text_list = tokenizer(text_list,\n",
    "                             truncation=True, padding=True,\n",
    "                             return_tensors='pt')\n",
    "\n",
    "        self.text_list = text_list['input_ids']\n",
    "        self.mask_list = text_list['attention_mask']\n",
    "\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_list[idx]\n",
    "        mask = self.mask_list[idx]\n",
    "        if self.mode == 'test':\n",
    "            return text, mask\n",
    "        label = torch.tensor(self.label_list[idx])\n",
    "        return text, mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.910260600Z",
     "start_time": "2023-12-15T17:22:15.074759400Z"
    },
    "id": "nCmM4FSw7rIW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode train has 10248 datas\n",
      "mode val has 1317 datas\n",
      "mode test has 3075 datas\n"
     ]
    }
   ],
   "source": [
    "dataset_train = TwitterDataset(mode='train')\n",
    "dataset_val = TwitterDataset(mode='val')\n",
    "dataset_test = TwitterDataset(mode='test')\n",
    "\n",
    "batch_size = 32\n",
    "train_data = DataLoader(dataset_train, batch_size=batch_size,\n",
    "                       shuffle=True)\n",
    "val_data = DataLoader(dataset_val, batch_size=batch_size // 2,\n",
    "                       shuffle=False)\n",
    "test_data = DataLoader(dataset_test, batch_size=batch_size // 2,\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.927261400Z",
     "start_time": "2023-12-15T17:22:15.912261400Z"
    },
    "id": "bqkvofHc7rIY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['[CLS]', '@', 'united', 'I', 'have', 'never', 'been', 'mi', '##sle', '##ad', 'by', 'a', 'company', 'as', 'many', 'times', 'as', 'I', 'have', 'this', 'week', 'by', 'United', 'Airlines', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "token to s [CLS] @ united I have never been mislead by a company as many times as I have this week by United Airlines! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(dataset_train[0][0])\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:17.336407400Z",
     "start_time": "2023-12-15T17:22:15.930261900Z"
    },
    "id": "DxZrfCqW7rIY"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# 定義標籤平滑化的KL損失函數 Paper: https://arxiv.org/pdf/2312.06522.pdf\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = -1\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n",
    "# 設定標籤平滑化水平\n",
    "\"\"\"\n",
    "LS2: smoothing=0.03（即3%平滑化）\n",
    "LS3: smoothing=0.075（即7.5%平滑化）\n",
    "LS4: smoothing=0.15（即15%平滑化）\n",
    "LS5: smoothing=0.3（即30%平滑化）\n",
    "\"\"\"\n",
    "criterion = LabelSmoothingLoss(classes=3, smoothing=0.075)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpwgE2Gd7rIZ"
   },
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:17.352406900Z",
     "start_time": "2023-12-15T17:22:17.336407400Z"
    },
    "id": "zlaiAZAD7rIa"
   },
   "outputs": [],
   "source": [
    "def accuracy(raw_preds, y):\n",
    "    preds = raw_preds.argmax(dim=1)\n",
    "    acc = (preds == y).sum()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:18.138005700Z",
     "start_time": "2023-12-15T17:22:17.359407300Z"
    },
    "id": "dmc_Gms97rIa"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Testing process                              #\n",
    "        #########################################################################\n",
    "        # 1. Clean the gradients of optimizer\n",
    "        # 2. Put correct variables into model\n",
    "        # 3. Get prediction\n",
    "        # 4. Evalutate by criterion and accuracy\n",
    "\n",
    "        # Clean the gradients of optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(text, attention_mask=mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.logits, label)\n",
    "\n",
    "        # Compute accuracy using the provided function\n",
    "        acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "def test(model, data, criterion, log_loss=False):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Training process                             #\n",
    "        #########################################################################\n",
    "        # 1. Put correct variables into model\n",
    "        # 2. Get prediction\n",
    "        # 3. Evalutate by criterion and accuracy\n",
    "\n",
    "        # No gradient calculation for evaluation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(text, attention_mask=mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.logits, label)\n",
    "\n",
    "            # Compute accuracy\n",
    "            acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if log_loss:\n",
    "            val_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "# class for monitoring train and test acc/loss\n",
    "class Meter:\n",
    "    def __init__(self):\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def update(self, train_loss, train_acc, val_loss, val_acc):\n",
    "        self.train_loss_list.append(train_loss)\n",
    "        self.train_acc_list.append(train_acc)\n",
    "        self.val_loss_list.append(val_loss)\n",
    "        self.val_acc_list.append(val_acc)\n",
    "\n",
    "    def plot(self):\n",
    "        x = range(len(self.train_loss_list))\n",
    "        plt.plot(x, self.train_loss_list)\n",
    "        plt.plot(x, self.val_loss_list, color='r')\n",
    "        plt.legend(['train_loss', 'val_loss'])\n",
    "        plt.show()\n",
    "        plt.plot(x, self.train_acc_list)\n",
    "        plt.plot(x, self.val_acc_list, color='r')\n",
    "        plt.legend(['train_acc', 'val_acc'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExZyrKd57rIb"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:29:31.261703700Z",
     "start_time": "2023-12-15T17:22:18.142999600Z"
    },
    "id": "bVDe-fRe7rIc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:26<00:00,  3.72it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 24.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 0.02218528707554618 train_acc: 0.7659055425448869\n",
      "Epoch 1 val_loss:  0.03831913383874191 val_acc : 0.835990888382688\n",
      "---------- e 1 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:25<00:00,  3.77it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 23.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss: 0.01783091526632659 train_acc: 0.8566549570647931\n",
      "Epoch 2 val_loss:  0.037285663770199186 val_acc : 0.8405466970387244\n",
      "---------- e 2 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:25<00:00,  3.75it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 23.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss: 0.015629414538798344 train_acc: 0.8994925839188135\n",
      "Epoch 3 val_loss:  0.037661733754946594 val_acc : 0.8443432042520881\n",
      "---------- e 3 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:26<00:00,  3.73it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 23.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss: 0.01394491302755361 train_acc: 0.9318891491022638\n",
      "Epoch 4 val_loss:  0.03865231291851313 val_acc : 0.8413059984813971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:26<00:00,  3.69it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 23.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss: 0.01263352097023008 train_acc: 0.9589188134270101\n",
      "Epoch 5 val_loss:  0.040723931024539806 val_acc : 0.8375094912680334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#                          Hyper-parameters                             #\n",
    "#########################################################################\n",
    "max_epoch = 5\n",
    "log_interval = 1\n",
    "best_acc = 0\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################\n",
    "\n",
    "m = Meter()\n",
    "\n",
    "for epoch in range(1, max_epoch + 1):\n",
    "    train_loss, train_acc = train(model, train_data, optimizer, criterion)\n",
    "    val_loss, val_acc = test(model, val_data, criterion, log_loss=True)\n",
    "\n",
    "    if epoch % log_interval == 0:\n",
    "        print('Epoch {} train_loss: {} train_acc: {}'.format(\n",
    "            epoch, train_loss, train_acc\n",
    "        ))\n",
    "        print('Epoch {} val_loss:  {} val_acc : {}'.format(\n",
    "            epoch, val_loss, val_acc\n",
    "        ))\n",
    "\n",
    "    m.update(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "    # model checkpoint\n",
    "    if val_acc > best_acc:\n",
    "        best_model = model\n",
    "        best_acc = val_acc\n",
    "        print('-'*10, 'e', epoch, 'save best model', '-'*10)\n",
    "        torch.save(model.state_dict(), 'ckpts/e{}.pt'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:29:31.468711500Z",
     "start_time": "2023-12-15T17:29:31.263701400Z"
    },
    "id": "SmtW58OR7rIc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot them out\n",
    "m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcJcHf7n7rId"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/f072e95f51bc48978225941dba218241).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:30:20.742309100Z",
     "start_time": "2023-12-15T17:30:20.255626800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = model\n",
    "best_model.load_state_dict(torch.load('ckpts/e2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:30:29.887422500Z",
     "start_time": "2023-12-15T17:30:21.670846900Z"
    },
    "id": "Sf5UTlMZ7rId"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:08<00:00, 22.01it/s]\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "\n",
    "total_out = []\n",
    "for text, mask in tqdm(test_data, total=len(test_data)):\n",
    "    text = text.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    output = best_model(text, mask)\n",
    "    pred = output.logits\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    total_out.append(pred)\n",
    "\n",
    "total_out = torch.cat(total_out).cpu().numpy().tolist()\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(total_out):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T15:32:25.986455900Z",
     "start_time": "2023-12-10T15:32:24.748887300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save best model\n",
    "torch.save(best_model.state_dict(), 'ckpts/bert_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: In-Context learning (32 points)\n",
    "\n",
    "In this task, you will learn how to perform sentiment classification using prompts without the need for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:45.402983900Z",
     "start_time": "2023-12-14T04:40:36.482722600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyprind\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertTokenizer, BertForMaskedLM\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.517694500Z",
     "start_time": "2023-12-14T04:40:45.404983600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#         TODO: Design your own template(prefix) and verbalizer         #\n",
    "#########################################################################\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.prefix = 'This is [MASK] sentence.'\n",
    "\n",
    "        # Define a more comprehensive verbalizer\n",
    "        self.verbalizer = {\n",
    "            \"negative\": 0,\n",
    "            \"neutral\": 1,\n",
    "            \"positive\": 2,\n",
    "        }\n",
    "\n",
    "        self.max_seq_length = 512\n",
    "        self.batch_size = 32\n",
    "\n",
    "\n",
    "config = Config()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bert_type = 'bert-base-uncased'\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(bert_type, num_labels=3)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(bert_type)\n",
    "\n",
    "bert = model.from_pretrained(bert_type, config=bert_config).to(device)\n",
    "\n",
    "#######################################################################\n",
    "#                        End of your code                             #\n",
    "#######################################################################\n",
    "\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.520531400Z",
     "start_time": "2023-12-14T04:40:49.519021900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to obtaion verbalizer ids\n",
    "def obtain_verbalizer_ids(verbalizer, tokenizer):\n",
    "    verbalizer_ids = tokenizer.convert_tokens_to_ids(list(verbalizer.keys()))\n",
    "    index2ids = {i: verbalizer_ids[i] for i in range(len(verbalizer_ids))}\n",
    "    return verbalizer_ids, index2ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.528860Z",
     "start_time": "2023-12-14T04:40:49.520531400Z"
    }
   },
   "outputs": [],
   "source": [
    "verbalizer_ids, index2ids = obtain_verbalizer_ids(config.verbalizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.536240100Z",
     "start_time": "2023-12-14T04:40:49.530869700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to concatenate prefix and text\n",
    "def concatenate_prefix(texts, config):\n",
    "    ##################################################\n",
    "    #   TODO: concatenate your own prefix and text   #                               \n",
    "    ##################################################\n",
    "    prefix_texts = [config.prefix + \" \" + text for text in texts]\n",
    "    ##################################################\n",
    "    #                 End of your code               #                               \n",
    "    ##################################################\n",
    "    return prefix_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.571086Z",
     "start_time": "2023-12-14T04:40:49.537240100Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(config):\n",
    "    # ['texts', 'labels']\n",
    "    df = pd.read_csv('./twitter_sentiment/train.csv')\n",
    "    original_texts = df['text'].tolist()\n",
    "    labels = df['sentiment_label'].tolist()\n",
    "\n",
    "    texts = concatenate_prefix(original_texts, config)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "texts, labels = load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.571086Z",
     "start_time": "2023-12-14T04:40:49.566484500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['[CLS]', 'this', 'is', '[MASK]', 'sentence', '.', '@', 'united', 'i', 'have', 'never', 'been', 'mis', '##lea', '##d', 'by', 'a', 'company', 'as', 'many', 'times', 'as', 'i', 'have', 'this', 'week', 'by', 'united', 'airlines', '!', '[SEP]']\n",
      "token to s [CLS] this is [MASK] sentence . @ united i have never been mislead by a company as many times as i have this week by united airlines ! [SEP]\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(tokenizer.encode(texts[0], add_special_tokens=True))\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.578533300Z",
     "start_time": "2023-12-14T04:40:49.572086600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Batching of texts and labels for training or processing in batches\n",
    "def pack_batch(texts, labels, batch_size):\n",
    "    \"\"\"\n",
    "    :param texts: list\n",
    "    :param labels: list\n",
    "    :param batch_size: int\n",
    "    :return batch_X: list\n",
    "            [[text11, text12, ...], [text21, text22, ...], ...]\n",
    "    :return batch_y: list\n",
    "            [[label11, label12, ...], [label21, label22, ...], ...]\n",
    "    :return batch_count: int\n",
    "    \"\"\"\n",
    "    assert len(texts) == len(labels)\n",
    "\n",
    "    if len(texts) % batch_size != 0:\n",
    "        flag = False\n",
    "        batch_count = int(len(texts) / batch_size) + 1\n",
    "    else:\n",
    "        flag = True\n",
    "        batch_count = int(len(texts) / batch_size)\n",
    "\n",
    "    batch_X, batch_y = [], []\n",
    "\n",
    "    if flag:\n",
    "        for i in range(batch_count):\n",
    "            batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "            batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "    else:\n",
    "        for i in range(batch_count):\n",
    "            if i == batch_count - 1:\n",
    "                batch_X.append(texts[i * batch_size:])\n",
    "                batch_y.append(labels[i * batch_size:])\n",
    "            else:\n",
    "                batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "                batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "\n",
    "    return batch_X, batch_y, batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.586639100Z",
     "start_time": "2023-12-14T04:40:49.579533300Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_X, batch_y, batch_count = pack_batch(texts, labels, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:52:24.649661500Z",
     "start_time": "2023-12-14T04:40:49.590638700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[100 %] Time elapsed: 00:11:35 | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.643638 | precision: 0.578361 | recall: 0.643638 | f1: 0.557020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:11:35\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    pper = pyprind.ProgPercent(batch_count)\n",
    "    for i in range(batch_count):\n",
    "        inputs = batch_X[i]\n",
    "        labels = batch_y[i]\n",
    "\n",
    "        tokens = tokenizer.batch_encode_plus(inputs, add_special_tokens=True,\n",
    "                                             max_length=config.max_seq_length,\n",
    "                                             padding='max_length', truncation=True)\n",
    "        \n",
    "        ids = torch.tensor(tokens['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(tokens['attention_mask']).to(device)\n",
    "\n",
    "        # Shape: (batch_size, max_seq_length, vocab_size)\n",
    "        logits = bert(ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        mask_token_index = (ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "\n",
    "        # Find [MASK] logits\n",
    "        # shape: (batch_size, vocab_size)\n",
    "        masked_logits = logits[mask_token_index[0], mask_token_index[1], :]\n",
    "\n",
    "        # Extract the logits of the word in the verbalizer at the [MASK] position\n",
    "        # shape: (batch_size, verbalizer_size)\n",
    "        verbalizer_logits = masked_logits[:, verbalizer_ids]\n",
    "\n",
    "        # Construct a pseudo-distribution from the logits in these verbalizers\n",
    "        pseudo_distribution = softmax(verbalizer_logits)\n",
    "\n",
    "        #################################################################################\n",
    "        #   1. Find the index with the maximum probability in the pseudo-distribution   #\n",
    "        #   2. Convert the index to the corresponding word ID                           #\n",
    "        #   3. Convert the ID to a token                                                #\n",
    "        #   4. Find the label corresponding to the token                                #\n",
    "        #################################################################################\n",
    "        # 1. Find the index with the maximum probability in the pseudo-distribution\n",
    "        pred_indices = torch.argmax(pseudo_distribution, dim=1)\n",
    "\n",
    "        # 2. Convert the index to the corresponding word ID\n",
    "        pred_ids = [verbalizer_ids[index] for index in pred_indices]\n",
    "\n",
    "        # 3. Convert the ID to a token\n",
    "        pred_tokens = tokenizer.convert_ids_to_tokens(pred_ids)\n",
    "\n",
    "        # 4. Find the label corresponding to the token\n",
    "        pred_labels = [config.verbalizer[token] if token in config.verbalizer else -1 for token in pred_tokens]\n",
    "        pred_labels = np.array(pred_labels)\n",
    "        #################################################################################\n",
    "        #                             End of your code                                  #                                       \n",
    "        #################################################################################\n",
    "\n",
    "        predict_all = np.append(predict_all, pred_labels)\n",
    "        labels_all = np.append(labels_all, labels)\n",
    "\n",
    "        pper.update()\n",
    "\n",
    "    acc = accuracy_score(labels_all, predict_all)\n",
    "    p = precision_score(labels_all, predict_all, average=\"weighted\")\n",
    "    r = recall_score(labels_all, predict_all, average=\"weighted\")\n",
    "    f1 = f1_score(labels_all, predict_all, average=\"weighted\")\n",
    "\n",
    "    print('accuracy: %f | precision: %f | recall: %f | f1: %f' % (acc, p, r, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: LM-BFF (45 points)\n",
    "\n",
    "https://arxiv.org/pdf/2012.15723.pdf\n",
    "\n",
    "Unlike the previous task, LM-BFF can generate templates and verbalizers automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請先到共用雲端硬碟將檔案 `SST-2.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/14MDYFasXU94dUE9DjgfcZE61iTRI2007/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install openprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T03:31:13.543193800Z",
     "start_time": "2023-12-09T03:30:21.808076200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openprompt\n",
      "  Downloading openprompt-1.0.1-py3-none-any.whl (146 kB)\n",
      "     ---------------------------------------- 0.0/146.4 kB ? eta -:--:--\n",
      "     ---------------- ---------------------- 61.4/146.4 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 146.4/146.4 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting transformers>=4.10.0 (from openprompt)\n",
      "  Obtaining dependency information for transformers>=4.10.0 from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 123.5/123.5 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting sentencepiece==0.1.96 (from openprompt)\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 0.1/1.1 MB 2.2 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.2/1.1 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.3/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.4/1.1 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.5/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 0.6/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.6/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 0.7/1.1 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 0.8/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 0.9/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.0/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.1/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.1/1.1 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting tqdm>=4.62.2 (from openprompt)\n",
      "  Obtaining dependency information for tqdm>=4.62.2 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting tensorboardX (from openprompt)\n",
      "  Obtaining dependency information for tensorboardX from https://files.pythonhosted.org/packages/44/71/f3e7c9b2ab67e28c572ab4e9d5fa3499e0d252650f96d8a3a03e26677f53/tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nltk (from openprompt)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting yacs (from openprompt)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting dill (from openprompt)\n",
      "  Obtaining dependency information for dill from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting datasets (from openprompt)\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting rouge==1.0.0 (from openprompt)\n",
      "  Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting pyarrow (from openprompt)\n",
      "  Obtaining dependency information for pyarrow from https://files.pythonhosted.org/packages/28/82/9adfafaf0de581a39a1c86002cafd1a55a1255c9e0d362dc3e970aab0656/pyarrow-14.0.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading pyarrow-14.0.1-cp39-cp39-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openprompt) (1.8.0)\n",
      "Requirement already satisfied: six in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rouge==1.0.0->openprompt) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\eddie\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.62.2->openprompt) (0.4.4)\n",
      "Collecting filelock (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/fc/85/0d1038f068900896a8590d6d0da198b90d31f731a39166a432aa2b92249b/regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (2.25.1)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/9f/90/a6821e7757d2db194c16cbca78c80e206f30f6cc62c7f15fb27428f8c6dd/tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/4e/96/f4ee4434d8b6452fe7d5d44df2e72d1c6b2add1c3a5fb5c81aae83cb90c6/safetensors-0.4.1-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.1-cp39-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->openprompt)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets->openprompt) (1.4.1)\n",
      "Collecting xxhash (from datasets->openprompt)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/33/db/e07aa80e39a7ee82e9ca6f5a0fa9bf0b4465934272116a1b578eaee7f4b3/xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->openprompt)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/c6/c9/820b5ab056f4ada76fbe05bd481a948f287957d6cbfd59e2dd2618b408c1/multiprocess-0.70.15-py39-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets->openprompt)\n",
      "  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets->openprompt) (3.7.4.post0)\n",
      "Requirement already satisfied: click in c:\\users\\eddie\\appdata\\roaming\\python\\python39\\site-packages (from nltk->openprompt) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->openprompt) (1.1.0)\n",
      "Collecting protobuf>=3.20 (from tensorboardX->openprompt)\n",
      "  Obtaining dependency information for protobuf>=3.20 from https://files.pythonhosted.org/packages/b6/4b/f4f3334784576822d7817a664b757030ebb35b981978baf9c2eb3c5f33a8/protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (21.2.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (3.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (5.2.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (1.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers>=4.10.0->openprompt) (3.0.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets->openprompt) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets->openprompt) (2021.3)\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 41.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/7.9 MB 3.2 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.2/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.4/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.4/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.6/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.7/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.8/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.9/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.0/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.1/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.2/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.3/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.4/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.5/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.6/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.7/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.8/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.9/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.2/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.3/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.4/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.5/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.6/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.7/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.8/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.9/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.2/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.3/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.4/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.6/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.7/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.9/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.0/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.1/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.2/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.3/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.5/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.6/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.9/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.0/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.1/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.2/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.3/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.5/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.6/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.7/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.9/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.0/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.1/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.3/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.4/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.5/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.6/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.7/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.8/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.9/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.0/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.3/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.4/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.5/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.6/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.7/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.8/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "   ---------------------------------------- 0.0/521.2 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 92.2/521.2 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 174.1/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 276.5/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 368.6/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 471.0/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 521.2/521.2 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 61.4/115.3 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 115.3/115.3 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow-14.0.1-cp39-cp39-win_amd64.whl (24.6 MB)\n",
      "   ---------------------------------------- 0.0/24.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/24.6 MB 3.2 MB/s eta 0:00:08\n",
      "   ---------------------------------------- 0.2/24.6 MB 2.6 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 0.3/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.4/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.5/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.6/24.6 MB 2.2 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.7/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.8/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.8/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.9/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.0/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.1/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.2/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.3/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.4/24.6 MB 1.9 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.4/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.7/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.8/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.9/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.0/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.1/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.2/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.3/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.4/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.5/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.6/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.7/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.8/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.9/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 3.0/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.2/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.3/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.4/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.5/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.6/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.7/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.8/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.9/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.0/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.2/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.3/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.4/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.5/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.6/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.8/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.9/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.0/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.1/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.2/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.3/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.4/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.5/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.6/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.8/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.9/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.0/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.1/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.2/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.3/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.4/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.5/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.6/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.7/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 6.8/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 6.9/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.0/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.1/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.2/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.3/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.5/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.6/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.7/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.8/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.9/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.0/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.1/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.2/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.3/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.5/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 8.6/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.7/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.8/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.3/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.4/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.5/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.6/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.7/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.8/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 9.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.0/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.3/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.4/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 10.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.6/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.7/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.8/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.9/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 11.0/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.1/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.2/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.4/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.6/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.7/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 11.8/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 11.9/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.0/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.1/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.2/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.7/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.8/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.0/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.1/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.3/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.7/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.8/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.1/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.3/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 14.8/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 14.9/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.0/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.1/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.2/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.3/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.4/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.5/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.8/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.9/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.0/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.1/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.2/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.3/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.4/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.5/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 16.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 16.8/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 16.9/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.0/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.1/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.3/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.4/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.5/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.6/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.7/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.8/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 17.9/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.0/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.1/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.3/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.4/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.5/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.6/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.7/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.7/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.8/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 19.0/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.1/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.2/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.3/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.4/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.5/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.7/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.8/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.0/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.1/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.2/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.3/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.4/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.5/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.7/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.9/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.0/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.1/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.2/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.3/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.4/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.5/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.9/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.0/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.1/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.2/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.3/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.4/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.5/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.7/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.8/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.2/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.3/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.4/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.5/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.6/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.7/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.8/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.2/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.5/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.6/24.6 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.7 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 92.2/101.7 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 101.7/101.7 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 0.0/311.7 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 112.6/311.7 kB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 194.6/311.7 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  307.2/311.7 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 311.7/311.7 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 112.6/413.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 194.6/413.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 307.2/413.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 368.6/413.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 413.4/413.4 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 92.2/269.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 174.1/269.6 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/269.6 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 269.6/269.6 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp39-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.8 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 112.6/277.8 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 194.6/277.8 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 277.8/277.8 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.2 MB 1.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 1.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.4/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.3 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 92.2/133.3 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 133.3/133.3 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl (29 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 71.7/166.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 166.4/166.4 kB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, yacs, xxhash, tqdm, safetensors, rouge, regex, pyarrow-hotfix, pyarrow, protobuf, fsspec, filelock, dill, tensorboardX, nltk, multiprocess, huggingface-hub, tokenizers, transformers, datasets, openprompt\n",
      "Successfully installed datasets-2.15.0 dill-0.3.7 filelock-3.13.1 fsspec-2023.10.0 huggingface-hub-0.19.4 multiprocess-0.70.15 nltk-3.8.1 openprompt-1.0.1 protobuf-4.25.1 pyarrow-14.0.1 pyarrow-hotfix-0.6 regex-2023.10.3 rouge-1.0.0 safetensors-0.4.1 sentencepiece-0.1.96 tensorboardX-2.6.2.2 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.35.2 xxhash-3.4.1 yacs-0.1.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install openprompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import openprompt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.181341Z",
     "start_time": "2023-12-10T03:35:52.654085900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.194342100Z",
     "start_time": "2023-12-10T03:36:10.182341900Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda = True\n",
    "auto_t = True # Whether to perform automatic template generation\n",
    "auto_v = True # Whether to perform automatic verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.226340200Z",
     "start_time": "2023-12-10T03:36:10.195343500Z"
    }
   },
   "outputs": [],
   "source": [
    "from openprompt.data_utils.text_classification_dataset import SST2Processor\n",
    "dataset = {'train': SST2Processor().get_train_examples(\"SST-2/\"),\n",
    "           'validation': SST2Processor().get_dev_examples(\"SST-2/\"),\n",
    "           'test': SST2Processor().get_test_examples(\"SST-2/\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.607530300Z",
     "start_time": "2023-12-10T03:36:10.228340900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: {\n",
      "  \"guid\": \"train-0\",\n",
      "  \"label\": 0,\n",
      "  \"meta\": {\n",
      "    \"labelword\": \"terrible\"\n",
      "  },\n",
      "  \"text_a\": \"nothing happens , and it happens to flat characters .\",\n",
      "  \"text_b\": \"\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print('load model...')\n",
    "from openprompt.plms import load_plm\n",
    "# load mlm model for main tasks\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")\n",
    "\n",
    "# load generation model for template generation\n",
    "template_generate_model, template_generate_tokenizer, template_generate_model_config, template_tokenizer_wrapper = load_plm('t5', 't5-large')\n",
    "\n",
    "from openprompt.prompts import ManualVerbalizer, ManualTemplate\n",
    "verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "#   TODO: You need to switch LMBFFTemplateGenerationTemplate or ManualTemplate to                                 #\n",
    "#         compare auto generate template and manual generate template                                             #\n",
    "###################################################################################################################\n",
    "from openprompt.prompts.prompt_generator import LMBFFTemplateGenerationTemplate\n",
    "########################################\n",
    "#   LMBFFTemplateGenerationTemplate    #\n",
    "########################################\n",
    "import random\n",
    "\n",
    "# number of demonstrations\n",
    "num_demonstrations = 1  # try different number\n",
    "\n",
    "demonstrations = []\n",
    "\n",
    "for _ in range(num_demonstrations):\n",
    "    # random choice training set example with label 0 \n",
    "    random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n",
    "\n",
    "    # random choice training set example with label 1\n",
    "    random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n",
    "    \n",
    "    demonstration = f'{random_example_1.text_a} It was terrible. {random_example_2.text_a} It was great.'\n",
    "    demonstrations.append(demonstration)\n",
    "\n",
    "# You can modify the demonstrations and try different combinations\n",
    "template_text = '{\"placeholder\": \"text_a\"} {\"mask\"} {\"meta\": \"labelword\"} {\"mask\"}.' + ' '.join(demonstrations)\n",
    "template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "\n",
    "#############################################\n",
    "#   End of LMBFFTemplateGenerationTemplate  #\n",
    "#############################################\n",
    "\n",
    "########################################\n",
    "#          ManualTemplate              #\n",
    "########################################\n",
    "\n",
    "#template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} It was {\"mask\"}.')\n",
    "\n",
    "#############################################\n",
    "#          End of ManualTemplate            # \n",
    "#############################################\n",
    "\n",
    "###################################################################################################################\n",
    "#                                           End of your code                                                      #\n",
    "###################################################################################################################\n",
    "\n",
    "\n",
    "# view wrapped example\n",
    "wrapped_example = template.wrap_one_example(dataset['train'][0])\n",
    "print(\"dataset:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.673523500Z",
     "start_time": "2023-12-10T03:36:46.625523700Z"
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "\n",
    "# Returns the best evaluation score achieved during training\n",
    "def fit(model, train_dataloader, val_dataloader, loss_func, optimizer, epochs=5):\n",
    "    best_score = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_dataloader, loss_func, optimizer)\n",
    "        score = evaluate(model, val_dataloader)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "        print(f\"Epoch {epoch+1}: Train loss={train_loss}, Eval score={score}\")\n",
    "    return best_score\n",
    "\n",
    "# Trains the model on the training data and computes the training loss\n",
    "def train_epoch(model, train_dataloader, loss_func, optimizer):\n",
    "    model.train()\n",
    "    loss_all = []\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits #\n",
    "        # 2. Get labels                                     #\n",
    "        # 3. Evalutate using loss_func                      #\n",
    "        # 4. Append loss to loss_all                        #\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits\n",
    "        logits = model(batch=inputs)\n",
    "\n",
    "        # 2. Get labels\n",
    "        labels = inputs['label']\n",
    "\n",
    "        # 3. Evalutate using loss_func\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Append loss to loss_all\n",
    "        loss_all.append(loss.item())\n",
    "        #####################################################\n",
    "        #                 End of your code                  #\n",
    "        #####################################################\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return np.mean(loss_all)\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    with torch.no_grad():\n",
    "        for step, inputs in enumerate(val_dataloader):\n",
    "            if cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits #\n",
    "            # 2. Get labels                                     #\n",
    "            # 3. Extend labels to list                          #\n",
    "            # 4. Get predictions and extend preds to list       #\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits\n",
    "            logits = model(batch=inputs)\n",
    "\n",
    "            # 2. Get labels\n",
    "            labels = inputs['label']\n",
    "\n",
    "            # 3. Extend labels to list\n",
    "            alllabels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # 4. Get predictions and extend preds to list\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            allpreds.extend(preds.cpu().numpy())\n",
    "            #####################################################\n",
    "            #                 End of your code                  #\n",
    "            #####################################################\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generated template from TemplateGenerator and find the best template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:13:36.594464200Z",
     "start_time": "2023-12-10T03:36:46.673523500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 1454.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:37<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"placeholder\": \"text_a\"} It was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} it was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001b[A\n",
      "tokenizing: 32it [00:00, 202.53it/s]A\n",
      "\n",
      "tokenizing: 32it [00:00, 727.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.562330920593297, Eval score=0.5\n",
      "Epoch 2: Train loss=1.029085214715451, Eval score=0.5\n",
      "Epoch 3: Train loss=0.671642615867313, Eval score=0.71875\n",
      "Epoch 4: Train loss=0.2566610455396585, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [15:59<1:03:57, 959.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.2469192902863142, Eval score=0.78125\n",
      "Current template: {\"placeholder\": \"text_a\"} it was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' it was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 379.37it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.511358927668084, Eval score=0.5\n",
      "Epoch 2: Train loss=0.7635227439459413, Eval score=0.5\n",
      "Epoch 3: Train loss=0.3154368309772053, Eval score=0.53125\n",
      "Epoch 4: Train loss=0.8977778584977472, Eval score=0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [36:43<56:21, 1127.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.7606429383413342, Eval score=0.5\n",
      "Current template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 571.42it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1180.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.765889931773188, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.5693292848736746, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.041312409681268036, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.2322409824218994, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [56:36<38:33, 1156.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.22881872234574985, Eval score=0.84375\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 1066.65it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1523.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.7746381501195, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9883591458201408, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8319057337939739, Eval score=0.625\n",
      "Epoch 4: Train loss=0.508084288907412, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [1:16:21<19:27, 1167.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.39746711112820776, Eval score=0.53125\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 864.68it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1454.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.6042319842349002, Eval score=0.5\n",
      "Epoch 2: Train loss=1.069442194304429, Eval score=0.5\n",
      "Epoch 3: Train loss=0.47684725234284997, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.2768472472046142, Eval score=0.59375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:36:08<00:00, 1153.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.07557142606344769, Eval score=0.625\n",
      "Final best template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class ManualTemplateWithoutParse(ManualTemplate):\n",
    "    \"\"\"The generated template from TemplateGenerator is a list of dict of parsed template_text. So no further parsing is needed.\"\"\"\n",
    "    def on_text_set(self):\n",
    "        pass\n",
    "\n",
    "# Template generation\n",
    "if auto_t:\n",
    "    print('performing auto_t...')\n",
    "\n",
    "    if cuda:\n",
    "        template_generate_model = template_generate_model.cuda()\n",
    "    template_generator = T5TemplateGenerator(template_generate_model, template_generate_tokenizer, template_tokenizer_wrapper, verbalizer, beam_width=5) # Beam_width is set to 5 here for efficiency; to improve performance, try a larger number.\n",
    "\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=template_generate_tokenizer, tokenizer_wrapper_class=template_tokenizer_wrapper, batch_size=len(dataset['train']), decoder_max_length=128, max_seq_length=128, shuffle=False, teacher_forcing=False) # Register all data at once\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        template_generator._register_buffer(data)\n",
    "\n",
    "    template_generate_model.eval()\n",
    "    print('generating...')\n",
    "    template_texts = template_generator._get_templates()\n",
    "\n",
    "    original_template = template.text\n",
    "    template_texts = [template_generator.convert_template(template_text, original_template) for template_text in template_texts]\n",
    "    # template_generator._show_template()\n",
    "    template_generator.release_memory()\n",
    "    # Generate a number of candidate template text\n",
    "    print(template_texts)\n",
    "    \n",
    "    # Iterate over each candidate and select the best one\n",
    "    best_metrics = 0.0\n",
    "    best_template_text = None\n",
    "    for template_text in tqdm(template_texts):\n",
    "        verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "        template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "        print(f\"Current template: {template_text}\\n\\nWrapped example: {template.wrap_one_example(dataset['train'][0])}\")\n",
    "\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "        \n",
    "        #######################################################\n",
    "        # TODO: Use score to Find your best template_text     #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_template_text = template_text\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # Use the best template\n",
    "    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "    template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=best_template_text)\n",
    "    print(\"Final best template:\", best_template_text)\n",
    "    print()\n",
    "    print(\"Wrapped example:\", template.wrap_one_example(dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verbalizer template from VerbalizerGenerator and find the best verbalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T11:58:04.838076700Z",
     "start_time": "2023-12-10T05:13:36.638466100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_v...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 1391.30it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current label_words: ['terrible', 'beautiful']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001b[A\n",
      "tokenizing: 3it [00:00, 21.48it/s]\u001b[A\n",
      "tokenizing: 6it [00:01,  5.15it/s]\u001b[A\n",
      "tokenizing: 8it [00:01,  6.27it/s]\u001b[A\n",
      "tokenizing: 32it [00:01, 23.45it/s]\u001b[A\n",
      "\n",
      "tokenizing: 32it [00:00, 969.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.1593105591260544, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.5407680265489034, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.17979280870349612, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.006995583800744498, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [21:05<6:40:40, 1265.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.002051841642924046, Eval score=0.84375\n",
      "current label_words: ['horrible', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 695.63it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1388.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.5843039118335565, Eval score=0.84375\n",
      "Epoch 2: Train loss=0.0798521326687478, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.0026221817748819376, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.0010607897513068565, Eval score=0.78125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [42:02<6:18:07, 1260.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0004625524882726495, Eval score=0.78125\n",
      "current label_words: ['horrible', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 762.07it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1333.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.0936600251085586, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.849827597849071, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.414547988853883, Eval score=0.75\n",
      "Epoch 4: Train loss=0.0500250239797424, Eval score=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [1:02:46<5:55:04, 1253.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.004471211226245941, Eval score=0.8125\n",
      "current label_words: ['sad', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 415.60it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 941.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4185917818103917, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.5065413688316767, Eval score=0.875\n",
      "Epoch 3: Train loss=0.012464412650388113, Eval score=0.875\n",
      "Epoch 4: Train loss=0.002708091426967485, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [1:23:35<5:33:44, 1251.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0004699288858773798, Eval score=0.875\n",
      "current label_words: ['bad', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 551.73it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=3.1230944280390602, Eval score=0.5\n",
      "Epoch 2: Train loss=1.270681380527094, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8551086119841784, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.6224154182709754, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [1:45:07<5:16:30, 1266.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.22766433987999335, Eval score=0.8125\n",
      "current label_words: ['horrible', 'remarkable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 426.62it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1031.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.2276666148868003, Eval score=0.6875\n",
      "Epoch 2: Train loss=0.4889321715090773, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2634941844644345, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.25736280560994373, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [2:07:36<5:01:58, 1294.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.3903749104914027, Eval score=0.625\n",
      "current label_words: ['terrible', 'brilliant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 493.33it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.128300400949797, Eval score=0.5\n",
      "Epoch 2: Train loss=1.0674331626432831, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.6011688623111695, Eval score=0.875\n",
      "Epoch 4: Train loss=0.6810656589186692, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [2:28:49<4:38:53, 1287.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.1570308672144165, Eval score=0.84375\n",
      "current label_words: ['terrible', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 451.65it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1033.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.1662085960851982, Eval score=0.84375\n",
      "Epoch 2: Train loss=0.046194135922632995, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.20158991879031873, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.009880203132524912, Eval score=0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [2:51:16<4:21:16, 1306.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.27703922392970526, Eval score=0.90625\n",
      "current label_words: ['disappointing', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 415.57it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1032.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.3897865504303581, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.6985758165101288, Eval score=0.5625\n",
      "Epoch 3: Train loss=0.20094251398768392, Eval score=0.875\n",
      "Epoch 4: Train loss=0.021359096241212683, Eval score=0.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [3:11:17<3:53:28, 1273.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0014102687238164435, Eval score=0.90625\n",
      "current label_words: ['strange', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 410.23it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4916955009493904, Eval score=0.5\n",
      "Epoch 2: Train loss=0.5601507005485473, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.050391235166898696, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.04447055474645367, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [3:31:10<3:28:06, 1248.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.5673459974156145, Eval score=0.78125\n",
      "current label_words: ['terrible', 'good']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 363.21it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.9351653824437118, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9128216816243366, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.16854792425147025, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.004711857527581742, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [3:50:34<3:03:23, 1222.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0015127657302400621, Eval score=0.84375\n",
      "current label_words: ['bad', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 524.59it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.8022562539517715, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9302898038731655, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.43625156552297994, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.04196147369020764, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [4:10:06<2:40:57, 1207.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.012102704274127518, Eval score=0.65625\n",
      "current label_words: ['short', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 412.44it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1203.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.8797737168388267, Eval score=0.75\n",
      "Epoch 2: Train loss=0.3757321042244257, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2311989847357836, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.43366863449273296, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [4:30:04<2:20:30, 1204.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.620734619528541, Eval score=0.59375\n",
      "current label_words: ['disappointing', 'delightful']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 426.69it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1143.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.324861448905718, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.5381804386270232, Eval score=0.5625\n",
      "Epoch 3: Train loss=0.36967586419450527, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.12384688404563349, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [4:49:18<1:58:55, 1189.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.18691727038913086, Eval score=0.6875\n",
      "current label_words: ['bad', 'brilliant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 490.61it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1103.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.5005056243027184, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.7745201710495166, Eval score=0.875\n",
      "Epoch 3: Train loss=0.13810731278863386, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.004411970109288177, Eval score=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [5:08:29<1:38:08, 1177.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.00313117326692236, Eval score=0.78125\n",
      "current label_words: ['bad', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 492.79it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.182302282977588, Eval score=0.5\n",
      "Epoch 2: Train loss=0.5332906207768247, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.08191546555644891, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.35411459776105403, Eval score=0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [5:26:58<1:17:08, 1157.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.13951104862388775, Eval score=0.46875\n",
      "current label_words: ['horrible', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 466.99it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1219.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4121702450024713, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.32750329683221935, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.01347528245059948, Eval score=0.90625\n",
      "Epoch 4: Train loss=0.3011642301885331, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [5:45:44<57:23, 1147.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.022153147599055956, Eval score=0.875\n",
      "current label_words: ['fine', 'remarkable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 492.08it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1185.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.7259275259780793, Eval score=0.5\n",
      "Epoch 2: Train loss=0.7986743192886934, Eval score=0.65625\n",
      "Epoch 3: Train loss=0.6269949703128077, Eval score=0.5\n",
      "Epoch 4: Train loss=0.16056611831118062, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [6:04:43<38:10, 1145.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.003870869544044808, Eval score=0.78125\n",
      "current label_words: ['disappointing', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 454.61it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1183.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4480454477061357, Eval score=0.5\n",
      "Epoch 2: Train loss=0.8791331336833537, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.4247932309117459, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.2258107879970339, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [6:23:21<18:56, 1137.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0734178872121447, Eval score=0.875\n",
      "current label_words: ['terrible', 'fine']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 489.28it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1164.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.6715138442009447, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.6137157797584223, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.38552796499482156, Eval score=0.875\n",
      "Epoch 4: Train loss=0.34600197029577373, Eval score=0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [6:41:58<00:00, 1205.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.6120385159649686, Eval score=0.78125\n",
      "final best label words: ['terrible', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verbalizer generation\n",
    "from openprompt.prompts.prompt_generator import RobertaVerbalizerGenerator\n",
    "if auto_v:\n",
    "    print('performing auto_v...')\n",
    "    # Load generation model for verbalizer generation\n",
    "    if cuda:\n",
    "        plm = plm.cuda()\n",
    "    verbalizer_generator = RobertaVerbalizerGenerator(model=plm, tokenizer=tokenizer, candidate_num=20, label_word_num_per_class=20)\n",
    "    # To improve performance, try larger numbers\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, batch_size=32)\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        verbalizer_generator.register_buffer(data)\n",
    "    label_words_list = verbalizer_generator.generate()\n",
    "    verbalizer_generator.release_memory()\n",
    "\n",
    "    # Iterate over each candidate and select the best one\n",
    "    current_verbalizer = copy.deepcopy(verbalizer)\n",
    "    best_metrics = 0.0\n",
    "    best_label_words = None\n",
    "    for label_words in tqdm(label_words_list):\n",
    "        print(f\"current label_words: {label_words}\")\n",
    "        current_verbalizer.label_words = label_words\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, current_verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "\n",
    "        #######################################################\n",
    "        # TODO: Use score to find your best_label_word        #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_label_words = label_words\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # use the best verbalizer\n",
    "    print(\"final best label words:\", best_label_words)\n",
    "    verbalizer = ManualVerbalizer(tokenizer, num_classes=2, label_words=best_label_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:18:43.541949400Z",
     "start_time": "2023-12-10T11:58:04.849042700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 383.01it/s]\n",
      "tokenizing: 32it [00:00, 1142.78it/s]\n",
      "tokenizing: 872it [00:00, 1095.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.2980121186483302, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.38737686289732665, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.08258779709103692, Eval score=0.875\n",
      "Epoch 4: Train loss=0.3668047572554656, Eval score=0.84375\n",
      "Epoch 5: Train loss=0.00465579945631589, Eval score=0.84375\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "test_dataloader = PromptDataLoader(dataset['test'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "\n",
    "model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# It's always good practice to set no decay to bias and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/5b8876ed26fd495b8353ad7ce94b6f65)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:46:57.884699300Z",
     "start_time": "2023-12-10T12:18:43.474013600Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "allpreds = []\n",
    "for step, inputs in tqdm(enumerate(test_dataloader)):\n",
    "    if cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = model(inputs)\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(allpreds):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report (15 points)\n",
    "\n",
    "- Task 1: Compare **two** different models you employed and provide a brief discussion of your implementation.\n",
    "\n",
    "- Task 2: You need to try at least **three** different templates and verbalizers to compare how your prompts work with the model. Report your performance in zero-shot, one-shot, and few-shot scenarios, with examples drawn from the training set.\n",
    "\n",
    "- Task 3: Try at least three different manually crafted templates to compare them with auto-generated templates. Evaluate the performance with different numbers of demonstrations and plot the graph from Figure 3 in the paper (https://arxiv.org/pdf/2012.15723.pdf). Also, report your best template and verbalizer.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
