{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6Z1Snuk7rIK"
   },
   "source": [
    "# MIS 583 Assignment 6: Text Sentiment Classification with Prompt Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSwr9MgZogRZ"
   },
   "source": [
    "Before we start, please put your name and SID in following format: <br>\n",
    ": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷, M114020035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DzsjuDhlz_k"
   },
   "source": [
    "**Your Answer:**   \n",
    "Hi I'm 鄔仁迪, B104020009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d-Zzebq7rIM"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kc9gd_Wk7rIN"
   },
   "source": [
    "**Sentiment Classification** is an automated process of identifying opinions in text and labeling them as positive or negative based on the emotions customers express within them.\n",
    "\n",
    "In Task 1, you need to fine-tune a pre-trained language model (e.g., BERT) to predict the sentiment of given tweets.\n",
    "\n",
    "In Task 2, we employ prompts to enable the model to perform sentiment analysis through in-context learning, eliminating the need for additional training.\n",
    "\n",
    "In Task 3, you will use the method called LM-BFF to utilize the model in generating the optimal template and verbalizer autonomously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice \n",
    "**You are not allow to use the model like GPT family or pre-trained weight using SST-2 and twitter dataset!!!!!!!!!!!!!!!!!**\n",
    "\n",
    "You can use BERT and RoBERTa encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giUId1Naqacs",
    "tags": []
   },
   "source": [
    "##  Versions of used packages\n",
    "\n",
    "We will check PyTorch version to make sure everything work properly.  \n",
    "We use `python==3.7.14`, `torch==1.12.1+cu113` and `torchvision==0.13.1+cu113`.  \n",
    "This is the default version in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:11.995277800Z",
     "start_time": "2023-12-15T17:22:01.289099200Z"
    },
    "id": "Vuw-gNvjqcYe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]\n",
      "torch 2.1.0+cu118\n",
      "torchvision 0.16.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "print('python', sys.version.split('\\n')[0])\n",
    "print('torch', torch.__version__)\n",
    "print('torchvision', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Text Sentiment Classification (40 points)\n",
    "\n",
    "In this task, you need to fine-tune a pre-trained language model (e.g., BERT or RoBERTa encoder) to predict the sentiment of given tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a4s_a5D7rIR"
   },
   "source": [
    "## Loading Model and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPUkTbnL7rIR"
   },
   "source": [
    "First, let's talk about the model. The Hugging Face team has created an amazing framework called \"transformers\" for NLP tasks. It includes many state-of-the-art machine learning models for PyTorch, TensorFlow, and JAX.\n",
    "\n",
    "To start with this package, follow [this link to installation and a basic tutorial](https://pytorch.org/hub/huggingface_pytorch-transformers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:12.037278200Z",
     "start_time": "2023-12-15T17:22:11.997299100Z"
    },
    "id": "rK0ouXa09pDU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!echo happy installation\\n!pip -V\\n!pip install grpcio\\n!pip install google-auth\\n!pip install protobuf==3.9.2\\n!pip install pyprind\\n!pip install tqdm boto3 requests regex sentencepiece sacremoses'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you might need some additional installations there\n",
    "\"\"\"!echo happy installation\n",
    "!pip -V\n",
    "!pip install grpcio\n",
    "!pip install google-auth\n",
    "!pip install protobuf==3.9.2\n",
    "!pip install pyprind\n",
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.040759300Z",
     "start_time": "2023-12-15T17:22:12.010275100Z"
    },
    "id": "dmGCAevi7rIS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "#########################################################################\n",
    "#            Loading tokenizer and model from transformer               #\n",
    "#########################################################################\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, AutoConfig\n",
    "\n",
    "bert_type = 'bert-base-cased'\n",
    "\n",
    "# ---------- 1. load from torch.hub ----------\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_type)\n",
    "\n",
    "# create a Bert-extended task (classification)\n",
    "model = BertForSequenceClassification.from_pretrained(bert_type)\n",
    "\n",
    "# finetune from the output from bert to your task\n",
    "model.classifier = nn.Linear(in_features=768, out_features=3, bias=True)\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiMThsYeDa2O"
   },
   "source": [
    "## How to Get Data\n",
    "\n",
    "Please open the file `twitter_sentiment.zip`, creat shortcut to your Google Drive.\n",
    "\n",
    "1. open [LINK of Google Drive](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
    "3. Select the location where you want to place the shortcut.\n",
    "4. Click Add shortcut.\n",
    "\n",
    "After above procedures, we have a shortcut of zip file of dataset.  \n",
    "We can access this in colab after granting the permission of Google Drive.\n",
    "\n",
    "---\n",
    "\n",
    "請先到共用雲端硬碟將檔案 `twitter_sentiment.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.052762Z",
     "start_time": "2023-12-15T17:22:15.012758200Z"
    },
    "id": "lZnFgi5i_2oA"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqO8DiB6VRQZ"
   },
   "source": [
    "## Unzip Data\n",
    "\n",
    "解壓縮 `twitter_sentiment.zip` 後可以發現裡面有三個csv檔。\n",
    "\n",
    "- `train.csv`, `test.csv` and `val.csv`\n",
    "\n",
    "Training set 有 **10248** 筆資料.  \n",
    "Validation set 有 **1317** 筆資料.  \n",
    "Testing set 有 **3075** 筆資料.  \n",
    "\n",
    "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.052762Z",
     "start_time": "2023-12-15T17:22:15.040759300Z"
    },
    "id": "OSlTMdxf8Zd7"
   },
   "outputs": [],
   "source": [
    "# !unzip -qq ./drive/MyDrive/twitter_sentiment.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.065760100Z",
     "start_time": "2023-12-15T17:22:15.045759400Z"
    },
    "id": "wf5GXTme7rIT"
   },
   "outputs": [],
   "source": [
    "# Utility function to extract text and label from csv file\n",
    "def get_texts(f_name='./twitter_sentiment', mode='train'):\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "\n",
    "    f_path = os.path.join(f_name, '{}.csv'.format(mode))\n",
    "    with open(f_path, encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for line in reader:\n",
    "            text_list.append(line['text'])\n",
    "            if mode != 'test':\n",
    "                label_list.append(int(line['sentiment_label']))\n",
    "\n",
    "    return text_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.102758500Z",
     "start_time": "2023-12-15T17:22:15.061758900Z"
    },
    "id": "6fpY0ZrK7rIV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, f_name='./twitter_sentiment', mode='train'):\n",
    "        self.mode = mode\n",
    "\n",
    "        text_list, label_list = get_texts(f_name, mode)\n",
    "        print('mode', mode, 'has', len(text_list), 'datas')\n",
    "        text_list = tokenizer(text_list,\n",
    "                             truncation=True, padding=True,\n",
    "                             return_tensors='pt')\n",
    "\n",
    "        self.text_list = text_list['input_ids']\n",
    "        self.mask_list = text_list['attention_mask']\n",
    "\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_list[idx]\n",
    "        mask = self.mask_list[idx]\n",
    "        if self.mode == 'test':\n",
    "            return text, mask\n",
    "        label = torch.tensor(self.label_list[idx])\n",
    "        return text, mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.910260600Z",
     "start_time": "2023-12-15T17:22:15.074759400Z"
    },
    "id": "nCmM4FSw7rIW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode train has 10248 datas\n",
      "mode val has 1317 datas\n",
      "mode test has 3075 datas\n"
     ]
    }
   ],
   "source": [
    "dataset_train = TwitterDataset(mode='train')\n",
    "dataset_val = TwitterDataset(mode='val')\n",
    "dataset_test = TwitterDataset(mode='test')\n",
    "\n",
    "batch_size = 32\n",
    "train_data = DataLoader(dataset_train, batch_size=batch_size,\n",
    "                       shuffle=True)\n",
    "val_data = DataLoader(dataset_val, batch_size=batch_size // 2,\n",
    "                       shuffle=False)\n",
    "test_data = DataLoader(dataset_test, batch_size=batch_size // 2,\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.927261400Z",
     "start_time": "2023-12-15T17:22:15.912261400Z"
    },
    "id": "bqkvofHc7rIY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['[CLS]', '@', 'united', 'I', 'have', 'never', 'been', 'mi', '##sle', '##ad', 'by', 'a', 'company', 'as', 'many', 'times', 'as', 'I', 'have', 'this', 'week', 'by', 'United', 'Airlines', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "token to s [CLS] @ united I have never been mislead by a company as many times as I have this week by United Airlines! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(dataset_train[0][0])\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:17.336407400Z",
     "start_time": "2023-12-15T17:22:15.930261900Z"
    },
    "id": "DxZrfCqW7rIY"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# 定義標籤平滑化的KL損失函數 Paper: https://arxiv.org/pdf/2312.06522.pdf\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = -1\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n",
    "# 設定標籤平滑化水平\n",
    "\"\"\"\n",
    "LS2: smoothing=0.03（即3%平滑化）\n",
    "LS3: smoothing=0.075（即7.5%平滑化）\n",
    "LS4: smoothing=0.15（即15%平滑化）\n",
    "LS5: smoothing=0.3（即30%平滑化）\n",
    "\"\"\"\n",
    "criterion = LabelSmoothingLoss(classes=3, smoothing=0.3)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpwgE2Gd7rIZ"
   },
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:17.352406900Z",
     "start_time": "2023-12-15T17:22:17.336407400Z"
    },
    "id": "zlaiAZAD7rIa"
   },
   "outputs": [],
   "source": [
    "def accuracy(raw_preds, y):\n",
    "    preds = raw_preds.argmax(dim=1)\n",
    "    acc = (preds == y).sum()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:18.138005700Z",
     "start_time": "2023-12-15T17:22:17.359407300Z"
    },
    "id": "dmc_Gms97rIa"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Testing process                              #\n",
    "        #########################################################################\n",
    "        # 1. Clean the gradients of optimizer\n",
    "        # 2. Put correct variables into model\n",
    "        # 3. Get prediction\n",
    "        # 4. Evalutate by criterion and accuracy\n",
    "\n",
    "        # Clean the gradients of optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(text, attention_mask=mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.logits, label)\n",
    "\n",
    "        # Compute accuracy using the provided function\n",
    "        acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "def test(model, data, criterion, log_loss=False):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Training process                             #\n",
    "        #########################################################################\n",
    "        # 1. Put correct variables into model\n",
    "        # 2. Get prediction\n",
    "        # 3. Evalutate by criterion and accuracy\n",
    "\n",
    "        # No gradient calculation for evaluation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(text, attention_mask=mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.logits, label)\n",
    "\n",
    "            # Compute accuracy\n",
    "            acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if log_loss:\n",
    "            val_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "# class for monitoring train and test acc/loss\n",
    "class Meter:\n",
    "    def __init__(self):\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def update(self, train_loss, train_acc, val_loss, val_acc):\n",
    "        self.train_loss_list.append(train_loss)\n",
    "        self.train_acc_list.append(train_acc)\n",
    "        self.val_loss_list.append(val_loss)\n",
    "        self.val_acc_list.append(val_acc)\n",
    "\n",
    "    def plot(self):\n",
    "        x = range(len(self.train_loss_list))\n",
    "        plt.plot(x, self.train_loss_list)\n",
    "        plt.plot(x, self.val_loss_list, color='r')\n",
    "        plt.legend(['train_loss', 'val_loss'])\n",
    "        plt.show()\n",
    "        plt.plot(x, self.train_acc_list)\n",
    "        plt.plot(x, self.val_acc_list, color='r')\n",
    "        plt.legend(['train_acc', 'val_acc'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExZyrKd57rIb"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:29:31.261703700Z",
     "start_time": "2023-12-15T17:22:18.142999600Z"
    },
    "id": "bVDe-fRe7rIc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:23<00:00,  3.82it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 24.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 0.030300163535239826 train_acc: 0.7453161592505855\n",
      "Epoch 1 val_loss:  0.057784445646049946 val_acc : 0.8405466970387244\n",
      "---------- e 1 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:24<00:00,  3.81it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 24.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss: 0.02846814728662616 train_acc: 0.8583138173302107\n",
      "Epoch 2 val_loss:  0.05745779141930803 val_acc : 0.8451025056947609\n",
      "---------- e 2 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:24<00:00,  3.81it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 24.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss: 0.027747445651835338 train_acc: 0.8985167837626854\n",
      "Epoch 3 val_loss:  0.057580934615413984 val_acc : 0.8375094912680334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:24<00:00,  3.78it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 24.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss: 0.027182757650893893 train_acc: 0.929839968774395\n",
      "Epoch 4 val_loss:  0.05795935916466159 val_acc : 0.8435839028094153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:25<00:00,  3.75it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 23.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss: 0.026730615346111607 train_acc: 0.9547228727556596\n",
      "Epoch 5 val_loss:  0.057976341736343834 val_acc : 0.853454821564161\n",
      "---------- e 5 save best model ----------\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#                          Hyper-parameters                             #\n",
    "#########################################################################\n",
    "max_epoch = 5\n",
    "log_interval = 1\n",
    "best_acc = 0\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################\n",
    "\n",
    "m = Meter()\n",
    "\n",
    "for epoch in range(1, max_epoch + 1):\n",
    "    train_loss, train_acc = train(model, train_data, optimizer, criterion)\n",
    "    val_loss, val_acc = test(model, val_data, criterion, log_loss=True)\n",
    "\n",
    "    if epoch % log_interval == 0:\n",
    "        print('Epoch {} train_loss: {} train_acc: {}'.format(\n",
    "            epoch, train_loss, train_acc\n",
    "        ))\n",
    "        print('Epoch {} val_loss:  {} val_acc : {}'.format(\n",
    "            epoch, val_loss, val_acc\n",
    "        ))\n",
    "\n",
    "    m.update(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "    # model checkpoint\n",
    "    if val_acc > best_acc:\n",
    "        best_model = model\n",
    "        best_acc = val_acc\n",
    "        print('-'*10, 'e', epoch, 'save best model', '-'*10)\n",
    "        torch.save(model.state_dict(), 'ckpts/e{}.pt'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:29:31.468711500Z",
     "start_time": "2023-12-15T17:29:31.263701400Z"
    },
    "id": "SmtW58OR7rIc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot them out\n",
    "m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcJcHf7n7rId"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/f072e95f51bc48978225941dba218241).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:30:20.742309100Z",
     "start_time": "2023-12-15T17:30:20.255626800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = model\n",
    "best_model.load_state_dict(torch.load('ckpts/e2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:30:29.887422500Z",
     "start_time": "2023-12-15T17:30:21.670846900Z"
    },
    "id": "Sf5UTlMZ7rId"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:08<00:00, 23.17it/s]\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "\n",
    "total_out = []\n",
    "for text, mask in tqdm(test_data, total=len(test_data)):\n",
    "    text = text.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    output = best_model(text, mask)\n",
    "    pred = output.logits\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    total_out.append(pred)\n",
    "\n",
    "total_out = torch.cat(total_out).cpu().numpy().tolist()\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(total_out):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T15:32:25.986455900Z",
     "start_time": "2023-12-10T15:32:24.748887300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save best model\n",
    "torch.save(best_model.state_dict(), 'ckpts/bert_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: In-Context learning (32 points)\n",
    "\n",
    "In this task, you will learn how to perform sentiment classification using prompts without the need for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:45.402983900Z",
     "start_time": "2023-12-14T04:40:36.482722600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyprind\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertTokenizer, BertForMaskedLM\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.517694500Z",
     "start_time": "2023-12-14T04:40:45.404983600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#         TODO: Design your own template(prefix) and verbalizer         #\n",
    "#########################################################################\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Zero-shot learning template\n",
    "        # self.prefix = \"This is [MASK] sentence.\"\n",
    "        \n",
    "        # One-shot learning template\n",
    "        # self.prefix = (\n",
    "        #     \"This is bad sentence. @united be worse?oh you can't! delayed with no reason on the way to Lon. [SEP] \"\n",
    "        #     \"This is [MASK] sentence.\"\n",
    "        # )\n",
    "\n",
    "        # Few-shot learning template\n",
    "        self.prefix = (\n",
    "            \"This is bad sentence. @united be worse?oh you can't! delayed with no reason on the way to Lon. [SEP] \"\n",
    "            \"This is okay sentence. @AmericanAir what's the best number to use? [SEP] \"\n",
    "            \"This is good sentence. @JetBlue I was so excited when I saw that you fly there! #ionlyflyblue. [SEP] \"\n",
    "            \"This is [MASK] sentence.\"\n",
    "        )\n",
    "\n",
    "        # Define a more comprehensive verbalizer\n",
    "        self.verbalizer = {\n",
    "            'bad': 0,\n",
    "            'okay': 1,\n",
    "            'good': 2,\n",
    "        }\n",
    "\n",
    "        self.max_seq_length = 512\n",
    "        self.batch_size = 32\n",
    "\n",
    "\n",
    "config = Config()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bert_type = 'bert-base-uncased'\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(bert_type, num_labels=3)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(bert_type)\n",
    "\n",
    "bert = model.from_pretrained(bert_type, config=bert_config).to(device)\n",
    "\n",
    "#######################################################################\n",
    "#                        End of your code                             #\n",
    "#######################################################################\n",
    "\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.520531400Z",
     "start_time": "2023-12-14T04:40:49.519021900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to obtaion verbalizer ids\n",
    "def obtain_verbalizer_ids(verbalizer, tokenizer):\n",
    "    verbalizer_ids = tokenizer.convert_tokens_to_ids(list(verbalizer.keys()))\n",
    "    index2ids = {i: verbalizer_ids[i] for i in range(len(verbalizer_ids))}\n",
    "    return verbalizer_ids, index2ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.528860Z",
     "start_time": "2023-12-14T04:40:49.520531400Z"
    }
   },
   "outputs": [],
   "source": [
    "verbalizer_ids, index2ids = obtain_verbalizer_ids(config.verbalizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.536240100Z",
     "start_time": "2023-12-14T04:40:49.530869700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to concatenate prefix and text\n",
    "def concatenate_prefix(texts, config):\n",
    "    ##################################################\n",
    "    #   TODO: concatenate your own prefix and text   #                               \n",
    "    ##################################################\n",
    "    prefix_texts = [config.prefix + \" \" + text for text in texts]\n",
    "    ##################################################\n",
    "    #                 End of your code               #                               \n",
    "    ##################################################\n",
    "    return prefix_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.571086Z",
     "start_time": "2023-12-14T04:40:49.537240100Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(config):\n",
    "    # ['texts', 'labels']\n",
    "    df = pd.read_csv('./twitter_sentiment/train.csv')\n",
    "    original_texts = df['text'].tolist()\n",
    "    labels = df['sentiment_label'].tolist()\n",
    "\n",
    "    texts = concatenate_prefix(original_texts, config)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "texts, labels = load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.571086Z",
     "start_time": "2023-12-14T04:40:49.566484500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['[CLS]', 'this', 'is', 'bad', 'sentence', '.', '@', 'united', 'be', 'worse', '?', 'oh', 'you', 'can', \"'\", 't', '!', 'delayed', 'with', 'no', 'reason', 'on', 'the', 'way', 'to', 'lo', '##n', '.', '[SEP]', 'this', 'is', 'okay', 'sentence', '.', '@', 'americana', '##ir', 'what', \"'\", 's', 'the', 'best', 'number', 'to', 'use', '?', '[SEP]', 'this', 'is', 'good', 'sentence', '.', '@', 'jet', '##bl', '##ue', 'i', 'was', 'so', 'excited', 'when', 'i', 'saw', 'that', 'you', 'fly', 'there', '!', '#', 'ion', '##ly', '##fly', '##bl', '##ue', '.', '[SEP]', 'this', 'is', '[MASK]', 'sentence', '.', '@', 'united', 'i', 'have', 'never', 'been', 'mis', '##lea', '##d', 'by', 'a', 'company', 'as', 'many', 'times', 'as', 'i', 'have', 'this', 'week', 'by', 'united', 'airlines', '!', '[SEP]']\n",
      "token to s [CLS] this is bad sentence . @ united be worse ? oh you can ' t ! delayed with no reason on the way to lon . [SEP] this is okay sentence . @ americanair what ' s the best number to use ? [SEP] this is good sentence . @ jetblue i was so excited when i saw that you fly there ! # ionlyflyblue . [SEP] this is [MASK] sentence . @ united i have never been mislead by a company as many times as i have this week by united airlines ! [SEP]\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(tokenizer.encode(texts[0], add_special_tokens=True))\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.578533300Z",
     "start_time": "2023-12-14T04:40:49.572086600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Batching of texts and labels for training or processing in batches\n",
    "def pack_batch(texts, labels, batch_size):\n",
    "    \"\"\"\n",
    "    :param texts: list\n",
    "    :param labels: list\n",
    "    :param batch_size: int\n",
    "    :return batch_X: list\n",
    "            [[text11, text12, ...], [text21, text22, ...], ...]\n",
    "    :return batch_y: list\n",
    "            [[label11, label12, ...], [label21, label22, ...], ...]\n",
    "    :return batch_count: int\n",
    "    \"\"\"\n",
    "    assert len(texts) == len(labels)\n",
    "\n",
    "    if len(texts) % batch_size != 0:\n",
    "        flag = False\n",
    "        batch_count = int(len(texts) / batch_size) + 1\n",
    "    else:\n",
    "        flag = True\n",
    "        batch_count = int(len(texts) / batch_size)\n",
    "\n",
    "    batch_X, batch_y = [], []\n",
    "\n",
    "    if flag:\n",
    "        for i in range(batch_count):\n",
    "            batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "            batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "    else:\n",
    "        for i in range(batch_count):\n",
    "            if i == batch_count - 1:\n",
    "                batch_X.append(texts[i * batch_size:])\n",
    "                batch_y.append(labels[i * batch_size:])\n",
    "            else:\n",
    "                batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "                batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "\n",
    "    return batch_X, batch_y, batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.586639100Z",
     "start_time": "2023-12-14T04:40:49.579533300Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_X, batch_y, batch_count = pack_batch(texts, labels, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:52:24.649661500Z",
     "start_time": "2023-12-14T04:40:49.590638700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[100 %] Time elapsed: 00:06:04 | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.348946 | precision: 0.654360 | recall: 0.348946 | f1: 0.363316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:06:04\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    pper = pyprind.ProgPercent(batch_count)\n",
    "    for i in range(batch_count):\n",
    "        inputs = batch_X[i]\n",
    "        labels = batch_y[i]\n",
    "\n",
    "        tokens = tokenizer.batch_encode_plus(inputs, add_special_tokens=True,\n",
    "                                             max_length=config.max_seq_length,\n",
    "                                             padding='max_length', truncation=True)\n",
    "        \n",
    "        ids = torch.tensor(tokens['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(tokens['attention_mask']).to(device)\n",
    "\n",
    "        # Shape: (batch_size, max_seq_length, vocab_size)\n",
    "        logits = bert(ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        mask_token_index = (ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "\n",
    "        # Find [MASK] logits\n",
    "        # shape: (batch_size, vocab_size)\n",
    "        masked_logits = logits[mask_token_index[0], mask_token_index[1], :]\n",
    "\n",
    "        # Extract the logits of the word in the verbalizer at the [MASK] position\n",
    "        # shape: (batch_size, verbalizer_size)\n",
    "        verbalizer_logits = masked_logits[:, verbalizer_ids]\n",
    "\n",
    "        # Construct a pseudo-distribution from the logits in these verbalizers\n",
    "        pseudo_distribution = softmax(verbalizer_logits)\n",
    "\n",
    "        #################################################################################\n",
    "        #   1. Find the index with the maximum probability in the pseudo-distribution   #\n",
    "        #   2. Convert the index to the corresponding word ID                           #\n",
    "        #   3. Convert the ID to a token                                                #\n",
    "        #   4. Find the label corresponding to the token                                #\n",
    "        #################################################################################\n",
    "        # 1. Find the index with the maximum probability in the pseudo-distribution\n",
    "        pred_indices = torch.argmax(pseudo_distribution, dim=1)\n",
    "\n",
    "        # 2. Convert the index to the corresponding word ID\n",
    "        pred_ids = [verbalizer_ids[index] for index in pred_indices]\n",
    "\n",
    "        # 3. Convert the ID to a token\n",
    "        pred_tokens = tokenizer.convert_ids_to_tokens(pred_ids)\n",
    "\n",
    "        # 4. Find the label corresponding to the token\n",
    "        pred_labels = [config.verbalizer[token] if token in config.verbalizer else -1 for token in pred_tokens]\n",
    "        pred_labels = np.array(pred_labels)\n",
    "        #################################################################################\n",
    "        #                             End of your code                                  #                                       \n",
    "        #################################################################################\n",
    "\n",
    "        predict_all = np.append(predict_all, pred_labels)\n",
    "        labels_all = np.append(labels_all, labels)\n",
    "\n",
    "        pper.update()\n",
    "\n",
    "    acc = accuracy_score(labels_all, predict_all)\n",
    "    p = precision_score(labels_all, predict_all, average=\"weighted\")\n",
    "    r = recall_score(labels_all, predict_all, average=\"weighted\")\n",
    "    f1 = f1_score(labels_all, predict_all, average=\"weighted\")\n",
    "\n",
    "    print('accuracy: %f | precision: %f | recall: %f | f1: %f' % (acc, p, r, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: LM-BFF (45 points)\n",
    "\n",
    "https://arxiv.org/pdf/2012.15723.pdf\n",
    "\n",
    "Unlike the previous task, LM-BFF can generate templates and verbalizers automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請先到共用雲端硬碟將檔案 `SST-2.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/14MDYFasXU94dUE9DjgfcZE61iTRI2007/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install openprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T03:31:13.543193800Z",
     "start_time": "2023-12-09T03:30:21.808076200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openprompt\n",
      "  Downloading openprompt-1.0.1-py3-none-any.whl (146 kB)\n",
      "     ---------------------------------------- 0.0/146.4 kB ? eta -:--:--\n",
      "     ---------------- ---------------------- 61.4/146.4 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 146.4/146.4 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting transformers>=4.10.0 (from openprompt)\n",
      "  Obtaining dependency information for transformers>=4.10.0 from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 123.5/123.5 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting sentencepiece==0.1.96 (from openprompt)\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 0.1/1.1 MB 2.2 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.2/1.1 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.3/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.4/1.1 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.5/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 0.6/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.6/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 0.7/1.1 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 0.8/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 0.9/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.0/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.1/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.1/1.1 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting tqdm>=4.62.2 (from openprompt)\n",
      "  Obtaining dependency information for tqdm>=4.62.2 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting tensorboardX (from openprompt)\n",
      "  Obtaining dependency information for tensorboardX from https://files.pythonhosted.org/packages/44/71/f3e7c9b2ab67e28c572ab4e9d5fa3499e0d252650f96d8a3a03e26677f53/tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nltk (from openprompt)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting yacs (from openprompt)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting dill (from openprompt)\n",
      "  Obtaining dependency information for dill from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting datasets (from openprompt)\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting rouge==1.0.0 (from openprompt)\n",
      "  Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting pyarrow (from openprompt)\n",
      "  Obtaining dependency information for pyarrow from https://files.pythonhosted.org/packages/28/82/9adfafaf0de581a39a1c86002cafd1a55a1255c9e0d362dc3e970aab0656/pyarrow-14.0.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading pyarrow-14.0.1-cp39-cp39-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openprompt) (1.8.0)\n",
      "Requirement already satisfied: six in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rouge==1.0.0->openprompt) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\eddie\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.62.2->openprompt) (0.4.4)\n",
      "Collecting filelock (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/fc/85/0d1038f068900896a8590d6d0da198b90d31f731a39166a432aa2b92249b/regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (2.25.1)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/9f/90/a6821e7757d2db194c16cbca78c80e206f30f6cc62c7f15fb27428f8c6dd/tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/4e/96/f4ee4434d8b6452fe7d5d44df2e72d1c6b2add1c3a5fb5c81aae83cb90c6/safetensors-0.4.1-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.1-cp39-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->openprompt)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets->openprompt) (1.4.1)\n",
      "Collecting xxhash (from datasets->openprompt)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/33/db/e07aa80e39a7ee82e9ca6f5a0fa9bf0b4465934272116a1b578eaee7f4b3/xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->openprompt)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/c6/c9/820b5ab056f4ada76fbe05bd481a948f287957d6cbfd59e2dd2618b408c1/multiprocess-0.70.15-py39-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets->openprompt)\n",
      "  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets->openprompt) (3.7.4.post0)\n",
      "Requirement already satisfied: click in c:\\users\\eddie\\appdata\\roaming\\python\\python39\\site-packages (from nltk->openprompt) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->openprompt) (1.1.0)\n",
      "Collecting protobuf>=3.20 (from tensorboardX->openprompt)\n",
      "  Obtaining dependency information for protobuf>=3.20 from https://files.pythonhosted.org/packages/b6/4b/f4f3334784576822d7817a664b757030ebb35b981978baf9c2eb3c5f33a8/protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (21.2.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (3.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (5.2.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (1.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers>=4.10.0->openprompt) (3.0.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets->openprompt) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets->openprompt) (2021.3)\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 41.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/7.9 MB 3.2 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.2/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.4/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.4/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.6/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.7/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.8/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.9/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.0/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.1/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.2/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.3/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.4/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.5/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.6/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.7/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.8/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.9/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.2/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.3/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.4/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.5/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.6/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.7/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.8/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.9/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.2/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.3/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.4/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.6/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.7/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.9/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.0/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.1/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.2/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.3/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.5/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.6/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.9/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.0/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.1/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.2/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.3/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.5/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.6/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.7/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.9/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.0/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.1/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.3/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.4/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.5/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.6/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.7/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.8/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.9/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.0/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.3/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.4/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.5/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.6/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.7/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.8/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "   ---------------------------------------- 0.0/521.2 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 92.2/521.2 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 174.1/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 276.5/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 368.6/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 471.0/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 521.2/521.2 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 61.4/115.3 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 115.3/115.3 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow-14.0.1-cp39-cp39-win_amd64.whl (24.6 MB)\n",
      "   ---------------------------------------- 0.0/24.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/24.6 MB 3.2 MB/s eta 0:00:08\n",
      "   ---------------------------------------- 0.2/24.6 MB 2.6 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 0.3/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.4/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.5/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.6/24.6 MB 2.2 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.7/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.8/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.8/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.9/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.0/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.1/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.2/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.3/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.4/24.6 MB 1.9 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.4/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.7/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.8/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.9/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.0/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.1/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.2/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.3/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.4/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.5/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.6/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.7/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.8/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.9/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 3.0/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.2/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.3/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.4/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.5/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.6/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.7/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.8/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.9/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.0/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.2/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.3/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.4/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.5/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.6/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.8/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.9/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.0/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.1/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.2/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.3/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.4/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.5/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.6/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.8/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.9/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.0/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.1/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.2/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.3/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.4/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.5/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.6/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.7/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 6.8/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 6.9/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.0/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.1/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.2/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.3/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.5/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.6/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.7/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.8/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.9/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.0/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.1/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.2/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.3/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.5/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 8.6/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.7/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.8/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.3/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.4/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.5/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.6/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.7/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.8/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 9.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.0/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.3/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.4/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 10.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.6/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.7/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.8/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.9/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 11.0/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.1/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.2/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.4/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.6/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.7/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 11.8/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 11.9/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.0/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.1/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.2/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.7/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.8/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.0/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.1/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.3/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.7/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.8/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.1/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.3/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 14.8/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 14.9/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.0/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.1/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.2/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.3/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.4/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.5/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.8/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.9/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.0/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.1/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.2/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.3/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.4/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.5/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 16.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 16.8/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 16.9/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.0/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.1/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.3/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.4/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.5/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.6/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.7/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.8/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 17.9/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.0/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.1/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.3/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.4/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.5/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.6/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.7/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.7/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.8/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 19.0/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.1/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.2/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.3/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.4/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.5/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.7/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.8/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.0/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.1/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.2/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.3/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.4/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.5/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.7/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.9/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.0/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.1/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.2/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.3/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.4/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.5/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.9/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.0/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.1/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.2/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.3/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.4/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.5/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.7/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.8/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.2/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.3/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.4/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.5/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.6/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.7/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.8/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.2/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.5/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.6/24.6 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.7 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 92.2/101.7 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 101.7/101.7 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 0.0/311.7 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 112.6/311.7 kB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 194.6/311.7 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  307.2/311.7 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 311.7/311.7 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 112.6/413.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 194.6/413.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 307.2/413.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 368.6/413.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 413.4/413.4 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 92.2/269.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 174.1/269.6 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/269.6 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 269.6/269.6 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp39-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.8 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 112.6/277.8 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 194.6/277.8 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 277.8/277.8 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.2 MB 1.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 1.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.4/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.3 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 92.2/133.3 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 133.3/133.3 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl (29 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 71.7/166.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 166.4/166.4 kB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, yacs, xxhash, tqdm, safetensors, rouge, regex, pyarrow-hotfix, pyarrow, protobuf, fsspec, filelock, dill, tensorboardX, nltk, multiprocess, huggingface-hub, tokenizers, transformers, datasets, openprompt\n",
      "Successfully installed datasets-2.15.0 dill-0.3.7 filelock-3.13.1 fsspec-2023.10.0 huggingface-hub-0.19.4 multiprocess-0.70.15 nltk-3.8.1 openprompt-1.0.1 protobuf-4.25.1 pyarrow-14.0.1 pyarrow-hotfix-0.6 regex-2023.10.3 rouge-1.0.0 safetensors-0.4.1 sentencepiece-0.1.96 tensorboardX-2.6.2.2 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.35.2 xxhash-3.4.1 yacs-0.1.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install openprompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import openprompt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.181341Z",
     "start_time": "2023-12-10T03:35:52.654085900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.194342100Z",
     "start_time": "2023-12-10T03:36:10.182341900Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda = True\n",
    "auto_t = True # Whether to perform automatic template generation\n",
    "auto_v = True # Whether to perform automatic verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.226340200Z",
     "start_time": "2023-12-10T03:36:10.195343500Z"
    }
   },
   "outputs": [],
   "source": [
    "from openprompt.data_utils.text_classification_dataset import SST2Processor\n",
    "dataset = {'train': SST2Processor().get_train_examples(\"SST-2/\"),\n",
    "           'validation': SST2Processor().get_dev_examples(\"SST-2/\"),\n",
    "           'test': SST2Processor().get_test_examples(\"SST-2/\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.607530300Z",
     "start_time": "2023-12-10T03:36:10.228340900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: {\n",
      "  \"guid\": \"train-0\",\n",
      "  \"label\": 0,\n",
      "  \"meta\": {\n",
      "    \"labelword\": \"terrible\"\n",
      "  },\n",
      "  \"text_a\": \"nothing happens , and it happens to flat characters .\",\n",
      "  \"text_b\": \"\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print('load model...')\n",
    "from openprompt.plms import load_plm\n",
    "# load mlm model for main tasks\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")\n",
    "\n",
    "# load generation model for template generation\n",
    "template_generate_model, template_generate_tokenizer, template_generate_model_config, template_tokenizer_wrapper = load_plm('t5', 't5-large')\n",
    "\n",
    "from openprompt.prompts import ManualVerbalizer, ManualTemplate\n",
    "verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "#   TODO: You need to switch LMBFFTemplateGenerationTemplate or ManualTemplate to                                 #\n",
    "#         compare auto generate template and manual generate template                                             #\n",
    "###################################################################################################################\n",
    "from openprompt.prompts.prompt_generator import LMBFFTemplateGenerationTemplate\n",
    "########################################\n",
    "#   LMBFFTemplateGenerationTemplate    #\n",
    "########################################\n",
    "import random\n",
    "\n",
    "# number of demonstrations\n",
    "num_demonstrations = 1  # try different number\n",
    "\n",
    "demonstrations = []\n",
    "\n",
    "for _ in range(num_demonstrations):\n",
    "    # random choice training set example with label 0 \n",
    "    random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n",
    "\n",
    "    # random choice training set example with label 1\n",
    "    random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n",
    "    \n",
    "    demonstration = f'{random_example_1.text_a} It was terrible. {random_example_2.text_a} It was great.'\n",
    "    demonstrations.append(demonstration)\n",
    "\n",
    "# You can modify the demonstrations and try different combinations\n",
    "template_text = '{\"placeholder\": \"text_a\"} {\"mask\"} {\"meta\": \"labelword\"} {\"mask\"}.' + ' '.join(demonstrations)\n",
    "template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "\n",
    "#############################################\n",
    "#   End of LMBFFTemplateGenerationTemplate  #\n",
    "#############################################\n",
    "\n",
    "########################################\n",
    "#          ManualTemplate              #\n",
    "########################################\n",
    "\n",
    "#template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} It was {\"mask\"}.')\n",
    "\n",
    "#############################################\n",
    "#          End of ManualTemplate            # \n",
    "#############################################\n",
    "\n",
    "###################################################################################################################\n",
    "#                                           End of your code                                                      #\n",
    "###################################################################################################################\n",
    "\n",
    "\n",
    "# view wrapped example\n",
    "wrapped_example = template.wrap_one_example(dataset['train'][0])\n",
    "print(\"dataset:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.673523500Z",
     "start_time": "2023-12-10T03:36:46.625523700Z"
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "\n",
    "# Returns the best evaluation score achieved during training\n",
    "def fit(model, train_dataloader, val_dataloader, loss_func, optimizer, epochs=5):\n",
    "    best_score = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_dataloader, loss_func, optimizer)\n",
    "        score = evaluate(model, val_dataloader)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "        print(f\"Epoch {epoch+1}: Train loss={train_loss}, Eval score={score}\")\n",
    "    return best_score\n",
    "\n",
    "# Trains the model on the training data and computes the training loss\n",
    "def train_epoch(model, train_dataloader, loss_func, optimizer):\n",
    "    model.train()\n",
    "    loss_all = []\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits #\n",
    "        # 2. Get labels                                     #\n",
    "        # 3. Evalutate using loss_func                      #\n",
    "        # 4. Append loss to loss_all                        #\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits\n",
    "        logits = model(batch=inputs)\n",
    "\n",
    "        # 2. Get labels\n",
    "        labels = inputs['label']\n",
    "\n",
    "        # 3. Evalutate using loss_func\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Append loss to loss_all\n",
    "        loss_all.append(loss.item())\n",
    "        #####################################################\n",
    "        #                 End of your code                  #\n",
    "        #####################################################\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return np.mean(loss_all)\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    with torch.no_grad():\n",
    "        for step, inputs in enumerate(val_dataloader):\n",
    "            if cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits #\n",
    "            # 2. Get labels                                     #\n",
    "            # 3. Extend labels to list                          #\n",
    "            # 4. Get predictions and extend preds to list       #\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits\n",
    "            logits = model(batch=inputs)\n",
    "\n",
    "            # 2. Get labels\n",
    "            labels = inputs['label']\n",
    "\n",
    "            # 3. Extend labels to list\n",
    "            alllabels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # 4. Get predictions and extend preds to list\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            allpreds.extend(preds.cpu().numpy())\n",
    "            #####################################################\n",
    "            #                 End of your code                  #\n",
    "            #####################################################\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generated template from TemplateGenerator and find the best template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:13:36.594464200Z",
     "start_time": "2023-12-10T03:36:46.673523500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 1454.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:37<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"placeholder\": \"text_a\"} It was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} it was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001b[A\n",
      "tokenizing: 32it [00:00, 202.53it/s]A\n",
      "\n",
      "tokenizing: 32it [00:00, 727.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.562330920593297, Eval score=0.5\n",
      "Epoch 2: Train loss=1.029085214715451, Eval score=0.5\n",
      "Epoch 3: Train loss=0.671642615867313, Eval score=0.71875\n",
      "Epoch 4: Train loss=0.2566610455396585, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [15:59<1:03:57, 959.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.2469192902863142, Eval score=0.78125\n",
      "Current template: {\"placeholder\": \"text_a\"} it was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' it was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 379.37it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.511358927668084, Eval score=0.5\n",
      "Epoch 2: Train loss=0.7635227439459413, Eval score=0.5\n",
      "Epoch 3: Train loss=0.3154368309772053, Eval score=0.53125\n",
      "Epoch 4: Train loss=0.8977778584977472, Eval score=0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [36:43<56:21, 1127.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.7606429383413342, Eval score=0.5\n",
      "Current template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 571.42it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1180.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.765889931773188, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.5693292848736746, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.041312409681268036, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.2322409824218994, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [56:36<38:33, 1156.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.22881872234574985, Eval score=0.84375\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 1066.65it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1523.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.7746381501195, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9883591458201408, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8319057337939739, Eval score=0.625\n",
      "Epoch 4: Train loss=0.508084288907412, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [1:16:21<19:27, 1167.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.39746711112820776, Eval score=0.53125\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 864.68it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1454.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.6042319842349002, Eval score=0.5\n",
      "Epoch 2: Train loss=1.069442194304429, Eval score=0.5\n",
      "Epoch 3: Train loss=0.47684725234284997, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.2768472472046142, Eval score=0.59375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:36:08<00:00, 1153.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.07557142606344769, Eval score=0.625\n",
      "Final best template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class ManualTemplateWithoutParse(ManualTemplate):\n",
    "    \"\"\"The generated template from TemplateGenerator is a list of dict of parsed template_text. So no further parsing is needed.\"\"\"\n",
    "    def on_text_set(self):\n",
    "        pass\n",
    "\n",
    "# Template generation\n",
    "if auto_t:\n",
    "    print('performing auto_t...')\n",
    "\n",
    "    if cuda:\n",
    "        template_generate_model = template_generate_model.cuda()\n",
    "    template_generator = T5TemplateGenerator(template_generate_model, template_generate_tokenizer, template_tokenizer_wrapper, verbalizer, beam_width=5) # Beam_width is set to 5 here for efficiency; to improve performance, try a larger number.\n",
    "\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=template_generate_tokenizer, tokenizer_wrapper_class=template_tokenizer_wrapper, batch_size=len(dataset['train']), decoder_max_length=128, max_seq_length=128, shuffle=False, teacher_forcing=False) # Register all data at once\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        template_generator._register_buffer(data)\n",
    "\n",
    "    template_generate_model.eval()\n",
    "    print('generating...')\n",
    "    template_texts = template_generator._get_templates()\n",
    "\n",
    "    original_template = template.text\n",
    "    template_texts = [template_generator.convert_template(template_text, original_template) for template_text in template_texts]\n",
    "    # template_generator._show_template()\n",
    "    template_generator.release_memory()\n",
    "    # Generate a number of candidate template text\n",
    "    print(template_texts)\n",
    "    \n",
    "    # Iterate over each candidate and select the best one\n",
    "    best_metrics = 0.0\n",
    "    best_template_text = None\n",
    "    for template_text in tqdm(template_texts):\n",
    "        verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "        template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "        print(f\"Current template: {template_text}\\n\\nWrapped example: {template.wrap_one_example(dataset['train'][0])}\")\n",
    "\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "        \n",
    "        #######################################################\n",
    "        # TODO: Use score to Find your best template_text     #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_template_text = template_text\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # Use the best template\n",
    "    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "    template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=best_template_text)\n",
    "    print(\"Final best template:\", best_template_text)\n",
    "    print()\n",
    "    print(\"Wrapped example:\", template.wrap_one_example(dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verbalizer template from VerbalizerGenerator and find the best verbalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T11:58:04.838076700Z",
     "start_time": "2023-12-10T05:13:36.638466100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_v...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 1391.30it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current label_words: ['terrible', 'beautiful']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001b[A\n",
      "tokenizing: 3it [00:00, 21.48it/s]\u001b[A\n",
      "tokenizing: 6it [00:01,  5.15it/s]\u001b[A\n",
      "tokenizing: 8it [00:01,  6.27it/s]\u001b[A\n",
      "tokenizing: 32it [00:01, 23.45it/s]\u001b[A\n",
      "\n",
      "tokenizing: 32it [00:00, 969.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.1593105591260544, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.5407680265489034, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.17979280870349612, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.006995583800744498, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [21:05<6:40:40, 1265.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.002051841642924046, Eval score=0.84375\n",
      "current label_words: ['horrible', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 695.63it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1388.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.5843039118335565, Eval score=0.84375\n",
      "Epoch 2: Train loss=0.0798521326687478, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.0026221817748819376, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.0010607897513068565, Eval score=0.78125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [42:02<6:18:07, 1260.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0004625524882726495, Eval score=0.78125\n",
      "current label_words: ['horrible', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 762.07it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1333.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.0936600251085586, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.849827597849071, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.414547988853883, Eval score=0.75\n",
      "Epoch 4: Train loss=0.0500250239797424, Eval score=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [1:02:46<5:55:04, 1253.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.004471211226245941, Eval score=0.8125\n",
      "current label_words: ['sad', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 415.60it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 941.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4185917818103917, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.5065413688316767, Eval score=0.875\n",
      "Epoch 3: Train loss=0.012464412650388113, Eval score=0.875\n",
      "Epoch 4: Train loss=0.002708091426967485, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [1:23:35<5:33:44, 1251.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0004699288858773798, Eval score=0.875\n",
      "current label_words: ['bad', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 551.73it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=3.1230944280390602, Eval score=0.5\n",
      "Epoch 2: Train loss=1.270681380527094, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8551086119841784, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.6224154182709754, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [1:45:07<5:16:30, 1266.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.22766433987999335, Eval score=0.8125\n",
      "current label_words: ['horrible', 'remarkable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 426.62it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1031.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.2276666148868003, Eval score=0.6875\n",
      "Epoch 2: Train loss=0.4889321715090773, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2634941844644345, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.25736280560994373, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [2:07:36<5:01:58, 1294.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.3903749104914027, Eval score=0.625\n",
      "current label_words: ['terrible', 'brilliant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 493.33it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.128300400949797, Eval score=0.5\n",
      "Epoch 2: Train loss=1.0674331626432831, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.6011688623111695, Eval score=0.875\n",
      "Epoch 4: Train loss=0.6810656589186692, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [2:28:49<4:38:53, 1287.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.1570308672144165, Eval score=0.84375\n",
      "current label_words: ['terrible', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 451.65it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1033.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.1662085960851982, Eval score=0.84375\n",
      "Epoch 2: Train loss=0.046194135922632995, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.20158991879031873, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.009880203132524912, Eval score=0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [2:51:16<4:21:16, 1306.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.27703922392970526, Eval score=0.90625\n",
      "current label_words: ['disappointing', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 415.57it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1032.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.3897865504303581, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.6985758165101288, Eval score=0.5625\n",
      "Epoch 3: Train loss=0.20094251398768392, Eval score=0.875\n",
      "Epoch 4: Train loss=0.021359096241212683, Eval score=0.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [3:11:17<3:53:28, 1273.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0014102687238164435, Eval score=0.90625\n",
      "current label_words: ['strange', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 410.23it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4916955009493904, Eval score=0.5\n",
      "Epoch 2: Train loss=0.5601507005485473, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.050391235166898696, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.04447055474645367, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [3:31:10<3:28:06, 1248.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.5673459974156145, Eval score=0.78125\n",
      "current label_words: ['terrible', 'good']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 363.21it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.9351653824437118, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9128216816243366, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.16854792425147025, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.004711857527581742, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [3:50:34<3:03:23, 1222.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0015127657302400621, Eval score=0.84375\n",
      "current label_words: ['bad', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 524.59it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.8022562539517715, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9302898038731655, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.43625156552297994, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.04196147369020764, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [4:10:06<2:40:57, 1207.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.012102704274127518, Eval score=0.65625\n",
      "current label_words: ['short', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 412.44it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1203.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.8797737168388267, Eval score=0.75\n",
      "Epoch 2: Train loss=0.3757321042244257, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2311989847357836, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.43366863449273296, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [4:30:04<2:20:30, 1204.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.620734619528541, Eval score=0.59375\n",
      "current label_words: ['disappointing', 'delightful']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 426.69it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1143.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.324861448905718, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.5381804386270232, Eval score=0.5625\n",
      "Epoch 3: Train loss=0.36967586419450527, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.12384688404563349, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [4:49:18<1:58:55, 1189.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.18691727038913086, Eval score=0.6875\n",
      "current label_words: ['bad', 'brilliant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 490.61it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1103.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.5005056243027184, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.7745201710495166, Eval score=0.875\n",
      "Epoch 3: Train loss=0.13810731278863386, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.004411970109288177, Eval score=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [5:08:29<1:38:08, 1177.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.00313117326692236, Eval score=0.78125\n",
      "current label_words: ['bad', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 492.79it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.182302282977588, Eval score=0.5\n",
      "Epoch 2: Train loss=0.5332906207768247, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.08191546555644891, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.35411459776105403, Eval score=0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [5:26:58<1:17:08, 1157.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.13951104862388775, Eval score=0.46875\n",
      "current label_words: ['horrible', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 466.99it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1219.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4121702450024713, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.32750329683221935, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.01347528245059948, Eval score=0.90625\n",
      "Epoch 4: Train loss=0.3011642301885331, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [5:45:44<57:23, 1147.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.022153147599055956, Eval score=0.875\n",
      "current label_words: ['fine', 'remarkable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 492.08it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1185.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.7259275259780793, Eval score=0.5\n",
      "Epoch 2: Train loss=0.7986743192886934, Eval score=0.65625\n",
      "Epoch 3: Train loss=0.6269949703128077, Eval score=0.5\n",
      "Epoch 4: Train loss=0.16056611831118062, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [6:04:43<38:10, 1145.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.003870869544044808, Eval score=0.78125\n",
      "current label_words: ['disappointing', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 454.61it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1183.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4480454477061357, Eval score=0.5\n",
      "Epoch 2: Train loss=0.8791331336833537, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.4247932309117459, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.2258107879970339, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [6:23:21<18:56, 1137.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0734178872121447, Eval score=0.875\n",
      "current label_words: ['terrible', 'fine']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 489.28it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1164.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.6715138442009447, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.6137157797584223, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.38552796499482156, Eval score=0.875\n",
      "Epoch 4: Train loss=0.34600197029577373, Eval score=0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [6:41:58<00:00, 1205.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.6120385159649686, Eval score=0.78125\n",
      "final best label words: ['terrible', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verbalizer generation\n",
    "from openprompt.prompts.prompt_generator import RobertaVerbalizerGenerator\n",
    "if auto_v:\n",
    "    print('performing auto_v...')\n",
    "    # Load generation model for verbalizer generation\n",
    "    if cuda:\n",
    "        plm = plm.cuda()\n",
    "    verbalizer_generator = RobertaVerbalizerGenerator(model=plm, tokenizer=tokenizer, candidate_num=20, label_word_num_per_class=20)\n",
    "    # To improve performance, try larger numbers\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, batch_size=32)\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        verbalizer_generator.register_buffer(data)\n",
    "    label_words_list = verbalizer_generator.generate()\n",
    "    verbalizer_generator.release_memory()\n",
    "\n",
    "    # Iterate over each candidate and select the best one\n",
    "    current_verbalizer = copy.deepcopy(verbalizer)\n",
    "    best_metrics = 0.0\n",
    "    best_label_words = None\n",
    "    for label_words in tqdm(label_words_list):\n",
    "        print(f\"current label_words: {label_words}\")\n",
    "        current_verbalizer.label_words = label_words\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, current_verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "\n",
    "        #######################################################\n",
    "        # TODO: Use score to find your best_label_word        #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_label_words = label_words\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # use the best verbalizer\n",
    "    print(\"final best label words:\", best_label_words)\n",
    "    verbalizer = ManualVerbalizer(tokenizer, num_classes=2, label_words=best_label_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:18:43.541949400Z",
     "start_time": "2023-12-10T11:58:04.849042700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 383.01it/s]\n",
      "tokenizing: 32it [00:00, 1142.78it/s]\n",
      "tokenizing: 872it [00:00, 1095.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.2980121186483302, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.38737686289732665, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.08258779709103692, Eval score=0.875\n",
      "Epoch 4: Train loss=0.3668047572554656, Eval score=0.84375\n",
      "Epoch 5: Train loss=0.00465579945631589, Eval score=0.84375\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "test_dataloader = PromptDataLoader(dataset['test'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "\n",
    "model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# It's always good practice to set no decay to bias and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/5b8876ed26fd495b8353ad7ce94b6f65)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:46:57.884699300Z",
     "start_time": "2023-12-10T12:18:43.474013600Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "allpreds = []\n",
    "for step, inputs in tqdm(enumerate(test_dataloader)):\n",
    "    if cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = model(inputs)\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(allpreds):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report (15 points)\n",
    "\n",
    "- Task 1: Compare **two** different models you employed and provide a brief discussion of your implementation.\n",
    "\n",
    "- Task 2: You need to try at least **three** different templates and verbalizers to compare how your prompts work with the model. Report your performance in zero-shot, one-shot, and few-shot scenarios, with examples drawn from the training set.\n",
    "\n",
    "- Task 3: Try at least three different manually crafted templates to compare them with auto-generated templates. Evaluate the performance with different numbers of demonstrations and plot the graph from Figure 3 in the paper (https://arxiv.org/pdf/2012.15723.pdf). Also, report your best template and verbalizer.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
