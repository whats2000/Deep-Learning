{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6Z1Snuk7rIK"
   },
   "source": [
    "# MIS 583 Assignment 6: Text Sentiment Classification with Prompt Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSwr9MgZogRZ"
   },
   "source": [
    "Before we start, please put your name and SID in following format: <br>\n",
    ": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷, M114020035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DzsjuDhlz_k"
   },
   "source": [
    "**Your Answer:**   \n",
    "Hi I'm 鄔仁迪, B104020009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d-Zzebq7rIM"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kc9gd_Wk7rIN"
   },
   "source": [
    "**Sentiment Classification** is an automated process of identifying opinions in text and labeling them as positive or negative based on the emotions customers express within them.\n",
    "\n",
    "In Task 1, you need to fine-tune a pre-trained language model (e.g., BERT) to predict the sentiment of given tweets.\n",
    "\n",
    "In Task 2, we employ prompts to enable the model to perform sentiment analysis through in-context learning, eliminating the need for additional training.\n",
    "\n",
    "In Task 3, you will use the method called LM-BFF to utilize the model in generating the optimal template and verbalizer autonomously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice \n",
    "**You are not allow to use the model like GPT family or pre-trained weight using SST-2 and twitter dataset!!!!!!!!!!!!!!!!!**\n",
    "\n",
    "You can use BERT and RoBERTa encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giUId1Naqacs",
    "tags": []
   },
   "source": [
    "##  Versions of used packages\n",
    "\n",
    "We will check PyTorch version to make sure everything work properly.  \n",
    "We use `python==3.7.14`, `torch==1.12.1+cu113` and `torchvision==0.13.1+cu113`.  \n",
    "This is the default version in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:11.995277800Z",
     "start_time": "2023-12-15T17:22:01.289099200Z"
    },
    "id": "Vuw-gNvjqcYe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]\n",
      "torch 2.1.0+cu118\n",
      "torchvision 0.16.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "print('python', sys.version.split('\\n')[0])\n",
    "print('torch', torch.__version__)\n",
    "print('torchvision', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Text Sentiment Classification (40 points)\n",
    "\n",
    "In this task, you need to fine-tune a pre-trained language model (e.g., BERT or RoBERTa encoder) to predict the sentiment of given tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a4s_a5D7rIR"
   },
   "source": [
    "## Loading Model and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPUkTbnL7rIR"
   },
   "source": [
    "First, let's talk about the model. The Hugging Face team has created an amazing framework called \"transformers\" for NLP tasks. It includes many state-of-the-art machine learning models for PyTorch, TensorFlow, and JAX.\n",
    "\n",
    "To start with this package, follow [this link to installation and a basic tutorial](https://pytorch.org/hub/huggingface_pytorch-transformers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:12.037278200Z",
     "start_time": "2023-12-15T17:22:11.997299100Z"
    },
    "id": "rK0ouXa09pDU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!echo happy installation\\n!pip -V\\n!pip install grpcio\\n!pip install google-auth\\n!pip install protobuf==3.9.2\\n!pip install pyprind\\n!pip install tqdm boto3 requests regex sentencepiece sacremoses'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you might need some additional installations there\n",
    "\"\"\"!echo happy installation\n",
    "!pip -V\n",
    "!pip install grpcio\n",
    "!pip install google-auth\n",
    "!pip install protobuf==3.9.2\n",
    "!pip install pyprind\n",
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.040759300Z",
     "start_time": "2023-12-15T17:22:12.010275100Z"
    },
    "id": "dmGCAevi7rIS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "#########################################################################\n",
    "#            Loading tokenizer and model from transformer               #\n",
    "#########################################################################\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, AutoConfig\n",
    "\n",
    "bert_type = 'bert-base-cased'\n",
    "\n",
    "# ---------- 1. load from torch.hub ----------\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_type)\n",
    "\n",
    "# create a Bert-extended task (classification)\n",
    "model = BertForSequenceClassification.from_pretrained(bert_type)\n",
    "\n",
    "# finetune from the output from bert to your task\n",
    "model.classifier = nn.Linear(in_features=768, out_features=3, bias=True)\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiMThsYeDa2O"
   },
   "source": [
    "## How to Get Data\n",
    "\n",
    "Please open the file `twitter_sentiment.zip`, creat shortcut to your Google Drive.\n",
    "\n",
    "1. open [LINK of Google Drive](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
    "3. Select the location where you want to place the shortcut.\n",
    "4. Click Add shortcut.\n",
    "\n",
    "After above procedures, we have a shortcut of zip file of dataset.  \n",
    "We can access this in colab after granting the permission of Google Drive.\n",
    "\n",
    "---\n",
    "\n",
    "請先到共用雲端硬碟將檔案 `twitter_sentiment.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.052762Z",
     "start_time": "2023-12-15T17:22:15.012758200Z"
    },
    "id": "lZnFgi5i_2oA"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqO8DiB6VRQZ"
   },
   "source": [
    "## Unzip Data\n",
    "\n",
    "解壓縮 `twitter_sentiment.zip` 後可以發現裡面有三個csv檔。\n",
    "\n",
    "- `train.csv`, `test.csv` and `val.csv`\n",
    "\n",
    "Training set 有 **10248** 筆資料.  \n",
    "Validation set 有 **1317** 筆資料.  \n",
    "Testing set 有 **3075** 筆資料.  \n",
    "\n",
    "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.052762Z",
     "start_time": "2023-12-15T17:22:15.040759300Z"
    },
    "id": "OSlTMdxf8Zd7"
   },
   "outputs": [],
   "source": [
    "# !unzip -qq ./drive/MyDrive/twitter_sentiment.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.065760100Z",
     "start_time": "2023-12-15T17:22:15.045759400Z"
    },
    "id": "wf5GXTme7rIT"
   },
   "outputs": [],
   "source": [
    "# Utility function to extract text and label from csv file\n",
    "def get_texts(f_name='./twitter_sentiment', mode='train'):\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "\n",
    "    f_path = os.path.join(f_name, '{}.csv'.format(mode))\n",
    "    with open(f_path, encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for line in reader:\n",
    "            text_list.append(line['text'])\n",
    "            if mode != 'test':\n",
    "                label_list.append(int(line['sentiment_label']))\n",
    "\n",
    "    return text_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.102758500Z",
     "start_time": "2023-12-15T17:22:15.061758900Z"
    },
    "id": "6fpY0ZrK7rIV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, f_name='./twitter_sentiment', mode='train'):\n",
    "        self.mode = mode\n",
    "\n",
    "        text_list, label_list = get_texts(f_name, mode)\n",
    "        print('mode', mode, 'has', len(text_list), 'datas')\n",
    "        text_list = tokenizer(text_list,\n",
    "                             truncation=True, padding=True,\n",
    "                             return_tensors='pt')\n",
    "\n",
    "        self.text_list = text_list['input_ids']\n",
    "        self.mask_list = text_list['attention_mask']\n",
    "\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_list[idx]\n",
    "        mask = self.mask_list[idx]\n",
    "        if self.mode == 'test':\n",
    "            return text, mask\n",
    "        label = torch.tensor(self.label_list[idx])\n",
    "        return text, mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.910260600Z",
     "start_time": "2023-12-15T17:22:15.074759400Z"
    },
    "id": "nCmM4FSw7rIW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode train has 10248 datas\n",
      "mode val has 1317 datas\n",
      "mode test has 3075 datas\n"
     ]
    }
   ],
   "source": [
    "dataset_train = TwitterDataset(mode='train')\n",
    "dataset_val = TwitterDataset(mode='val')\n",
    "dataset_test = TwitterDataset(mode='test')\n",
    "\n",
    "batch_size = 32\n",
    "train_data = DataLoader(dataset_train, batch_size=batch_size,\n",
    "                       shuffle=True)\n",
    "val_data = DataLoader(dataset_val, batch_size=batch_size // 2,\n",
    "                       shuffle=False)\n",
    "test_data = DataLoader(dataset_test, batch_size=batch_size // 2,\n",
    "                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:15.927261400Z",
     "start_time": "2023-12-15T17:22:15.912261400Z"
    },
    "id": "bqkvofHc7rIY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['[CLS]', '@', 'united', 'I', 'have', 'never', 'been', 'mi', '##sle', '##ad', 'by', 'a', 'company', 'as', 'many', 'times', 'as', 'I', 'have', 'this', 'week', 'by', 'United', 'Airlines', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "token to s [CLS] @ united I have never been mislead by a company as many times as I have this week by United Airlines! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(dataset_train[0][0])\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:17.336407400Z",
     "start_time": "2023-12-15T17:22:15.930261900Z"
    },
    "id": "DxZrfCqW7rIY"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# 定義標籤平滑化的KL損失函數 Paper: https://arxiv.org/pdf/2312.06522.pdf\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = -1\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "\n",
    "# 設定標籤平滑化水平\n",
    "\"\"\"\n",
    "LS2: smoothing=0.03（即3%平滑化）\n",
    "LS3: smoothing=0.075（即7.5%平滑化）\n",
    "LS4: smoothing=0.15（即15%平滑化）\n",
    "LS5: smoothing=0.3（即30%平滑化）\n",
    "\"\"\"\n",
    "criterion = LabelSmoothingLoss(classes=3, smoothing=0.15)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpwgE2Gd7rIZ"
   },
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:17.352406900Z",
     "start_time": "2023-12-15T17:22:17.336407400Z"
    },
    "id": "zlaiAZAD7rIa"
   },
   "outputs": [],
   "source": [
    "def accuracy(raw_preds, y):\n",
    "    preds = raw_preds.argmax(dim=1)\n",
    "    acc = (preds == y).sum()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:22:18.138005700Z",
     "start_time": "2023-12-15T17:22:17.359407300Z"
    },
    "id": "dmc_Gms97rIa"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Testing process                              #\n",
    "        #########################################################################\n",
    "        # 1. Clean the gradients of optimizer\n",
    "        # 2. Put correct variables into model\n",
    "        # 3. Get prediction\n",
    "        # 4. Evalutate by criterion and accuracy\n",
    "\n",
    "        # Clean the gradients of optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(text, attention_mask=mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.logits, label)\n",
    "\n",
    "        # Compute accuracy using the provided function\n",
    "        acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "def test(model, data, criterion, log_loss=False):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total = 0\n",
    "    for text, mask, label in tqdm(data, total=len(data)):\n",
    "        text = text.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        #########################################################################\n",
    "        #                          Training process                             #\n",
    "        #########################################################################\n",
    "        # 1. Put correct variables into model\n",
    "        # 2. Get prediction\n",
    "        # 3. Evalutate by criterion and accuracy\n",
    "\n",
    "        # No gradient calculation for evaluation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(text, attention_mask=mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.logits, label)\n",
    "\n",
    "            # Compute accuracy\n",
    "            acc = accuracy(outputs.logits, label)\n",
    "        #########################################################################\n",
    "        #                          End of your code                             #\n",
    "        #########################################################################\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if log_loss:\n",
    "            val_loss_list.append(loss.item())\n",
    "        epoch_acc += acc.item()\n",
    "        total += len(text)\n",
    "    return epoch_loss / total, epoch_acc / total\n",
    "\n",
    "# class for monitoring train and test acc/loss\n",
    "class Meter:\n",
    "    def __init__(self):\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def update(self, train_loss, train_acc, val_loss, val_acc):\n",
    "        self.train_loss_list.append(train_loss)\n",
    "        self.train_acc_list.append(train_acc)\n",
    "        self.val_loss_list.append(val_loss)\n",
    "        self.val_acc_list.append(val_acc)\n",
    "\n",
    "    def plot(self):\n",
    "        x = range(len(self.train_loss_list))\n",
    "        plt.plot(x, self.train_loss_list)\n",
    "        plt.plot(x, self.val_loss_list, color='r')\n",
    "        plt.legend(['train_loss', 'val_loss'])\n",
    "        plt.show()\n",
    "        plt.plot(x, self.train_acc_list)\n",
    "        plt.plot(x, self.val_acc_list, color='r')\n",
    "        plt.legend(['train_acc', 'val_acc'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExZyrKd57rIb"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:29:31.261703700Z",
     "start_time": "2023-12-15T17:22:18.142999600Z"
    },
    "id": "bVDe-fRe7rIc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:25<00:00,  3.74it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 24.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 0.025294102385209745 train_acc: 0.7611241217798594\n",
      "Epoch 1 val_loss:  0.04674747538729518 val_acc : 0.8352315869400152\n",
      "---------- e 1 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:25<00:00,  3.76it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 24.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train_loss: 0.02228542480103026 train_acc: 0.8529469164715067\n",
      "Epoch 2 val_loss:  0.04558997056478791 val_acc : 0.8367501898253606\n",
      "---------- e 2 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:26<00:00,  3.73it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 23.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train_loss: 0.020824312849681326 train_acc: 0.8951990632318502\n",
      "Epoch 3 val_loss:  0.045737336735519024 val_acc : 0.8458618071374335\n",
      "---------- e 3 save best model ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:26<00:00,  3.70it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 23.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train_loss: 0.01958170957764455 train_acc: 0.9299375487900078\n",
      "Epoch 4 val_loss:  0.04605197354347849 val_acc : 0.8451025056947609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [01:27<00:00,  3.68it/s]\n",
      "100%|██████████| 83/83 [00:03<00:00, 23.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train_loss: 0.018761460561011564 train_acc: 0.9514051522248244\n",
      "Epoch 5 val_loss:  0.04736869015653838 val_acc : 0.8344722854973424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#                          Hyper-parameters                             #\n",
    "#########################################################################\n",
    "max_epoch = 5\n",
    "log_interval = 1\n",
    "best_acc = 0\n",
    "#########################################################################\n",
    "#                          End of your code                             #\n",
    "#########################################################################\n",
    "\n",
    "m = Meter()\n",
    "\n",
    "for epoch in range(1, max_epoch + 1):\n",
    "    train_loss, train_acc = train(model, train_data, optimizer, criterion)\n",
    "    val_loss, val_acc = test(model, val_data, criterion, log_loss=True)\n",
    "\n",
    "    if epoch % log_interval == 0:\n",
    "        print('Epoch {} train_loss: {} train_acc: {}'.format(\n",
    "            epoch, train_loss, train_acc\n",
    "        ))\n",
    "        print('Epoch {} val_loss:  {} val_acc : {}'.format(\n",
    "            epoch, val_loss, val_acc\n",
    "        ))\n",
    "\n",
    "    m.update(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "    # model checkpoint\n",
    "    if val_acc > best_acc:\n",
    "        best_model = model\n",
    "        best_acc = val_acc\n",
    "        print('-'*10, 'e', epoch, 'save best model', '-'*10)\n",
    "        torch.save(model.state_dict(), 'ckpts/e{}.pt'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:29:31.468711500Z",
     "start_time": "2023-12-15T17:29:31.263701400Z"
    },
    "id": "SmtW58OR7rIc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot them out\n",
    "m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcJcHf7n7rId"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/f072e95f51bc48978225941dba218241).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:30:20.742309100Z",
     "start_time": "2023-12-15T17:30:20.255626800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = model\n",
    "best_model.load_state_dict(torch.load('ckpts/e2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T17:30:29.887422500Z",
     "start_time": "2023-12-15T17:30:21.670846900Z"
    },
    "id": "Sf5UTlMZ7rId"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:08<00:00, 23.43it/s]\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "\n",
    "total_out = []\n",
    "for text, mask in tqdm(test_data, total=len(test_data)):\n",
    "    text = text.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    output = best_model(text, mask)\n",
    "    pred = output.logits\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    total_out.append(pred)\n",
    "\n",
    "total_out = torch.cat(total_out).cpu().numpy().tolist()\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(total_out):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T15:32:25.986455900Z",
     "start_time": "2023-12-10T15:32:24.748887300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save best model\n",
    "torch.save(best_model.state_dict(), 'ckpts/bert_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: In-Context learning (32 points)\n",
    "\n",
    "In this task, you will learn how to perform sentiment classification using prompts without the need for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:45.402983900Z",
     "start_time": "2023-12-14T04:40:36.482722600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyprind\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig, BertTokenizer, BertForMaskedLM\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.517694500Z",
     "start_time": "2023-12-14T04:40:45.404983600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "#         TODO: Design your own template(prefix) and verbalizer         #\n",
    "#########################################################################\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.prefix = 'This is [MASK] sentence.'\n",
    "\n",
    "        # Define a more comprehensive verbalizer\n",
    "        self.verbalizer = {\n",
    "            \"negative\": 0,\n",
    "            \"neutral\": 1,\n",
    "            \"positive\": 2,\n",
    "        }\n",
    "\n",
    "        self.max_seq_length = 512\n",
    "        self.batch_size = 32\n",
    "\n",
    "\n",
    "config = Config()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bert_type = 'bert-base-uncased'\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(bert_type, num_labels=3)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
    "\n",
    "bert_config = BertConfig.from_pretrained(bert_type)\n",
    "\n",
    "bert = model.from_pretrained(bert_type, config=bert_config).to(device)\n",
    "\n",
    "#######################################################################\n",
    "#                        End of your code                             #\n",
    "#######################################################################\n",
    "\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.520531400Z",
     "start_time": "2023-12-14T04:40:49.519021900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to obtaion verbalizer ids\n",
    "def obtain_verbalizer_ids(verbalizer, tokenizer):\n",
    "    verbalizer_ids = tokenizer.convert_tokens_to_ids(list(verbalizer.keys()))\n",
    "    index2ids = {i: verbalizer_ids[i] for i in range(len(verbalizer_ids))}\n",
    "    return verbalizer_ids, index2ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.528860Z",
     "start_time": "2023-12-14T04:40:49.520531400Z"
    }
   },
   "outputs": [],
   "source": [
    "verbalizer_ids, index2ids = obtain_verbalizer_ids(config.verbalizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.536240100Z",
     "start_time": "2023-12-14T04:40:49.530869700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to concatenate prefix and text\n",
    "def concatenate_prefix(texts, config):\n",
    "    ##################################################\n",
    "    #   TODO: concatenate your own prefix and text   #                               \n",
    "    ##################################################\n",
    "    prefix_texts = [config.prefix + \" \" + text for text in texts]\n",
    "    ##################################################\n",
    "    #                 End of your code               #                               \n",
    "    ##################################################\n",
    "    return prefix_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.571086Z",
     "start_time": "2023-12-14T04:40:49.537240100Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(config):\n",
    "    # ['texts', 'labels']\n",
    "    df = pd.read_csv('./twitter_sentiment/train.csv')\n",
    "    original_texts = df['text'].tolist()\n",
    "    labels = df['sentiment_label'].tolist()\n",
    "\n",
    "    texts = concatenate_prefix(original_texts, config)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "texts, labels = load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.571086Z",
     "start_time": "2023-12-14T04:40:49.566484500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token ['[CLS]', 'this', 'is', '[MASK]', 'sentence', '.', '@', 'united', 'i', 'have', 'never', 'been', 'mis', '##lea', '##d', 'by', 'a', 'company', 'as', 'many', 'times', 'as', 'i', 'have', 'this', 'week', 'by', 'united', 'airlines', '!', '[SEP]']\n",
      "token to s [CLS] this is [MASK] sentence . @ united i have never been mislead by a company as many times as i have this week by united airlines ! [SEP]\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.convert_ids_to_tokens(tokenizer.encode(texts[0], add_special_tokens=True))\n",
    "print('token', t)\n",
    "print('token to s', tokenizer.convert_tokens_to_string(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.578533300Z",
     "start_time": "2023-12-14T04:40:49.572086600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Batching of texts and labels for training or processing in batches\n",
    "def pack_batch(texts, labels, batch_size):\n",
    "    \"\"\"\n",
    "    :param texts: list\n",
    "    :param labels: list\n",
    "    :param batch_size: int\n",
    "    :return batch_X: list\n",
    "            [[text11, text12, ...], [text21, text22, ...], ...]\n",
    "    :return batch_y: list\n",
    "            [[label11, label12, ...], [label21, label22, ...], ...]\n",
    "    :return batch_count: int\n",
    "    \"\"\"\n",
    "    assert len(texts) == len(labels)\n",
    "\n",
    "    if len(texts) % batch_size != 0:\n",
    "        flag = False\n",
    "        batch_count = int(len(texts) / batch_size) + 1\n",
    "    else:\n",
    "        flag = True\n",
    "        batch_count = int(len(texts) / batch_size)\n",
    "\n",
    "    batch_X, batch_y = [], []\n",
    "\n",
    "    if flag:\n",
    "        for i in range(batch_count):\n",
    "            batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "            batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "    else:\n",
    "        for i in range(batch_count):\n",
    "            if i == batch_count - 1:\n",
    "                batch_X.append(texts[i * batch_size:])\n",
    "                batch_y.append(labels[i * batch_size:])\n",
    "            else:\n",
    "                batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
    "                batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
    "\n",
    "    return batch_X, batch_y, batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:40:49.586639100Z",
     "start_time": "2023-12-14T04:40:49.579533300Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_X, batch_y, batch_count = pack_batch(texts, labels, config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-14T04:52:24.649661500Z",
     "start_time": "2023-12-14T04:40:49.590638700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[100 %] Time elapsed: 00:11:35 | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.643638 | precision: 0.578361 | recall: 0.643638 | f1: 0.557020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:11:35\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    pper = pyprind.ProgPercent(batch_count)\n",
    "    for i in range(batch_count):\n",
    "        inputs = batch_X[i]\n",
    "        labels = batch_y[i]\n",
    "\n",
    "        tokens = tokenizer.batch_encode_plus(inputs, add_special_tokens=True,\n",
    "                                             max_length=config.max_seq_length,\n",
    "                                             padding='max_length', truncation=True)\n",
    "        \n",
    "        ids = torch.tensor(tokens['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(tokens['attention_mask']).to(device)\n",
    "\n",
    "        # Shape: (batch_size, max_seq_length, vocab_size)\n",
    "        logits = bert(ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        mask_token_index = (ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "\n",
    "        # Find [MASK] logits\n",
    "        # shape: (batch_size, vocab_size)\n",
    "        masked_logits = logits[mask_token_index[0], mask_token_index[1], :]\n",
    "\n",
    "        # Extract the logits of the word in the verbalizer at the [MASK] position\n",
    "        # shape: (batch_size, verbalizer_size)\n",
    "        verbalizer_logits = masked_logits[:, verbalizer_ids]\n",
    "\n",
    "        # Construct a pseudo-distribution from the logits in these verbalizers\n",
    "        pseudo_distribution = softmax(verbalizer_logits)\n",
    "\n",
    "        #################################################################################\n",
    "        #   1. Find the index with the maximum probability in the pseudo-distribution   #\n",
    "        #   2. Convert the index to the corresponding word ID                           #\n",
    "        #   3. Convert the ID to a token                                                #\n",
    "        #   4. Find the label corresponding to the token                                #\n",
    "        #################################################################################\n",
    "        # 1. Find the index with the maximum probability in the pseudo-distribution\n",
    "        pred_indices = torch.argmax(pseudo_distribution, dim=1)\n",
    "\n",
    "        # 2. Convert the index to the corresponding word ID\n",
    "        pred_ids = [verbalizer_ids[index] for index in pred_indices]\n",
    "\n",
    "        # 3. Convert the ID to a token\n",
    "        pred_tokens = tokenizer.convert_ids_to_tokens(pred_ids)\n",
    "\n",
    "        # 4. Find the label corresponding to the token\n",
    "        pred_labels = [config.verbalizer[token] if token in config.verbalizer else -1 for token in pred_tokens]\n",
    "        pred_labels = np.array(pred_labels)\n",
    "        #################################################################################\n",
    "        #                             End of your code                                  #                                       \n",
    "        #################################################################################\n",
    "\n",
    "        predict_all = np.append(predict_all, pred_labels)\n",
    "        labels_all = np.append(labels_all, labels)\n",
    "\n",
    "        pper.update()\n",
    "\n",
    "    acc = accuracy_score(labels_all, predict_all)\n",
    "    p = precision_score(labels_all, predict_all, average=\"weighted\")\n",
    "    r = recall_score(labels_all, predict_all, average=\"weighted\")\n",
    "    f1 = f1_score(labels_all, predict_all, average=\"weighted\")\n",
    "\n",
    "    print('accuracy: %f | precision: %f | recall: %f | f1: %f' % (acc, p, r, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: LM-BFF (45 points)\n",
    "\n",
    "https://arxiv.org/pdf/2012.15723.pdf\n",
    "\n",
    "Unlike the previous task, LM-BFF can generate templates and verbalizers automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請先到共用雲端硬碟將檔案 `SST-2.zip`，建立捷徑到自己的雲端硬碟中。\n",
    "\n",
    "> 操作步驟\n",
    "1. 點開雲端[連結](https://drive.google.com/file/d/14MDYFasXU94dUE9DjgfcZE61iTRI2007/view?usp=sharing)\n",
    "2. 點選右上角「新增雲端硬碟捷徑」\n",
    "3. 點選「我的雲端硬碟」\n",
    "4. 點選「新增捷徑」\n",
    "\n",
    "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install openprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T03:31:13.543193800Z",
     "start_time": "2023-12-09T03:30:21.808076200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openprompt\n",
      "  Downloading openprompt-1.0.1-py3-none-any.whl (146 kB)\n",
      "     ---------------------------------------- 0.0/146.4 kB ? eta -:--:--\n",
      "     ---------------- ---------------------- 61.4/146.4 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 146.4/146.4 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting transformers>=4.10.0 (from openprompt)\n",
      "  Obtaining dependency information for transformers>=4.10.0 from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 123.5/123.5 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting sentencepiece==0.1.96 (from openprompt)\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 0.1/1.1 MB 2.2 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.2/1.1 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.3/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.4/1.1 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.5/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 0.6/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.6/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 0.7/1.1 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 0.8/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 0.9/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.0/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.1/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.1/1.1 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting tqdm>=4.62.2 (from openprompt)\n",
      "  Obtaining dependency information for tqdm>=4.62.2 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting tensorboardX (from openprompt)\n",
      "  Obtaining dependency information for tensorboardX from https://files.pythonhosted.org/packages/44/71/f3e7c9b2ab67e28c572ab4e9d5fa3499e0d252650f96d8a3a03e26677f53/tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nltk (from openprompt)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting yacs (from openprompt)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting dill (from openprompt)\n",
      "  Obtaining dependency information for dill from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting datasets (from openprompt)\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/e2/cf/db41e572d7ed958e8679018f8190438ef700aeb501b62da9e1eed9e4d69a/datasets-2.15.0-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting rouge==1.0.0 (from openprompt)\n",
      "  Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting pyarrow (from openprompt)\n",
      "  Obtaining dependency information for pyarrow from https://files.pythonhosted.org/packages/28/82/9adfafaf0de581a39a1c86002cafd1a55a1255c9e0d362dc3e970aab0656/pyarrow-14.0.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading pyarrow-14.0.1-cp39-cp39-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openprompt) (1.8.0)\n",
      "Requirement already satisfied: six in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rouge==1.0.0->openprompt) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\eddie\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.62.2->openprompt) (0.4.4)\n",
      "Collecting filelock (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/fc/85/0d1038f068900896a8590d6d0da198b90d31f731a39166a432aa2b92249b/regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers>=4.10.0->openprompt) (2.25.1)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/9f/90/a6821e7757d2db194c16cbca78c80e206f30f6cc62c7f15fb27428f8c6dd/tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers>=4.10.0->openprompt)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/4e/96/f4ee4434d8b6452fe7d5d44df2e72d1c6b2add1c3a5fb5c81aae83cb90c6/safetensors-0.4.1-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.1-cp39-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->openprompt)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets->openprompt) (1.4.1)\n",
      "Collecting xxhash (from datasets->openprompt)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/33/db/e07aa80e39a7ee82e9ca6f5a0fa9bf0b4465934272116a1b578eaee7f4b3/xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->openprompt)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/c6/c9/820b5ab056f4ada76fbe05bd481a948f287957d6cbfd59e2dd2618b408c1/multiprocess-0.70.15-py39-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets->openprompt)\n",
      "  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets->openprompt) (3.7.4.post0)\n",
      "Requirement already satisfied: click in c:\\users\\eddie\\appdata\\roaming\\python\\python39\\site-packages (from nltk->openprompt) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk->openprompt) (1.1.0)\n",
      "Collecting protobuf>=3.20 (from tensorboardX->openprompt)\n",
      "  Obtaining dependency information for protobuf>=3.20 from https://files.pythonhosted.org/packages/b6/4b/f4f3334784576822d7817a664b757030ebb35b981978baf9c2eb3c5f33a8/protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (21.2.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (3.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (5.2.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (1.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets->openprompt) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers>=4.10.0->openprompt) (3.0.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers>=4.10.0->openprompt) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets->openprompt) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\eddie\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets->openprompt) (2021.3)\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 41.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/7.9 MB 3.2 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.2/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.4/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.4/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.6/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.7/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.8/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.9/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.0/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.1/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.2/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.3/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.4/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.5/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.6/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.7/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 1.8/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.9/7.9 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.2/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.3/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.4/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.5/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.6/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.7/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.8/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.9/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.0/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.1/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.2/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.3/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.4/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.6/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 3.7/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.8/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 3.9/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.0/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.1/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.2/7.9 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 4.3/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.5/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.6/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.9/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.0/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.1/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.2/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.3/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.4/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.5/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.6/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.7/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.8/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.9/7.9 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.0/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.1/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.3/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.4/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.5/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.6/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.7/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.8/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.9/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.0/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.2/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.3/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.4/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.5/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.6/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.7/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.8/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "   ---------------------------------------- 0.0/521.2 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 92.2/521.2 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 174.1/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 276.5/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 368.6/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 471.0/521.2 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 521.2/521.2 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 61.4/115.3 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 115.3/115.3 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow-14.0.1-cp39-cp39-win_amd64.whl (24.6 MB)\n",
      "   ---------------------------------------- 0.0/24.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/24.6 MB 3.2 MB/s eta 0:00:08\n",
      "   ---------------------------------------- 0.2/24.6 MB 2.6 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 0.3/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.4/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.5/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "    --------------------------------------- 0.6/24.6 MB 2.2 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.7/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.8/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.8/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.9/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.0/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.1/24.6 MB 2.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.2/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.3/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.4/24.6 MB 1.9 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.4/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.6/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.7/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 1.8/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.9/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.0/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.1/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.2/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.3/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 2.4/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.5/24.6 MB 2.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 2.6/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.7/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.8/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 2.9/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 3.0/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.2/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.3/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.4/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.5/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.6/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 3.7/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.8/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 3.9/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.0/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.1/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 4.2/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.3/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.4/24.6 MB 2.0 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 4.5/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.6/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.8/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 4.9/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.0/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.1/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.2/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.3/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.4/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 5.5/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.6/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.7/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.8/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 5.9/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.0/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.1/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.2/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.3/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.4/24.6 MB 2.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 6.5/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.6/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 6.7/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 6.8/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 6.9/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.0/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.1/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.2/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.3/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.5/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.6/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.7/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.8/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 7.9/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.0/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.1/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.2/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.3/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.4/24.6 MB 2.0 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.5/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 8.6/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.7/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.8/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 8.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 9.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.3/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.4/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.5/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.6/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.7/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 9.8/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 9.9/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.0/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.1/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.2/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.3/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 10.4/24.6 MB 2.0 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 10.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.6/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.7/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.8/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 10.9/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 11.0/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.1/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.2/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.4/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.6/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.7/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 11.8/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 11.9/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.0/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.1/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.2/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.3/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.5/24.6 MB 2.0 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 12.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.7/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.8/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.0/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.1/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.3/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.7/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.8/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 13.9/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.1/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 14.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.2/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.3/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.4/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.5/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.6/24.6 MB 2.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 14.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 14.8/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 14.9/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.0/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.1/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.2/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.3/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 15.4/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.5/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.8/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 15.9/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.0/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.1/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.2/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.3/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.4/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.5/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 16.6/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 16.7/24.6 MB 2.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 16.8/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 16.9/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.0/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.1/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.3/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.4/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.5/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.6/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.7/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 17.8/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 17.9/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.0/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.1/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.2/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.3/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 18.4/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.5/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.6/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.7/24.6 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 18.7/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.8/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 19.0/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.1/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.2/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.3/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.4/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.5/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.7/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.8/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.9/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.0/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.1/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.2/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 20.3/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.4/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.5/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.6/24.6 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 20.7/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.9/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.0/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.1/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.2/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.3/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.4/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.5/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.9/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.0/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.1/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.2/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.3/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.4/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.5/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.6/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.7/24.6 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.8/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.2/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.3/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.4/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.5/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.6/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.7/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.8/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.2/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.5/24.6 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.6 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.6/24.6 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.7 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 92.2/101.7 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 101.7/101.7 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 0.0/311.7 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 112.6/311.7 kB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 194.6/311.7 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  307.2/311.7 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 311.7/311.7 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 112.6/413.4 kB 2.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 194.6/413.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 307.2/413.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 368.6/413.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 413.4/413.4 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 92.2/269.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 174.1/269.6 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  266.2/269.6 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 269.6/269.6 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp39-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.8 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 112.6/277.8 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 194.6/277.8 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 277.8/277.8 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.2 MB 1.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 1.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.4/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.0/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.0 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.3 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 92.2/133.3 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 133.3/133.3 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl (29 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 71.7/166.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 166.4/166.4 kB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, yacs, xxhash, tqdm, safetensors, rouge, regex, pyarrow-hotfix, pyarrow, protobuf, fsspec, filelock, dill, tensorboardX, nltk, multiprocess, huggingface-hub, tokenizers, transformers, datasets, openprompt\n",
      "Successfully installed datasets-2.15.0 dill-0.3.7 filelock-3.13.1 fsspec-2023.10.0 huggingface-hub-0.19.4 multiprocess-0.70.15 nltk-3.8.1 openprompt-1.0.1 protobuf-4.25.1 pyarrow-14.0.1 pyarrow-hotfix-0.6 regex-2023.10.3 rouge-1.0.0 safetensors-0.4.1 sentencepiece-0.1.96 tensorboardX-2.6.2.2 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.35.2 xxhash-3.4.1 yacs-0.1.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install openprompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import openprompt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.181341Z",
     "start_time": "2023-12-10T03:35:52.654085900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.194342100Z",
     "start_time": "2023-12-10T03:36:10.182341900Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda = True\n",
    "auto_t = True # Whether to perform automatic template generation\n",
    "auto_v = True # Whether to perform automatic verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:10.226340200Z",
     "start_time": "2023-12-10T03:36:10.195343500Z"
    }
   },
   "outputs": [],
   "source": [
    "from openprompt.data_utils.text_classification_dataset import SST2Processor\n",
    "dataset = {'train': SST2Processor().get_train_examples(\"SST-2/\"),\n",
    "           'validation': SST2Processor().get_dev_examples(\"SST-2/\"),\n",
    "           'test': SST2Processor().get_test_examples(\"SST-2/\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.607530300Z",
     "start_time": "2023-12-10T03:36:10.228340900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: {\n",
      "  \"guid\": \"train-0\",\n",
      "  \"label\": 0,\n",
      "  \"meta\": {\n",
      "    \"labelword\": \"terrible\"\n",
      "  },\n",
      "  \"text_a\": \"nothing happens , and it happens to flat characters .\",\n",
      "  \"text_b\": \"\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print('load model...')\n",
    "from openprompt.plms import load_plm\n",
    "# load mlm model for main tasks\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")\n",
    "\n",
    "# load generation model for template generation\n",
    "template_generate_model, template_generate_tokenizer, template_generate_model_config, template_tokenizer_wrapper = load_plm('t5', 't5-large')\n",
    "\n",
    "from openprompt.prompts import ManualVerbalizer, ManualTemplate\n",
    "verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "#   TODO: You need to switch LMBFFTemplateGenerationTemplate or ManualTemplate to                                 #\n",
    "#         compare auto generate template and manual generate template                                             #\n",
    "###################################################################################################################\n",
    "from openprompt.prompts.prompt_generator import LMBFFTemplateGenerationTemplate\n",
    "########################################\n",
    "#   LMBFFTemplateGenerationTemplate    #\n",
    "########################################\n",
    "import random\n",
    "\n",
    "# number of demonstrations\n",
    "num_demonstrations = 1  # try different number\n",
    "\n",
    "demonstrations = []\n",
    "\n",
    "for _ in range(num_demonstrations):\n",
    "    # random choice training set example with label 0 \n",
    "    random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n",
    "\n",
    "    # random choice training set example with label 1\n",
    "    random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n",
    "    \n",
    "    demonstration = f'{random_example_1.text_a} It was terrible. {random_example_2.text_a} It was great.'\n",
    "    demonstrations.append(demonstration)\n",
    "\n",
    "# You can modify the demonstrations and try different combinations\n",
    "template_text = '{\"placeholder\": \"text_a\"} {\"mask\"} {\"meta\": \"labelword\"} {\"mask\"}.' + ' '.join(demonstrations)\n",
    "template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "\n",
    "#############################################\n",
    "#   End of LMBFFTemplateGenerationTemplate  #\n",
    "#############################################\n",
    "\n",
    "########################################\n",
    "#          ManualTemplate              #\n",
    "########################################\n",
    "\n",
    "#template = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} It was {\"mask\"}.')\n",
    "\n",
    "#############################################\n",
    "#          End of ManualTemplate            # \n",
    "#############################################\n",
    "\n",
    "###################################################################################################################\n",
    "#                                           End of your code                                                      #\n",
    "###################################################################################################################\n",
    "\n",
    "\n",
    "# view wrapped example\n",
    "wrapped_example = template.wrap_one_example(dataset['train'][0])\n",
    "print(\"dataset:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T03:36:46.673523500Z",
     "start_time": "2023-12-10T03:36:46.625523700Z"
    }
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
    "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "\n",
    "# Returns the best evaluation score achieved during training\n",
    "def fit(model, train_dataloader, val_dataloader, loss_func, optimizer, epochs=5):\n",
    "    best_score = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_dataloader, loss_func, optimizer)\n",
    "        score = evaluate(model, val_dataloader)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "        print(f\"Epoch {epoch+1}: Train loss={train_loss}, Eval score={score}\")\n",
    "    return best_score\n",
    "\n",
    "# Trains the model on the training data and computes the training loss\n",
    "def train_epoch(model, train_dataloader, loss_func, optimizer):\n",
    "    model.train()\n",
    "    loss_all = []\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits #\n",
    "        # 2. Get labels                                     #\n",
    "        # 3. Evalutate using loss_func                      #\n",
    "        # 4. Append loss to loss_all                        #\n",
    "        #####################################################\n",
    "        # 1. Put correct variables into model to get logits\n",
    "        logits = model(batch=inputs)\n",
    "\n",
    "        # 2. Get labels\n",
    "        labels = inputs['label']\n",
    "\n",
    "        # 3. Evalutate using loss_func\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Append loss to loss_all\n",
    "        loss_all.append(loss.item())\n",
    "        #####################################################\n",
    "        #                 End of your code                  #\n",
    "        #####################################################\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return np.mean(loss_all)\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    with torch.no_grad():\n",
    "        for step, inputs in enumerate(val_dataloader):\n",
    "            if cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits #\n",
    "            # 2. Get labels                                     #\n",
    "            # 3. Extend labels to list                          #\n",
    "            # 4. Get predictions and extend preds to list       #\n",
    "            #####################################################\n",
    "            # 1. Put correct variables into model to get logits\n",
    "            logits = model(batch=inputs)\n",
    "\n",
    "            # 2. Get labels\n",
    "            labels = inputs['label']\n",
    "\n",
    "            # 3. Extend labels to list\n",
    "            alllabels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # 4. Get predictions and extend preds to list\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            allpreds.extend(preds.cpu().numpy())\n",
    "            #####################################################\n",
    "            #                 End of your code                  #\n",
    "            #####################################################\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generated template from TemplateGenerator and find the best template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T05:13:36.594464200Z",
     "start_time": "2023-12-10T03:36:46.673523500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 1454.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:37<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"placeholder\": \"text_a\"} It was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} it was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001b[A\n",
      "tokenizing: 32it [00:00, 202.53it/s]A\n",
      "\n",
      "tokenizing: 32it [00:00, 727.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.562330920593297, Eval score=0.5\n",
      "Epoch 2: Train loss=1.029085214715451, Eval score=0.5\n",
      "Epoch 3: Train loss=0.671642615867313, Eval score=0.71875\n",
      "Epoch 4: Train loss=0.2566610455396585, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [15:59<1:03:57, 959.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.2469192902863142, Eval score=0.78125\n",
      "Current template: {\"placeholder\": \"text_a\"} it was {\"mask\"} ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' it was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' ..the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 379.37it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.511358927668084, Eval score=0.5\n",
      "Epoch 2: Train loss=0.7635227439459413, Eval score=0.5\n",
      "Epoch 3: Train loss=0.3154368309772053, Eval score=0.53125\n",
      "Epoch 4: Train loss=0.8977778584977472, Eval score=0.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [36:43<56:21, 1127.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.7606429383413342, Eval score=0.5\n",
      "Current template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 571.42it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1180.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.765889931773188, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.5693292848736746, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.041312409681268036, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.2322409824218994, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [56:36<38:33, 1156.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.22881872234574985, Eval score=0.84375\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' . It was terrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 1066.65it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1523.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.7746381501195, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9883591458201408, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8319057337939739, Eval score=0.625\n",
      "Epoch 4: Train loss=0.508084288907412, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [1:16:21<19:27, 1167.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.39746711112820776, Eval score=0.53125\n",
      "Current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' . It was horrible.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 864.68it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1454.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.6042319842349002, Eval score=0.5\n",
      "Epoch 2: Train loss=1.069442194304429, Eval score=0.5\n",
      "Epoch 3: Train loss=0.47684725234284997, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.2768472472046142, Eval score=0.59375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:36:08<00:00, 1153.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.07557142606344769, Eval score=0.625\n",
      "Final best template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.\n",
      "\n",
      "Wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': ' film.the big finish is a bit like getting all excited about a chocolate eclair and then biting into it and finding the filling missing . It was terrible. a film that will enthrall the whole family . It was great.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class ManualTemplateWithoutParse(ManualTemplate):\n",
    "    \"\"\"The generated template from TemplateGenerator is a list of dict of parsed template_text. So no further parsing is needed.\"\"\"\n",
    "    def on_text_set(self):\n",
    "        pass\n",
    "\n",
    "# Template generation\n",
    "if auto_t:\n",
    "    print('performing auto_t...')\n",
    "\n",
    "    if cuda:\n",
    "        template_generate_model = template_generate_model.cuda()\n",
    "    template_generator = T5TemplateGenerator(template_generate_model, template_generate_tokenizer, template_tokenizer_wrapper, verbalizer, beam_width=5) # Beam_width is set to 5 here for efficiency; to improve performance, try a larger number.\n",
    "\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=template_generate_tokenizer, tokenizer_wrapper_class=template_tokenizer_wrapper, batch_size=len(dataset['train']), decoder_max_length=128, max_seq_length=128, shuffle=False, teacher_forcing=False) # Register all data at once\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        template_generator._register_buffer(data)\n",
    "\n",
    "    template_generate_model.eval()\n",
    "    print('generating...')\n",
    "    template_texts = template_generator._get_templates()\n",
    "\n",
    "    original_template = template.text\n",
    "    template_texts = [template_generator.convert_template(template_text, original_template) for template_text in template_texts]\n",
    "    # template_generator._show_template()\n",
    "    template_generator.release_memory()\n",
    "    # Generate a number of candidate template text\n",
    "    print(template_texts)\n",
    "    \n",
    "    # Iterate over each candidate and select the best one\n",
    "    best_metrics = 0.0\n",
    "    best_template_text = None\n",
    "    for template_text in tqdm(template_texts):\n",
    "        verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "        template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
    "        print(f\"Current template: {template_text}\\n\\nWrapped example: {template.wrap_one_example(dataset['train'][0])}\")\n",
    "\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "        \n",
    "        #######################################################\n",
    "        # TODO: Use score to Find your best template_text     #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_template_text = template_text\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # Use the best template\n",
    "    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=['terrible','great'])\n",
    "    template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=best_template_text)\n",
    "    print(\"Final best template:\", best_template_text)\n",
    "    print()\n",
    "    print(\"Wrapped example:\", template.wrap_one_example(dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbalizer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verbalizer template from VerbalizerGenerator and find the best verbalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T11:58:04.838076700Z",
     "start_time": "2023-12-10T05:13:36.638466100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing auto_v...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 1391.30it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current label_words: ['terrible', 'beautiful']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 0it [00:00, ?it/s]\u001b[A\n",
      "tokenizing: 3it [00:00, 21.48it/s]\u001b[A\n",
      "tokenizing: 6it [00:01,  5.15it/s]\u001b[A\n",
      "tokenizing: 8it [00:01,  6.27it/s]\u001b[A\n",
      "tokenizing: 32it [00:01, 23.45it/s]\u001b[A\n",
      "\n",
      "tokenizing: 32it [00:00, 969.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.1593105591260544, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.5407680265489034, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.17979280870349612, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.006995583800744498, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [21:05<6:40:40, 1265.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.002051841642924046, Eval score=0.84375\n",
      "current label_words: ['horrible', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 695.63it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1388.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.5843039118335565, Eval score=0.84375\n",
      "Epoch 2: Train loss=0.0798521326687478, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.0026221817748819376, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.0010607897513068565, Eval score=0.78125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [42:02<6:18:07, 1260.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0004625524882726495, Eval score=0.78125\n",
      "current label_words: ['horrible', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 762.07it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1333.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.0936600251085586, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.849827597849071, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.414547988853883, Eval score=0.75\n",
      "Epoch 4: Train loss=0.0500250239797424, Eval score=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [1:02:46<5:55:04, 1253.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.004471211226245941, Eval score=0.8125\n",
      "current label_words: ['sad', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 415.60it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 941.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4185917818103917, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.5065413688316767, Eval score=0.875\n",
      "Epoch 3: Train loss=0.012464412650388113, Eval score=0.875\n",
      "Epoch 4: Train loss=0.002708091426967485, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [1:23:35<5:33:44, 1251.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0004699288858773798, Eval score=0.875\n",
      "current label_words: ['bad', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 551.73it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=3.1230944280390602, Eval score=0.5\n",
      "Epoch 2: Train loss=1.270681380527094, Eval score=0.5\n",
      "Epoch 3: Train loss=0.8551086119841784, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.6224154182709754, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [1:45:07<5:16:30, 1266.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.22766433987999335, Eval score=0.8125\n",
      "current label_words: ['horrible', 'remarkable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 426.62it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1031.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.2276666148868003, Eval score=0.6875\n",
      "Epoch 2: Train loss=0.4889321715090773, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2634941844644345, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.25736280560994373, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [2:07:36<5:01:58, 1294.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.3903749104914027, Eval score=0.625\n",
      "current label_words: ['terrible', 'brilliant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 493.33it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.128300400949797, Eval score=0.5\n",
      "Epoch 2: Train loss=1.0674331626432831, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.6011688623111695, Eval score=0.875\n",
      "Epoch 4: Train loss=0.6810656589186692, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [2:28:49<4:38:53, 1287.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.1570308672144165, Eval score=0.84375\n",
      "current label_words: ['terrible', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 451.65it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1033.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.1662085960851982, Eval score=0.84375\n",
      "Epoch 2: Train loss=0.046194135922632995, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.20158991879031873, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.009880203132524912, Eval score=0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [2:51:16<4:21:16, 1306.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.27703922392970526, Eval score=0.90625\n",
      "current label_words: ['disappointing', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 415.57it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1032.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.3897865504303581, Eval score=0.71875\n",
      "Epoch 2: Train loss=0.6985758165101288, Eval score=0.5625\n",
      "Epoch 3: Train loss=0.20094251398768392, Eval score=0.875\n",
      "Epoch 4: Train loss=0.021359096241212683, Eval score=0.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [3:11:17<3:53:28, 1273.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0014102687238164435, Eval score=0.90625\n",
      "current label_words: ['strange', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 410.23it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4916955009493904, Eval score=0.5\n",
      "Epoch 2: Train loss=0.5601507005485473, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.050391235166898696, Eval score=0.78125\n",
      "Epoch 4: Train loss=0.04447055474645367, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [3:31:10<3:28:06, 1248.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.5673459974156145, Eval score=0.78125\n",
      "current label_words: ['terrible', 'good']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 363.21it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.9351653824437118, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9128216816243366, Eval score=0.78125\n",
      "Epoch 3: Train loss=0.16854792425147025, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.004711857527581742, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [3:50:34<3:03:23, 1222.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0015127657302400621, Eval score=0.84375\n",
      "current label_words: ['bad', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 524.59it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1142.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.8022562539517715, Eval score=0.5\n",
      "Epoch 2: Train loss=0.9302898038731655, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.43625156552297994, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.04196147369020764, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [4:10:06<2:40:57, 1207.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.012102704274127518, Eval score=0.65625\n",
      "current label_words: ['short', 'superb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 412.44it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1203.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.8797737168388267, Eval score=0.75\n",
      "Epoch 2: Train loss=0.3757321042244257, Eval score=0.71875\n",
      "Epoch 3: Train loss=0.2311989847357836, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.43366863449273296, Eval score=0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [4:30:04<2:20:30, 1204.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.620734619528541, Eval score=0.59375\n",
      "current label_words: ['disappointing', 'delightful']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 426.69it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1143.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.324861448905718, Eval score=0.59375\n",
      "Epoch 2: Train loss=0.5381804386270232, Eval score=0.5625\n",
      "Epoch 3: Train loss=0.36967586419450527, Eval score=0.84375\n",
      "Epoch 4: Train loss=0.12384688404563349, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [4:49:18<1:58:55, 1189.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.18691727038913086, Eval score=0.6875\n",
      "current label_words: ['bad', 'brilliant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 490.61it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1103.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.5005056243027184, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.7745201710495166, Eval score=0.875\n",
      "Epoch 3: Train loss=0.13810731278863386, Eval score=0.8125\n",
      "Epoch 4: Train loss=0.004411970109288177, Eval score=0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [5:08:29<1:38:08, 1177.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.00313117326692236, Eval score=0.78125\n",
      "current label_words: ['bad', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 492.79it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1066.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=2.182302282977588, Eval score=0.5\n",
      "Epoch 2: Train loss=0.5332906207768247, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.08191546555644891, Eval score=0.65625\n",
      "Epoch 4: Train loss=0.35411459776105403, Eval score=0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [5:26:58<1:17:08, 1157.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.13951104862388775, Eval score=0.46875\n",
      "current label_words: ['horrible', 'perfect']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 466.99it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1219.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4121702450024713, Eval score=0.5625\n",
      "Epoch 2: Train loss=0.32750329683221935, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.01347528245059948, Eval score=0.90625\n",
      "Epoch 4: Train loss=0.3011642301885331, Eval score=0.84375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [5:45:44<57:23, 1147.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.022153147599055956, Eval score=0.875\n",
      "current label_words: ['fine', 'remarkable']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 492.08it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1185.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.7259275259780793, Eval score=0.5\n",
      "Epoch 2: Train loss=0.7986743192886934, Eval score=0.65625\n",
      "Epoch 3: Train loss=0.6269949703128077, Eval score=0.5\n",
      "Epoch 4: Train loss=0.16056611831118062, Eval score=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [6:04:43<38:10, 1145.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.003870869544044808, Eval score=0.78125\n",
      "current label_words: ['disappointing', 'fascinating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 454.61it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1183.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.4480454477061357, Eval score=0.5\n",
      "Epoch 2: Train loss=0.8791331336833537, Eval score=0.8125\n",
      "Epoch 3: Train loss=0.4247932309117459, Eval score=0.6875\n",
      "Epoch 4: Train loss=0.2258107879970339, Eval score=0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [6:23:21<18:56, 1137.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.0734178872121447, Eval score=0.875\n",
      "current label_words: ['terrible', 'fine']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizing: 32it [00:00, 489.28it/s]\n",
      "\n",
      "tokenizing: 32it [00:00, 1164.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.6715138442009447, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.6137157797584223, Eval score=0.6875\n",
      "Epoch 3: Train loss=0.38552796499482156, Eval score=0.875\n",
      "Epoch 4: Train loss=0.34600197029577373, Eval score=0.65625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [6:41:58<00:00, 1205.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss=0.6120385159649686, Eval score=0.78125\n",
      "final best label words: ['terrible', 'terrific']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verbalizer generation\n",
    "from openprompt.prompts.prompt_generator import RobertaVerbalizerGenerator\n",
    "if auto_v:\n",
    "    print('performing auto_v...')\n",
    "    # Load generation model for verbalizer generation\n",
    "    if cuda:\n",
    "        plm = plm.cuda()\n",
    "    verbalizer_generator = RobertaVerbalizerGenerator(model=plm, tokenizer=tokenizer, candidate_num=20, label_word_num_per_class=20)\n",
    "    # To improve performance, try larger numbers\n",
    "\n",
    "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, batch_size=32)\n",
    "    for data in dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        verbalizer_generator.register_buffer(data)\n",
    "    label_words_list = verbalizer_generator.generate()\n",
    "    verbalizer_generator.release_memory()\n",
    "\n",
    "    # Iterate over each candidate and select the best one\n",
    "    current_verbalizer = copy.deepcopy(verbalizer)\n",
    "    best_metrics = 0.0\n",
    "    best_label_words = None\n",
    "    for label_words in tqdm(label_words_list):\n",
    "        print(f\"current label_words: {label_words}\")\n",
    "        current_verbalizer.label_words = label_words\n",
    "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "        model = PromptForClassification(copy.deepcopy(plm), template, current_verbalizer)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        if cuda:\n",
    "            model = model.cuda()\n",
    "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
    "\n",
    "        #######################################################\n",
    "        # TODO: Use score to find your best_label_word        #\n",
    "        #######################################################\n",
    "        if score > best_metrics:\n",
    "            best_metrics = score\n",
    "            best_label_words = label_words\n",
    "        #######################################################\n",
    "        #                 End of your code                    #\n",
    "        #######################################################\n",
    "    # use the best verbalizer\n",
    "    print(\"final best label words:\", best_label_words)\n",
    "    verbalizer = ManualVerbalizer(tokenizer, num_classes=2, label_words=best_label_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:18:43.541949400Z",
     "start_time": "2023-12-10T11:58:04.849042700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 32it [00:00, 383.01it/s]\n",
      "tokenizing: 32it [00:00, 1142.78it/s]\n",
      "tokenizing: 872it [00:00, 1095.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss=1.2980121186483302, Eval score=0.8125\n",
      "Epoch 2: Train loss=0.38737686289732665, Eval score=0.90625\n",
      "Epoch 3: Train loss=0.08258779709103692, Eval score=0.875\n",
      "Epoch 4: Train loss=0.3668047572554656, Eval score=0.84375\n",
      "Epoch 5: Train loss=0.00465579945631589, Eval score=0.84375\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
    "valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "test_dataloader = PromptDataLoader(dataset['test'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
    "\n",
    "\n",
    "model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# It's always good practice to set no decay to bias and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/5b8876ed26fd495b8353ad7ce94b6f65)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-10T12:46:57.884699300Z",
     "start_time": "2023-12-10T12:18:43.474013600Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "allpreds = []\n",
    "for step, inputs in tqdm(enumerate(test_dataloader)):\n",
    "    if cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = model(inputs)\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    f.write('index,sentiment_label\\n')\n",
    "    for i, pred in enumerate(allpreds):\n",
    "        f.write('{},{}\\n'.format(i, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report (15 points)\n",
    "\n",
    "- Task 1: Compare **two** different models you employed and provide a brief discussion of your implementation.\n",
    "\n",
    "- Task 2: You need to try at least **three** different templates and verbalizers to compare how your prompts work with the model. Report your performance in zero-shot, one-shot, and few-shot scenarios, with examples drawn from the training set.\n",
    "\n",
    "- Task 3: Try at least three different manually crafted templates to compare them with auto-generated templates. Evaluate the performance with different numbers of demonstrations and plot the graph from Figure 3 in the paper (https://arxiv.org/pdf/2012.15723.pdf). Also, report your best template and verbalizer.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
